import os
import random
import numpy as np
import torch
import torch.nn.functional as F
import argparse
from datasets import load_from_disk
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    TrainerCallback
)
from typing import Dict, Any, List, Union
from dataclasses import dataclass

# ---------------------------------------------------------------------------
# éšæœºç§å­è®¾ç½®å‡½æ•°ï¼ˆç¡®ä¿å¯é‡å¤æ€§ï¼‰
# ---------------------------------------------------------------------------

def seed_everything(seed=42):
    """
    è®¾ç½®æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼Œç¡®ä¿è®­ç»ƒçš„å¯é‡å¤æ€§ã€‚
    
    Args:
        seed: éšæœºç§å­å€¼ï¼ˆé»˜è®¤: 2003ï¼‰
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False  # ä¸ºäº†å¯é‡å¤æ€§ï¼Œç¦ç”¨benchmark
    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ç¡®ä¿å®Œå…¨ç¡®å®šæ€§ï¼ˆå¦‚æœä½¿ç”¨CUDAï¼‰
    os.environ['PYTHONHASHSEED'] = str(seed)

# ---------------------------------------------------------------------------
# CustomSymPOTrainer
# ---------------------------------------------------------------------------

class CustomSymPOTrainer(Trainer):
    def __init__(
        self,
        model: Union[torch.nn.Module],
        ref_model: Union[torch.nn.Module], # 1. æ–°å¢ ref_model å‚æ•°
        args: TrainingArguments,
        beta_kl: float,
        log_ratio_clip_min: float,
        log_ratio_clip_max: float,
        **kwargs,
    ):
        super().__init__(model=model, args=args, **kwargs)

        if not hasattr(self, 'processor') or self.processor is None:
            self.processor = self.tokenizer

        self.ref_model = ref_model.to(self.args.device)
        self.beta_kl = beta_kl
        self.log_ratio_clip_min = log_ratio_clip_min
        self.log_ratio_clip_max = log_ratio_clip_max
        self.use_smooth_clip = False  # é»˜è®¤ä½¿ç”¨ç¡¬è£å‰ªï¼Œå¯é€šè¿‡å‚æ•°åˆ‡æ¢

    def _smooth_clamp(self, x: torch.Tensor, min_val: float, max_val: float, temperature: float = 0.1) -> torch.Tensor:
        """
        å¹³æ»‘è£å‰ªå‡½æ•°ï¼Œåœ¨è¾¹ç•Œå¤„ä¿æŒå°æ¢¯åº¦ï¼Œé¿å…æ¢¯åº¦çªç„¶æˆªæ–­ã€‚
        
        ä½¿ç”¨ tanh-based å¹³æ»‘å‡½æ•°ï¼š
        - åœ¨è¾¹ç•Œå¤„æ¢¯åº¦è¿ç»­ï¼Œä¸ä¼šçªç„¶å˜ä¸º0
        - temperature æ§åˆ¶å¹³æ»‘ç¨‹åº¦ï¼ˆè¶Šå°è¶Šæ¥è¿‘ç¡¬è£å‰ªï¼‰
        """
        # å°†è¾“å…¥å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
        normalized = 2.0 * (x - min_val) / (max_val - min_val) - 1.0
        # ä½¿ç”¨ tanh è¿›è¡Œå¹³æ»‘è£å‰ª
        clipped_normalized = torch.tanh(normalized / temperature)
        # æ˜ å°„å›åŸå§‹èŒƒå›´
        return (clipped_normalized + 1.0) / 2.0 * (max_val - min_val) + min_val
    
    def _get_log_probs(self, model: AutoModelForCausalLM, prompts: List[str], responses: List[str]) -> torch.Tensor:
        # è¿™ä¸ªè¾…åŠ©å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œå®ƒå¯ä»¥é€šç”¨åœ°è®¡ç®—ä»»ä½•æ¨¡å‹ã€prompt å’Œ response çš„ log_probs
        original_padding_side = self.processor.padding_side
        self.processor.padding_side = 'left'
        full_texts = [p + r for p, r in zip(prompts, responses)]
        prompt_tokens = self.processor(prompts, padding=False, truncation=False)
        prompt_lengths = [len(p) for p in prompt_tokens['input_ids']]
        unwrapped_model = model.module if hasattr(model, "module") else model
        max_len = unwrapped_model.config.max_position_embeddings
        full_tokens = self.processor(
            full_texts, padding='longest', truncation=True,
            max_length=4096,
            return_tensors="pt"
        )
        input_ids = full_tokens['input_ids'].to(self.args.device)
        attention_mask = full_tokens['attention_mask'].to(self.args.device)
        outputs = model(input_ids, attention_mask=attention_mask, return_dict=True, use_cache=False)
        logits = outputs.logits
        shifted_logits = logits[..., :-1, :]
        shifted_labels = input_ids[..., 1:]
        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
        nll_per_token = loss_fct(shifted_logits.reshape(-1, shifted_logits.size(-1)), shifted_labels.reshape(-1))
        nll_per_token = nll_per_token.view(input_ids.size(0), -1)
        log_probs_per_token = -nll_per_token
        seq_len = shifted_labels.size(1)
        position_ids = torch.arange(seq_len, device=self.args.device).expand_as(shifted_labels)
        prompt_lengths_tensor = torch.tensor(prompt_lengths, device=self.args.device).unsqueeze(1)
        response_start_index = prompt_lengths_tensor - 1
        mask = position_ids >= response_start_index
        attention_mask_shifted = attention_mask[:, 1:].to(torch.bool)
        response_mask = mask & attention_mask_shifted
        masked_log_probs = log_probs_per_token * response_mask
        total_log_probs = masked_log_probs.sum(dim=1)
        self.processor.padding_side = original_padding_side
        return total_log_probs

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        prompts = inputs.pop("prompt")
        chosen_responses = inputs.pop("chosen")
        rejected_responses = inputs.pop("rejected")
        # 3. ç§»é™¤ä» inputs ä¸­è·å– ref_logp çš„ä»£ç 
        # log_probs_ref_y1 = inputs.pop("ref_logp_chosen").to(self.args.device)
        # log_probs_ref_y2 = inputs.pop("ref_logp_rejected").to(self.args.device)
        f_y1 = inputs.pop("reward_chosen").to(self.args.device)
        f_y2 = inputs.pop("reward_rejected").to(self.args.device)

        batch_size = len(prompts)
        combined_prompts = prompts + prompts
        combined_responses = chosen_responses + rejected_responses

        # --- è®¡ç®—ç­–ç•¥æ¨¡å‹çš„ log_probs ---
        all_log_probs_pi = self._get_log_probs(model, combined_prompts, combined_responses)
        log_probs_pi_y1 = all_log_probs_pi[:batch_size]
        log_probs_pi_y2 = all_log_probs_pi[batch_size:]

        # --- 4. åŠ¨æ€è®¡ç®—å‚è€ƒæ¨¡å‹çš„ log_probs ---
        with torch.no_grad():
            all_log_probs_ref = self._get_log_probs(self.ref_model, combined_prompts, combined_responses)
            log_probs_ref_y1 = all_log_probs_ref[:batch_size]
            log_probs_ref_y2 = all_log_probs_ref[batch_size:]
        if self.state.global_step == 0: # åªåœ¨ç¬¬ä¸€æ­¥æ‰“å°
            print(f"--- Sanity Check at Step 0 ---")
            print(f"Sample 0 - ref_logp_chosen: {log_probs_ref_y1[0].item()}")
            print(f"Sample 0 - pi_logp_chosen:  {log_probs_pi_y1[0].item()}")
            print(f"Sample 0 - Difference (pi - ref): {log_probs_pi_y1[0].item() - log_probs_ref_y1[0].item()}")
            print(f"---------------------------------")
        # --- åç»­æŸå¤±è®¡ç®—é€»è¾‘ä¸å˜ ---
        log_ratio_y1 = log_probs_pi_y1 - log_probs_ref_y1
        log_ratio_y2 = log_probs_pi_y2 - log_probs_ref_y2
        
        with torch.no_grad():
            kl_term1 = self.beta_kl * log_ratio_y1.detach()
            # kl_term2 = self.beta_kl * log_ratio_y2.detach()
            weight1 = 1 - f_y2 - kl_term1
            # weight2 = f_y1 + kl_term2
            weight2 = f_y1

            
        # ä½¿ç”¨å¹³æ»‘è£å‰ªæˆ–ç¡¬è£å‰ª
        if self.use_smooth_clip:
            # å¹³æ»‘è£å‰ªï¼šåœ¨è¾¹ç•Œå¤„ä¿æŒå°æ¢¯åº¦ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
            clamped_log_ratio_y1 = self._smooth_clamp(log_ratio_y1, self.log_ratio_clip_min, self.log_ratio_clip_max)
            clamped_log_ratio_y2 = self._smooth_clamp(log_ratio_y2, self.log_ratio_clip_min, self.log_ratio_clip_max)
        else:
            # ç¡¬è£å‰ªï¼šä¼ ç»Ÿæ–¹å¼ï¼Œè¾¹ç•Œå¤„æ¢¯åº¦ä¸º0
            clamped_log_ratio_y1 = torch.clamp(log_ratio_y1, min=self.log_ratio_clip_min, max=self.log_ratio_clip_max)
            clamped_log_ratio_y2 = torch.clamp(log_ratio_y2, min=self.log_ratio_clip_min, max=self.log_ratio_clip_max)
        
        ratio_y1 = torch.exp(clamped_log_ratio_y1)
        ratio_y2 = torch.exp(clamped_log_ratio_y2)
        
        # ratio_y1 = torch.exp(log_ratio_y1)
        # ratio_y2 = torch.exp(log_ratio_y2)
        J_sym_objective = ratio_y1 * weight1 - ratio_y2 * weight2
        total_loss = -J_sym_objective.mean()
        
        logs = {
            # "mean_log_probs_pi_chosen": log_probs_pi_y1.detach().mean().item(),
            # "mean_log_probs_pi_rejected": log_probs_pi_y2.detach().mean().item(),
            # "mean_log_probs_ref_chosen": log_probs_ref_y1.detach().mean().item(),
            # "mean_log_probs_ref_rejected": log_probs_ref_y2.detach().mean().item(),
            "mean_ratio_chosen": ratio_y1.detach().mean().item(),
            "mean_ratio_rejected": ratio_y2.detach().mean().item(),
            "weight_chosen": weight1.detach().mean().item(),
            "weight_rejected": weight2.detach().mean().item(),
            "kl_term_chosen": kl_term1.detach().mean().item(),
        }
        self._current_logs = logs
        
        return total_loss

    # log æ–¹æ³•æ— éœ€ä¿®æ”¹
    def log(self, logs: dict, *args, **kwargs) -> None:
        if hasattr(self, "_current_logs") and self._current_logs is not None:
            logs.update(self._current_logs)
            self._current_logs = None
        super().log(logs, *args, **kwargs)

# Data Collator å’Œ Callback æ— éœ€ä¿®æ”¹
@dataclass
class DataCollatorForCustomSymPO:
    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        batch = {}
        for key in features[0].keys():
            # ç§»é™¤ ref_logp ç›¸å…³å­—æ®µçš„å¤„ç†ï¼Œå› ä¸ºå®ƒä»¬å·²ä¸å­˜åœ¨
            if key.startswith("ref_logp"):
                continue
            if isinstance(features[0][key], str):
                batch[key] = [feature[key] for feature in features]
            else:
                batch[key] = torch.tensor([feature[key] for feature in features])
        return batch

class PrintingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero and logs:
            print(logs)

# argparse å‡½æ•°æ— éœ€ä¿®æ”¹
def parse_args():
    parser = argparse.ArgumentParser(description="Train a policy model with SymPO algorithm.")
    
    # --- è·¯å¾„å‚æ•° ---
    parser.add_argument("--sft_model_path", type=str, default="/train/Llama-3-8B-Instruct", help="Path to the SFT base model.")
    parser.add_argument("--preprocessed_dataset_path", type=str, default="/train/precomputed_traindataset", help="Path to the precomputed dataset.")
    parser.add_argument("--output_dir", type=str, default="/train/output_model/llama3-8b-sympo-1e-6_0.5_no_kl_2_seed_42", help="Directory to save checkpoints and final model.")
    
    # --- è®­ç»ƒè¶…å‚æ•° ---
    parser.add_argument("--learning_rate", type=float, default=1e-6, help="Learning rate.")
    parser.add_argument("--num_train_epochs", type=int, default=1, help="Number of training epochs.")
    parser.add_argument("--per_device_train_batch_size", type=int, default=1, help="Batch size per GPU.")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4, help="Number of gradient accumulation steps.")
    parser.add_argument("--gradient_checkpointing", action='store_true', help="Enable gradient checkpointing to save memory.")
    parser.add_argument("--warmup_ratio", type=float, default=0.1, help="Warmup ratio for learning rate scheduler.")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="Max gradient norm for clipping.")
    parser.add_argument("--logging_steps", type=int, default=10, help="Log metrics every N steps.")
    parser.add_argument("--save_steps", type=int, default=200, help="Save a checkpoint every N steps.")
    parser.add_argument("--save_total_limit", type=int, default=20, help="Limit the total number of saved checkpoints.")

    # --- SymPO ç‰¹å®šå‚æ•° ---
    parser.add_argument("--beta_kl", type=float, default=0.5, help="KL divergence penalty coefficient.")
    parser.add_argument("--log_ratio_clip_min", type=float, default=-2.3, help="Minimum clip value for log probability ratio.")
    parser.add_argument("--log_ratio_clip_max", type=float, default=2.3, help="Maximum clip value for log probability ratio.")
    parser.add_argument("--use_smooth_clip", action='store_true', help="Use smooth clipping for better gradient stability (recommended).")
    
    # --- W&B (Weights & Biases) æ—¥å¿—å‚æ•° ---
    parser.add_argument("--report_to", type=str, default="wandb", help="The integration to report results to (e.g., 'wandb').")
    parser.add_argument("--run_name", type=str, default=f"policy-llama3-8b-sympo-default", help="A name for the W&B run.")
    
    # --- éšæœºç§å­å‚æ•° ---
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility .")

    return parser.parse_args()


def main():
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    print(f"è®¾ç½®éšæœºç§å­ä¸º: {args.seed}")
    seed_everything(args.seed)

    tokenizer = AutoTokenizer.from_pretrained(args.sft_model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left' 

    print("ä»ç£ç›˜åŠ è½½æ•°æ®é›† (æ³¨æ„: æ•°æ®é›†ä¸­ä¸å†éœ€è¦ ref_logp)...")
    precomputed_dataset_train = load_from_disk(args.preprocessed_dataset_path)

    print("åŠ è½½ç”¨äºè®­ç»ƒçš„ç­–ç•¥æ¨¡å‹ (Policy Model)...")
    policy_model = AutoModelForCausalLM.from_pretrained(
        args.sft_model_path, dtype=torch.bfloat16
    )

    # 5. æ–°å¢ï¼šåŠ è½½ä½œä¸ºå‚è€ƒçš„SFTæ¨¡å‹ (Reference Model)
    print("åŠ è½½ä½œä¸ºå‚è€ƒçš„SFTæ¨¡å‹ (Reference Model)...")
    ref_model = AutoModelForCausalLM.from_pretrained(
        args.sft_model_path, dtype=torch.bfloat16
    )
    # # å°†å‚è€ƒæ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå¹¶ä¸”ä¸éœ€è¦è®¡ç®—å®ƒçš„æ¢¯åº¦
    # ref_model.eval()

    # TrainingArguments å®šä¹‰æ— éœ€ä¿®æ”¹
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        # gradient_checkpointing=args.gradient_checkpointing,
        gradient_checkpointing=False,
        gradient_checkpointing_kwargs={'use_reentrant': False},
        optim="adamw_torch",
        learning_rate=args.learning_rate,
        max_grad_norm=args.max_grad_norm,
        warmup_ratio=args.warmup_ratio,
        lr_scheduler_type="cosine",
        logging_strategy="steps",
        logging_steps=args.logging_steps,
        logging_first_step=True,
        bf16=True,
        save_strategy="steps",
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit, 
        eval_strategy="no",
        remove_unused_columns=False,
        report_to=args.report_to,
        run_name=args.run_name,
    )
    
    data_collator = DataCollatorForCustomSymPO()

    print("åˆå§‹åŒ– CustomSymPOTrainer...")
    trainer = CustomSymPOTrainer(
        model=policy_model,
        ref_model=ref_model, # 6. å°† ref_model ä¼ é€’ç»™ Trainer
        args=training_args,
        tokenizer=tokenizer,
        train_dataset=precomputed_dataset_train,
        data_collator=data_collator,
        callbacks=[PrintingCallback()],
        beta_kl=args.beta_kl,
        log_ratio_clip_min=args.log_ratio_clip_min,
        log_ratio_clip_max=args.log_ratio_clip_max,
    )
    # è®¾ç½®æ˜¯å¦ä½¿ç”¨å¹³æ»‘è£å‰ª
    trainer.use_smooth_clip = args.use_smooth_clip
    # ç§»é™¤ sft_model_pathï¼Œå› ä¸ºå®ƒå·²ç»é€šè¿‡ ref_model å¯¹è±¡ä¼ å…¥äº†
    # sft_model_path=args.sft_model_path, 

        # è®¾ç½®æ˜¯å¦ä½¿ç”¨å¹³æ»‘è£å‰ª
    trainer.use_smooth_clip = args.use_smooth_clip
    # ç§»é™¤ sft_model_pathï¼Œå› ä¸ºå®ƒå·²ç»é€šè¿‡ ref_model å¯¹è±¡ä¼ å…¥äº†
    # sft_model_path=args.sft_model_path, 

    print("å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒ...")
    train_result = trainer.train()
    print("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")

    ##################################
    # Save final model (HuggingFace format)
    ##################################
    print("\n" + "=" * 60)
    print("*** Save final model ***")
    print("=" * 60)
    print(f"âš ï¸  æ³¨æ„: å¦‚æœä½¿ç”¨ FSDPï¼Œæ­¤æ­¥éª¤ä¼šè‡ªåŠ¨åˆå¹¶åˆ†ç‰‡æƒé‡å¹¶ä¿å­˜ä¸º HuggingFace æ ¼å¼")
    print(f"ä¿å­˜è·¯å¾„: {args.output_dir}")
    
    # ç¡®ä¿ FSDP ä½¿ç”¨ FULL_STATE_DICT æ¨¡å¼ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if trainer.is_fsdp_enabled and trainer.accelerator.state.fsdp_plugin is not None:
        print("æ£€æµ‹åˆ° FSDPï¼Œè®¾ç½®çŠ¶æ€å­—å…¸ç±»å‹ä¸º FULL_STATE_DICT...")
        trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆè‡ªåŠ¨å¤„ç† FSDP åˆå¹¶ï¼‰
    trainer.save_model(args.output_dir)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_dir}")
    
    # ä¿å­˜ tokenizer
    try:
        tokenizer.save_pretrained(args.output_dir)
        print("âœ… Tokenizer å·²ä¿å­˜")
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: æ— æ³•è‡ªåŠ¨ä¿å­˜ tokenizer: {e}")
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    if trainer.accelerator.is_main_process:
        trainer.log_metrics("train", train_result.metrics)
        trainer.save_metrics("train", train_result.metrics)
        print("âœ… è®­ç»ƒæŒ‡æ ‡å·²ä¿å­˜")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå’Œæ¨¡å‹ä¿å­˜å®Œæˆï¼")
    print("=" * 60)
    print(f"\næœ€ç»ˆ HuggingFace æ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_dir}")
    print("å¯ä»¥ç›´æ¥ä½¿ç”¨ HuggingFace çš„ from_pretrained() åŠ è½½æ¨¡å‹")
    print(f"\næ³¨æ„: checkpoint-* ç›®å½•æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸­é—´æ£€æŸ¥ç‚¹ï¼ˆFSDP åˆ†ç‰‡æ ¼å¼ï¼‰")
    print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨ {args.output_dir} æ ¹ç›®å½•ï¼ˆHuggingFace æ ¼å¼ï¼‰")
if __name__ == "__main__":
    main()