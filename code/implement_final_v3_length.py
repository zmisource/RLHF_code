import os
import random
import numpy as np
import torch
import torch.nn.functional as F
import argparse
from datasets import load_from_disk
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    TrainerCallback
)
from typing import Dict, Any, List, Union
from dataclasses import dataclass

# ---------------------------------------------------------------------------
# éšæœºç§å­è®¾ç½®å‡½æ•°ï¼ˆç¡®ä¿å¯é‡å¤æ€§ï¼‰
# ---------------------------------------------------------------------------

def seed_everything(seed=42):
    """
    è®¾ç½®æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼Œç¡®ä¿è®­ç»ƒçš„å¯é‡å¤æ€§ã€‚
    
    Args:
        seed: éšæœºç§å­å€¼ï¼ˆé»˜è®¤: 2003ï¼‰
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False  # ä¸ºäº†å¯é‡å¤æ€§ï¼Œç¦ç”¨benchmark
    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ç¡®ä¿å®Œå…¨ç¡®å®šæ€§ï¼ˆå¦‚æœä½¿ç”¨CUDAï¼‰
    os.environ['PYTHONHASHSEED'] = str(seed)

# ---------------------------------------------------------------------------
# CustomSymPOTrainer
# ---------------------------------------------------------------------------

class CustomSymPOTrainer(Trainer):
    def __init__(
        self,
        model: Union[torch.nn.Module],
        ref_model: Union[torch.nn.Module], # 1. æ–°å¢ ref_model å‚æ•°
        args: TrainingArguments,
        beta_kl: float,
        log_ratio_clip_min: float,
        log_ratio_clip_max: float,
        max_length: int = None,
        max_prompt_length: int = None,
        truncation_mode: str = "keep_end",
        **kwargs,
    ):
        super().__init__(model=model, args=args, **kwargs)

        if not hasattr(self, 'processor') or self.processor is None:
            self.processor = self.tokenizer

        self.ref_model = ref_model.to(self.args.device)
        self.beta_kl = beta_kl
        self.log_ratio_clip_min = log_ratio_clip_min
        self.log_ratio_clip_max = log_ratio_clip_max
        self.use_smooth_clip = False  # é»˜è®¤ä½¿ç”¨ç¡¬è£å‰ªï¼Œå¯é€šè¿‡å‚æ•°åˆ‡æ¢
        
        # æˆªæ–­ç›¸å…³å‚æ•°
        unwrapped_model = model.module if hasattr(model, "module") else model
        self.max_length = max_length if max_length is not None else unwrapped_model.config.max_position_embeddings
        self.max_prompt_length = max_prompt_length if max_prompt_length is not None else self.max_length // 2
        self.truncation_mode = truncation_mode  # "keep_start" æˆ– "keep_end"

    def _smooth_clamp(self, x: torch.Tensor, min_val: float, max_val: float, temperature: float = 0.1) -> torch.Tensor:
        """
        å¹³æ»‘è£å‰ªå‡½æ•°ï¼Œåœ¨è¾¹ç•Œå¤„ä¿æŒå°æ¢¯åº¦ï¼Œé¿å…æ¢¯åº¦çªç„¶æˆªæ–­ã€‚
        
        ä½¿ç”¨ tanh-based å¹³æ»‘å‡½æ•°ï¼š
        - åœ¨è¾¹ç•Œå¤„æ¢¯åº¦è¿ç»­ï¼Œä¸ä¼šçªç„¶å˜ä¸º0
        - temperature æ§åˆ¶å¹³æ»‘ç¨‹åº¦ï¼ˆè¶Šå°è¶Šæ¥è¿‘ç¡¬è£å‰ªï¼‰
        """
        # å°†è¾“å…¥å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
        normalized = 2.0 * (x - min_val) / (max_val - min_val) - 1.0
        # ä½¿ç”¨ tanh è¿›è¡Œå¹³æ»‘è£å‰ª
        clipped_normalized = torch.tanh(normalized / temperature)
        # æ˜ å°„å›åŸå§‹èŒƒå›´
        return (clipped_normalized + 1.0) / 2.0 * (max_val - min_val) + min_val
    
    def _get_log_probs(self, model: AutoModelForCausalLM, prompts: List[str], responses: List[str]) -> tuple[torch.Tensor, torch.Tensor]:
        original_padding_side = self.processor.padding_side        
        self.processor.padding_side = 'right' 
        
        # 1. å…ˆ tokenize prompt å’Œ response è·å–é•¿åº¦ï¼ˆä¸æˆªæ–­ï¼Œä¸æ·»åŠ ç‰¹æ®Štokenï¼‰
        prompt_tokens_list = self.processor(prompts, padding=False, truncation=False, add_special_tokens=False)
        response_tokens_list = self.processor(responses, padding=False, truncation=False, add_special_tokens=False)
        
        # 2. å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œæˆªæ–­å¤„ç†ï¼ˆä»¿ç…§ SimPO çš„é€»è¾‘ï¼‰
        all_input_ids = []
        all_attention_masks = []
        prompt_lengths = []
        
        for i, (prompt_tokens, response_tokens) in enumerate(zip(prompt_tokens_list['input_ids'], response_tokens_list['input_ids'])):
            prompt_len = len(prompt_tokens)
            response_len = len(response_tokens)
            longer_response_length = response_len
            
            # å¦‚æœ combined sequence å¤ªé•¿ï¼Œå…ˆæˆªæ–­ prompt
            if prompt_len + longer_response_length > self.max_length:
                if self.truncation_mode == "keep_start":
                    # ä¿ç•™å¼€å¤´éƒ¨åˆ†
                    prompt_tokens = prompt_tokens[:self.max_prompt_length]
                elif self.truncation_mode == "keep_end":
                    # ä¿ç•™ç»“å°¾éƒ¨åˆ†
                    prompt_tokens = prompt_tokens[-self.max_prompt_length:] if len(prompt_tokens) > self.max_prompt_length else prompt_tokens
                else:
                    raise ValueError(f"Unknown truncation mode: {self.truncation_mode}")
                prompt_len = len(prompt_tokens)
            
            # å¦‚æœæˆªæ–­ prompt åè¿˜æ˜¯å¤ªé•¿ï¼Œæˆªæ–­ response
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ max_prompt_length è€Œä¸æ˜¯å®é™…çš„ prompt_lenï¼Œä¸ SimPO ä¿æŒä¸€è‡´
            if prompt_len + longer_response_length > self.max_length:
                max_response_len = self.max_length - self.max_prompt_length
                response_tokens = response_tokens[:max_response_len] if max_response_len > 0 else []
                response_len = len(response_tokens)
            
            # ç›´æ¥æ‹¼æ¥ prompt å’Œ response tokensï¼ˆä¸æ·»åŠ ç‰¹æ®Štokenï¼Œå› ä¸ºå·²ç»åœ¨åŸå§‹æ–‡æœ¬ä¸­ï¼‰
            full_input_ids = prompt_tokens + response_tokens
            full_attention_mask = [1] * len(full_input_ids)
            
            all_input_ids.append(full_input_ids)
            all_attention_masks.append(full_attention_mask)
            prompt_lengths.append(prompt_len)
        
        # 3. Paddingï¼ˆright paddingï¼‰
        max_seq_len = max(len(ids) for ids in all_input_ids)
        pad_token_id = self.processor.pad_token_id if self.processor.pad_token_id is not None else 0
        
        padded_input_ids = []
        padded_attention_masks = []
        for input_ids, attention_mask in zip(all_input_ids, all_attention_masks):
            pad_length = max_seq_len - len(input_ids)
            padded_input_ids.append(input_ids + [pad_token_id] * pad_length)
            padded_attention_masks.append(attention_mask + [0] * pad_length)
        
        # 4. è½¬æ¢ä¸º tensor
        input_ids = torch.tensor(padded_input_ids, dtype=torch.long, device=self.args.device)
        attention_mask = torch.tensor(padded_attention_masks, dtype=torch.long, device=self.args.device)
        
        # 5. Forward
        outputs = model(input_ids, attention_mask=attention_mask, return_dict=True, use_cache=False)
        logits = outputs.logits
        
        # 6. Shift logits/labels
        shifted_logits = logits[..., :-1, :]
        shifted_labels = input_ids[..., 1:]
        
        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
        nll_per_token = loss_fct(shifted_logits.reshape(-1, shifted_logits.size(-1)), shifted_labels.reshape(-1))
        nll_per_token = nll_per_token.view(input_ids.size(0), -1)
        log_probs_per_token = -nll_per_token
        
        # 7. Create Mask
        seq_len = shifted_labels.size(1)
        position_ids = torch.arange(seq_len, device=self.args.device).expand_as(shifted_labels)
        prompt_lengths_tensor = torch.tensor(prompt_lengths, device=self.args.device).unsqueeze(1)
        
        # Mask é€»è¾‘ï¼šä¿ç•™ index >= (prompt_len - 1) çš„éƒ¨åˆ†
        # å› ä¸º padding_side='right'ï¼Œæ‰€ä»¥ index 0 ç¡®å®æ˜¯æ–‡æœ¬å¼€å¤´ï¼Œé€»è¾‘æˆç«‹ã€‚
        response_start_index = prompt_lengths_tensor - 1
        mask = position_ids >= response_start_index
        
        attention_mask_shifted = attention_mask[:, 1:].to(torch.bool)
        response_mask = mask & attention_mask_shifted
        
        masked_log_probs = log_probs_per_token * response_mask
        total_log_probs = masked_log_probs.sum(dim=1)
        
        response_lengths = response_mask.float().sum(dim=1)
        
        # æ¢å¤åŸæ¥çš„ padding_side (é€šå¸¸æ˜¯ left)
        self.processor.padding_side = original_padding_side
        
        return total_log_probs, response_lengths
    
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        prompts = inputs.pop("prompt")
        chosen_responses = inputs.pop("chosen")
        rejected_responses = inputs.pop("rejected")
        f_y1 = inputs.pop("reward_chosen").to(self.args.device)
        f_y2 = inputs.pop("reward_rejected").to(self.args.device)

        batch_size = len(prompts)
        combined_prompts = prompts + prompts
        combined_responses = chosen_responses + rejected_responses

        # --- è®¡ç®—ç­–ç•¥æ¨¡å‹çš„ log_probs å’Œ é•¿åº¦ ---
        all_log_probs_pi, all_lengths = self._get_log_probs(model, combined_prompts, combined_responses)
        log_probs_pi_y1 = all_log_probs_pi[:batch_size]
        log_probs_pi_y2 = all_log_probs_pi[batch_size:]
        len_y1 = all_lengths[:batch_size] 
        len_y2 = all_lengths[batch_size:]

        # --- åŠ¨æ€è®¡ç®—å‚è€ƒæ¨¡å‹çš„ log_probs ---
        with torch.no_grad():
            # refæ¨¡å‹ä¸éœ€è¦é•¿åº¦ï¼Œç”¨ _ å ä½
            all_log_probs_ref, _ = self._get_log_probs(self.ref_model, combined_prompts, combined_responses)
            log_probs_ref_y1 = all_log_probs_ref[:batch_size]
            log_probs_ref_y2 = all_log_probs_ref[batch_size:]

        if self.state.global_step == 0:
            print(f"--- Sanity Check at Step 0 ---")
            print(f"Sample 0 - Length Chosen:   {len_y1[0].item()}")
            print(f"Sample 0 - Length Rejected: {len_y2[0].item()}")
            print(f"---------------------------------")

        log_ratio_y1 = log_probs_pi_y1 - log_probs_ref_y1
        log_ratio_y2 = log_probs_pi_y2 - log_probs_ref_y2
        
        with torch.no_grad():
            kl_term1 = self.beta_kl * log_ratio_y1.detach()
            kl_term2 = self.beta_kl * log_ratio_y2.detach()
            
            len_y1 = torch.clamp(len_y1, min=1.0)
            len_y2 = torch.clamp(len_y2, min=1.0)
            
            weight1 = (1 - f_y2 - kl_term1) / len_y1
            weight2 = (f_y1 + kl_term2) / len_y2
            
        clamped_log_ratio_y1 = torch.clamp(log_ratio_y1, min=self.log_ratio_clip_min, max=self.log_ratio_clip_max)
        ratio_y1 = torch.exp(clamped_log_ratio_y1)
        clamped_log_ratio_y2 = torch.clamp(log_ratio_y2, min=self.log_ratio_clip_min, max=self.log_ratio_clip_max)
        ratio_y2 = torch.exp(clamped_log_ratio_y2)
        
        J_sym_objective = ratio_y1 * weight1 - ratio_y2 * weight2
        total_loss = -J_sym_objective.mean()
        
        logs = {
            "mean_ratio_chosen": ratio_y1.detach().mean().item(),
            "mean_ratio_rejected": ratio_y2.detach().mean().item(),
            "weight_chosen": weight1.detach().mean().item(),
            "weight_rejected": weight2.detach().mean().item(),
            "kl_term_chosen": kl_term1.detach().mean().item(),
            "mean_len_chosen": len_y1.detach().mean().item(),
            "mean_len_rejected": len_y2.detach().mean().item()
        }
        self._current_logs = logs
        
        return total_loss


    # log æ–¹æ³•æ— éœ€ä¿®æ”¹
    def log(self, logs: dict, *args, **kwargs) -> None:
        if hasattr(self, "_current_logs") and self._current_logs is not None:
            logs.update(self._current_logs)
            self._current_logs = None
        super().log(logs, *args, **kwargs)

# Data Collator å’Œ Callback æ— éœ€ä¿®æ”¹
@dataclass
class DataCollatorForCustomSymPO:
    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        batch = {}
        for key in features[0].keys():
            # ç§»é™¤ ref_logp ç›¸å…³å­—æ®µçš„å¤„ç†ï¼Œå› ä¸ºå®ƒä»¬å·²ä¸å­˜åœ¨
            if key.startswith("ref_logp"):
                continue
            if isinstance(features[0][key], str):
                batch[key] = [feature[key] for feature in features]
            else:
                batch[key] = torch.tensor([feature[key] for feature in features])
        return batch

class PrintingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero and logs:
            print(logs)

# argparse å‡½æ•°æ— éœ€ä¿®æ”¹
def parse_args():
    parser = argparse.ArgumentParser(description="Train a policy model with SymPO algorithm.")
    
    # --- è·¯å¾„å‚æ•° ---
    parser.add_argument("--sft_model_path", type=str, default="/train/Llama-3-8B-Instruct", help="Path to the SFT base model.")
    parser.add_argument("--preprocessed_dataset_path", type=str, default="/train/precomputed_traindataset", help="Path to the precomputed dataset.")
    parser.add_argument("--output_dir", type=str, default="/train/output_model/llama3-8b-sympo-1e-6_0.5_length_seed_42_2048", help="Directory to save checkpoints and final model.")
    
    # --- è®­ç»ƒè¶…å‚æ•° ---
    parser.add_argument("--learning_rate", type=float, default=1e-6, help="Learning rate.")
    parser.add_argument("--num_train_epochs", type=int, default=1, help="Number of training epochs.")
    parser.add_argument("--per_device_train_batch_size", type=int, default=1, help="Batch size per GPU.")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4, help="Number of gradient accumulation steps.")
    parser.add_argument("--gradient_checkpointing", action='store_true', help="Enable gradient checkpointing to save memory.")
    parser.add_argument("--warmup_ratio", type=float, default=0.1, help="Warmup ratio for learning rate scheduler.")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="Max gradient norm for clipping.")
    parser.add_argument("--logging_steps", type=int, default=10, help="Log metrics every N steps.")
    parser.add_argument("--save_steps", type=int, default=10000, help="Save a checkpoint every N steps.")
    parser.add_argument("--save_total_limit", type=int, default=20, help="Limit the total number of saved checkpoints.")

    # --- SymPO ç‰¹å®šå‚æ•° ---
    parser.add_argument("--beta_kl", type=float, default=0.5, help="KL divergence penalty coefficient.")
    parser.add_argument("--log_ratio_clip_min", type=float, default=-2.3, help="Minimum clip value for log probability ratio.")
    parser.add_argument("--log_ratio_clip_max", type=float, default=2.3, help="Maximum clip value for log probability ratio.")
    parser.add_argument("--use_smooth_clip", action='store_true', help="Use smooth clipping for better gradient stability (recommended).")
    
    # --- æˆªæ–­å‚æ•°ï¼ˆä»¿ç…§ SimPOï¼‰ ---
    parser.add_argument("--max_length", type=int, default=2048, help="Maximum length of the combined prompt+response sequence. If None, uses model's max_position_embeddings.")
    parser.add_argument("--max_prompt_length", type=int, default=1800, help="Maximum length of the prompt. If None, uses max_length // 2.")
    parser.add_argument("--truncation_mode", type=str, default="keep_end", choices=["keep_start", "keep_end"], help="Truncation mode: 'keep_start' keeps the beginning of prompt, 'keep_end' keeps the end of prompt.")
    
    # --- W&B (Weights & Biases) æ—¥å¿—å‚æ•° ---
    parser.add_argument("--report_to", type=str, default="wandb", help="The integration to report results to (e.g., 'wandb').")
    parser.add_argument("--run_name", type=str, default=f"policy-llama3-8b-sympo-default", help="A name for the W&B run.")
    
    # --- éšæœºç§å­å‚æ•° ---
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility .")

    return parser.parse_args()


def main():
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    print(f"è®¾ç½®éšæœºç§å­ä¸º: {args.seed}")
    seed_everything(args.seed)

    tokenizer = AutoTokenizer.from_pretrained(args.sft_model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left' 

    print("ä»ç£ç›˜åŠ è½½æ•°æ®é›† (æ³¨æ„: æ•°æ®é›†ä¸­ä¸å†éœ€è¦ ref_logp)...")
    precomputed_dataset_train = load_from_disk(args.preprocessed_dataset_path)

    print("åŠ è½½ç”¨äºè®­ç»ƒçš„ç­–ç•¥æ¨¡å‹ (Policy Model)...")
    policy_model = AutoModelForCausalLM.from_pretrained(
        args.sft_model_path, dtype=torch.bfloat16
    )

    # 5. æ–°å¢ï¼šåŠ è½½ä½œä¸ºå‚è€ƒçš„SFTæ¨¡å‹ (Reference Model)
    print("åŠ è½½ä½œä¸ºå‚è€ƒçš„SFTæ¨¡å‹ (Reference Model)...")
    ref_model = AutoModelForCausalLM.from_pretrained(
        args.sft_model_path, dtype=torch.bfloat16
    )
    # # å°†å‚è€ƒæ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå¹¶ä¸”ä¸éœ€è¦è®¡ç®—å®ƒçš„æ¢¯åº¦
    # ref_model.eval()

    # TrainingArguments å®šä¹‰æ— éœ€ä¿®æ”¹
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        # gradient_checkpointing=args.gradient_checkpointing,
        gradient_checkpointing=False,
        gradient_checkpointing_kwargs={'use_reentrant': False},
        optim="adamw_torch",
        learning_rate=args.learning_rate,
        max_grad_norm=args.max_grad_norm,
        warmup_ratio=args.warmup_ratio,
        lr_scheduler_type="cosine",
        logging_strategy="steps",
        logging_steps=args.logging_steps,
        logging_first_step=True,
        bf16=True,
        save_strategy="steps",
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit, 
        eval_strategy="no",
        remove_unused_columns=False,
        report_to=args.report_to,
        run_name=args.run_name,
    )
    
    data_collator = DataCollatorForCustomSymPO()

    print("åˆå§‹åŒ– CustomSymPOTrainer...")
    trainer = CustomSymPOTrainer(
        model=policy_model,
        ref_model=ref_model, # 6. å°† ref_model ä¼ é€’ç»™ Trainer
        args=training_args,
        tokenizer=tokenizer,
        train_dataset=precomputed_dataset_train,
        data_collator=data_collator,
        callbacks=[PrintingCallback()],
        beta_kl=args.beta_kl,
        log_ratio_clip_min=args.log_ratio_clip_min,
        log_ratio_clip_max=args.log_ratio_clip_max,
        max_length=args.max_length,
        max_prompt_length=args.max_prompt_length,
        truncation_mode=args.truncation_mode,
    )
    # è®¾ç½®æ˜¯å¦ä½¿ç”¨å¹³æ»‘è£å‰ª
    trainer.use_smooth_clip = args.use_smooth_clip
    # ç§»é™¤ sft_model_pathï¼Œå› ä¸ºå®ƒå·²ç»é€šè¿‡ ref_model å¯¹è±¡ä¼ å…¥äº†
    # sft_model_path=args.sft_model_path, 

        # è®¾ç½®æ˜¯å¦ä½¿ç”¨å¹³æ»‘è£å‰ª
    trainer.use_smooth_clip = args.use_smooth_clip
    # ç§»é™¤ sft_model_pathï¼Œå› ä¸ºå®ƒå·²ç»é€šè¿‡ ref_model å¯¹è±¡ä¼ å…¥äº†
    # sft_model_path=args.sft_model_path, 

    print("å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒ...")
    train_result = trainer.train()
    print("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")

    ##################################
    # Save final model (HuggingFace format)
    ##################################
    print("\n" + "=" * 60)
    print("*** Save final model ***")
    print("=" * 60)
    print(f"âš ï¸  æ³¨æ„: å¦‚æœä½¿ç”¨ FSDPï¼Œæ­¤æ­¥éª¤ä¼šè‡ªåŠ¨åˆå¹¶åˆ†ç‰‡æƒé‡å¹¶ä¿å­˜ä¸º HuggingFace æ ¼å¼")
    print(f"ä¿å­˜è·¯å¾„: {args.output_dir}")
    
    # ç¡®ä¿ FSDP ä½¿ç”¨ FULL_STATE_DICT æ¨¡å¼ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if trainer.is_fsdp_enabled and trainer.accelerator.state.fsdp_plugin is not None:
        print("æ£€æµ‹åˆ° FSDPï¼Œè®¾ç½®çŠ¶æ€å­—å…¸ç±»å‹ä¸º FULL_STATE_DICT...")
        trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆè‡ªåŠ¨å¤„ç† FSDP åˆå¹¶ï¼‰
    trainer.save_model(args.output_dir)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_dir}")
    
    # ä¿å­˜ tokenizer
    try:
        tokenizer.save_pretrained(args.output_dir)
        print("âœ… Tokenizer å·²ä¿å­˜")
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: æ— æ³•è‡ªåŠ¨ä¿å­˜ tokenizer: {e}")
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    if trainer.accelerator.is_main_process:
        trainer.log_metrics("train", train_result.metrics)
        trainer.save_metrics("train", train_result.metrics)
        print("âœ… è®­ç»ƒæŒ‡æ ‡å·²ä¿å­˜")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå’Œæ¨¡å‹ä¿å­˜å®Œæˆï¼")
    print("=" * 60)
    print(f"\næœ€ç»ˆ HuggingFace æ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_dir}")
    print("å¯ä»¥ç›´æ¥ä½¿ç”¨ HuggingFace çš„ from_pretrained() åŠ è½½æ¨¡å‹")
    print(f"\næ³¨æ„: checkpoint-* ç›®å½•æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸­é—´æ£€æŸ¥ç‚¹ï¼ˆFSDP åˆ†ç‰‡æ ¼å¼ï¼‰")
    print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨ {args.output_dir} æ ¹ç›®å½•ï¼ˆHuggingFace æ ¼å¼ï¼‰")
if __name__ == "__main__":
    main()