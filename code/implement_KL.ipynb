{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Lj5_x_PXQV"
      },
      "outputs": [],
      "source": [
        "# !pip install torch transformers datasets peft bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMokQmH9PYWG"
      },
      "outputs": [],
      "source": [
        "# !pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "djfcDMXMPaH_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/advan/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import bitsandbytes\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from typing import Dict, Any, List, Union\n",
        "from tqdm import tqdm\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FZr89svxPfq8"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# CustomSymPOTrainer 类 non-unsloth\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "class CustomSymPOTrainer(Trainer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Union[PeftModel, torch.nn.Module],\n",
        "        ref_model: Union[PeftModel, torch.nn.Module],\n",
        "        reward_model: Union[PeftModel, torch.nn.Module],\n",
        "        args: TrainingArguments,\n",
        "        beta_kl: float,\n",
        "        log_ratio_clip_min: float,\n",
        "        log_ratio_clip_max: float,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        用于 SymPO 算法的自定义 Trainer。\n",
        "\n",
        "        Args:\n",
        "            model (PeftModel): 策略模型 (policy_model)。\n",
        "            ref_model (PeftModel): 参考模型 (ref_model)。\n",
        "            reward_model (torch.nn.Module): 奖励模型 (reward_model)。\n",
        "            args (TrainingArguments): 训练参数。\n",
        "            beta_kl (float): KL散度惩罚项的系数。\n",
        "            log_ratio_clip_min (float): 对数概率比的最小裁剪值。\n",
        "            log_ratio_clip_max (float): 对数概率比的最大裁剪值。\n",
        "        \"\"\"\n",
        "        super().__init__(model=model, args=args, **kwargs)\n",
        "\n",
        "        if not hasattr(self, 'processor') or self.processor is None:\n",
        "            self.processor = self.tokenizer\n",
        "\n",
        "        self.ref_model = ref_model\n",
        "        self.reward_model = reward_model\n",
        "        self.beta_kl = beta_kl\n",
        "        self.log_ratio_clip_min = log_ratio_clip_min\n",
        "        self.log_ratio_clip_max = log_ratio_clip_max\n",
        "\n",
        "    def _get_log_probs(self, model: AutoModelForCausalLM, prompts: List[str], responses: List[str]) -> torch.Tensor:\n",
        "        full_texts = [p + r for p, r in zip(prompts, responses)]\n",
        "\n",
        "        original_padding_side = self.processor.padding_side\n",
        "        self.processor.padding_side = 'left'\n",
        "\n",
        "        prompt_tokens = self.processor(prompts, padding=False, truncation=False)\n",
        "        prompt_lengths = [len(p) for p in prompt_tokens['input_ids']]\n",
        "\n",
        "        max_len = model.config.max_position_embeddings\n",
        "\n",
        "        full_tokens = self.processor(\n",
        "            full_texts, padding=True, truncation=True,\n",
        "            max_length=max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        self.processor.padding_side = original_padding_side\n",
        "\n",
        "        input_ids = full_tokens['input_ids'].to(self.args.device)\n",
        "        attention_mask = full_tokens['attention_mask'].to(self.args.device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        shifted_logits = logits[..., :-1, :]\n",
        "        shifted_labels = input_ids[..., 1:]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "        nll_per_token = loss_fct(shifted_logits.reshape(-1, shifted_logits.size(-1)), shifted_labels.reshape(-1))\n",
        "        nll_per_token = nll_per_token.view(input_ids.size(0), -1)\n",
        "        log_probs_per_token = -nll_per_token\n",
        "\n",
        "        # response_mask = torch.zeros_like(shifted_labels, dtype=torch.bool)\n",
        "        # for i, prompt_len in enumerate(prompt_lengths):\n",
        "        #     actual_prompt_len = min(prompt_len, max_len)\n",
        "        #     response_start_index = actual_prompt_len - 1\n",
        "\n",
        "        #     actual_seq_len = attention_mask[i].sum()\n",
        "        #     response_end_index = actual_seq_len -1\n",
        "\n",
        "        #     if response_start_index < response_end_index:\n",
        "        #          response_mask[i, response_start_index:response_end_index] = True\n",
        "\n",
        "        seq_len = shifted_labels.size(1)\n",
        "        position_ids = torch.arange(seq_len, device=self.args.device).expand_as(shifted_labels)\n",
        "        prompt_lengths_tensor = torch.tensor(prompt_lengths, device=self.args.device).unsqueeze(1)\n",
        "        response_start_index = prompt_lengths_tensor - 1\n",
        "        mask = position_ids >= response_start_index\n",
        "        attention_mask_shifted = attention_mask[:, 1:].to(torch.bool)\n",
        "        response_mask = mask & attention_mask_shifted\n",
        "\n",
        "        masked_log_probs = log_probs_per_token * response_mask\n",
        "        total_log_probs = masked_log_probs.sum(dim=1)\n",
        "        del logits, shifted_logits, shifted_labels, nll_per_token, log_probs_per_token, response_mask, masked_log_probs\n",
        "\n",
        "        return total_log_probs\n",
        "\n",
        "    def _get_rewards(self, prompts: List[str], responses: List[str]) -> torch.Tensor:\n",
        "        texts = [p + r for p, r in zip(prompts, responses)]\n",
        "        original_padding_side = self.processor.padding_side\n",
        "        self.processor.padding_side = 'right'\n",
        "        reward_max_len = self.reward_model.config.max_position_embeddings\n",
        "        inputs = self.processor(\n",
        "            texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=reward_max_len\n",
        "        ).to(self.args.device)\n",
        "        self.processor.padding_side = original_padding_side\n",
        "        with torch.no_grad():\n",
        "            outputs = self.reward_model(**inputs)\n",
        "            return torch.sigmoid(outputs.logits.squeeze(-1))\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        prompts = inputs.pop(\"prompt\")\n",
        "        chosen_responses = inputs.pop(\"chosen\")\n",
        "        rejected_responses = inputs.pop(\"rejected\")\n",
        "\n",
        "        # y1s = chosen_responses\n",
        "        # y2s = rejected_responses\n",
        "        y1s, y2s, zs = [], [], []\n",
        "        for j in range(len(prompts)):\n",
        "            if random.random() < 0.5:\n",
        "                y1s.append(chosen_responses[j]); y2s.append(rejected_responses[j]); zs.append(1.0)\n",
        "            else:\n",
        "                y1s.append(rejected_responses[j]); y2s.append(chosen_responses[j]); zs.append(0.0)\n",
        "        zs = torch.tensor(zs).to(self.args.device)\n",
        "\n",
        "        log_probs_pi_y1 = self._get_log_probs(model, prompts, y1s)\n",
        "        log_probs_pi_y2 = self._get_log_probs(model, prompts, y2s)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with self.model.disable_adapter():\n",
        "                log_probs_ref_y1 = self._get_log_probs(self.model, prompts, y1s)\n",
        "                log_probs_ref_y2 = self._get_log_probs(self.model, prompts, y2s)\n",
        "\n",
        "        log_ratio_y1 = log_probs_pi_y1 - log_probs_ref_y1\n",
        "        log_ratio_y2 = log_probs_pi_y2 - log_probs_ref_y2\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # f_y1 = self._get_rewards(prompts, y1s)\n",
        "            # f_y2 = self._get_rewards(prompts, y2s)\n",
        "\n",
        "            log_ratio_weight_y1 = log_ratio_y1.detach()\n",
        "            log_ratio_weight_y2 = log_ratio_y2.detach()\n",
        "\n",
        "            kl_term1 = self.beta_kl * log_ratio_weight_y1\n",
        "            kl_term2 = self.beta_kl * log_ratio_weight_y2\n",
        "\n",
        "            # weight1 = 1 - f_y2 - kl_term1\n",
        "            # weight2 = f_y1 + kl_term2\n",
        "            weight1 = zs - kl_term1\n",
        "            weight2 = zs + kl_term2\n",
        "\n",
        "        clamped_log_ratio_y1 = torch.clamp(log_ratio_y1, min=self.log_ratio_clip_min, max=self.log_ratio_clip_max)\n",
        "        ratio_y1 = torch.exp(clamped_log_ratio_y1)\n",
        "\n",
        "        clamped_log_ratio_y2 = torch.clamp(log_ratio_y2, min=self.log_ratio_clip_min, max=self.log_ratio_clip_max)\n",
        "        ratio_y2 = torch.exp(clamped_log_ratio_y2)\n",
        "\n",
        "        J_sym_objective = ratio_y1 * weight1 - ratio_y2 * weight2\n",
        "        total_loss = -J_sym_objective.mean()\n",
        "\n",
        "        logs = {\n",
        "            \"mean_ratio_chosen\": ratio_y1.detach().mean().item(),\n",
        "            \"mean_ratio_rejected\": ratio_y2.detach().mean().item(),\n",
        "            \"mean_weight_chosen\": weight1.detach().mean().item(),\n",
        "            \"mean_weight_rejected\": weight2.detach().mean().item(),\n",
        "            \"mean_logprob_pi_chosen\": log_probs_pi_y1.detach().mean().item(),\n",
        "            \"mean_logprob_ref_chosen\": log_probs_ref_y1.detach().mean().item(),\n",
        "            # \"f_chosen\": f_y1.detach().mean().item(),\n",
        "            # \"f_rejected\": f_y2.detach().mean().item(),\n",
        "            \"debug_ratio_chosen_max\": ratio_y1.detach().max().item(),\n",
        "            \"debug_ratio_rejected_max\": ratio_y2.detach().max().item(),\n",
        "            \"debug_weight_chosen_max\": weight1.detach().max().item(),\n",
        "            \"debug_weight_rejected_max\": weight2.detach().max().item(),\n",
        "            \"debug_logprob_pi_chosen_max\": log_probs_pi_y1.detach().max().item(),\n",
        "            \"debug_logprob_ref_chosen_max\": log_probs_ref_y1.detach().max().item(),\n",
        "            \"debug_objective_mean\": J_sym_objective.detach().mean().item()\n",
        "        }\n",
        "        self._current_logs = logs\n",
        "\n",
        "        if self.args.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def log(self, logs: dict, *args, **kwargs) -> None:\n",
        "        if hasattr(self, \"_current_logs\") and self._current_logs is not None:\n",
        "            logs.update(self._current_logs)\n",
        "            self._current_logs = None\n",
        "\n",
        "        super().log(logs, *args, **kwargs)\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        eval_dataset=None,\n",
        "        ignore_keys=None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "    ):\n",
        "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "        if eval_dataset is None:\n",
        "            raise ValueError(\"Evaluation requires an eval_dataset.\")\n",
        "\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        self.model.eval()\n",
        "\n",
        "        # 【优化】不再使用列表存储所有结果，而是使用累加器\n",
        "        total_samples = 0\n",
        "        total_accuracy_count = 0\n",
        "        total_policy_margins = 0\n",
        "        total_rewards_chosen = 0\n",
        "        total_rewards_rejected = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "                prompts = batch[\"prompt\"]\n",
        "                chosen_responses = batch[\"chosen\"]\n",
        "                rejected_responses = batch[\"rejected\"]\n",
        "\n",
        "                batch_size = len(prompts)\n",
        "                total_samples += batch_size\n",
        "\n",
        "                # 计算对数概率和奖励\n",
        "                policy_chosen_logps = self._get_log_probs(self.model, prompts, chosen_responses)\n",
        "                policy_rejected_logps = self._get_log_probs(self.model, prompts, rejected_responses)\n",
        "                rewards_chosen = self._get_rewards(prompts, chosen_responses)\n",
        "                rewards_rejected = self._get_rewards(prompts, rejected_responses)\n",
        "\n",
        "                # 【优化】在每个批次上直接计算指标，然后累加\n",
        "                # 计算本批次的准确样本数\n",
        "                correct_predictions = (policy_chosen_logps > policy_rejected_logps).sum().item()\n",
        "                total_accuracy_count += correct_predictions\n",
        "\n",
        "                # 累加其他指标的总和\n",
        "                total_policy_margins += (policy_chosen_logps - policy_rejected_logps).sum().item()\n",
        "                total_rewards_chosen += rewards_chosen.sum().item()\n",
        "                total_rewards_rejected += rewards_rejected.sum().item()\n",
        "\n",
        "        # 【优化】在循环结束后，根据累加结果计算最终的平均指标\n",
        "        accuracy = total_accuracy_count / total_samples if total_samples > 0 else 0\n",
        "        policy_margins = total_policy_margins / total_samples if total_samples > 0 else 0\n",
        "        rewards_chosen_mean = total_rewards_chosen / total_samples if total_samples > 0 else 0\n",
        "        rewards_rejected_mean = total_rewards_rejected / total_samples if total_samples > 0 else 0\n",
        "        reward_margins = rewards_chosen_mean - rewards_rejected_mean\n",
        "\n",
        "        metrics = {\n",
        "            f\"{metric_key_prefix}_accuracy\": accuracy,\n",
        "            f\"{metric_key_prefix}_policy_margins\": policy_margins,\n",
        "            f\"{metric_key_prefix}_rewards_chosen_mean\": rewards_chosen_mean,\n",
        "            f\"{metric_key_prefix}_rewards_rejected_mean\": rewards_rejected_mean,\n",
        "            f\"{metric_key_prefix}_rewards_margins\": reward_margins,\n",
        "        }\n",
        "\n",
        "        self.model.train()\n",
        "        self.log(metrics)\n",
        "        return metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cGWNxj6STK3n"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForCustomSymPO:\n",
        "    \"\"\"\n",
        "    一个简单的 Data Collator，它将一批字典（每个包含字符串）\n",
        "    转换为一个包含字符串列表的字典。\n",
        "    \"\"\"\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        # 'features' 是一个列表，列表中的每个元素都是一个数据集样本，\n",
        "        # 例如: [{'prompt': 'p1', 'chosen': 'c1', 'rejected': 'r1'}, {'prompt': 'p2', ...}]\n",
        "\n",
        "        # 初始化一个空字典来存放批次数据\n",
        "        batch = {}\n",
        "\n",
        "        # 遍历第一个样本的所有键\n",
        "        for key in features[0].keys():\n",
        "            # 为每个键创建一个列表，包含该批次所有样本中该键对应的值\n",
        "            batch[key] = [feature[key] for feature in features]\n",
        "\n",
        "        # 返回的结果\n",
        "        # 例如: {'prompt': ['p1', 'p2'], 'chosen': ['c1', 'c2'], 'rejected': ['r1', 'r2']}\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OefWKV85YKut"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class PrintingCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if state.is_local_process_zero and logs:\n",
        "            print(logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5q-4ByqWYOsz"
      },
      "outputs": [],
      "source": [
        "# 1. 定义路径\n",
        "sft_model_path = \"/train/Llama-3-8B-Instruct\"\n",
        "# sft_model_path = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "# reward_model_path = \"/content/drive/My Drive/Colab_outputs/f_model_llama3_8b_lr2e-4_ultrafeedback_armorm\"\n",
        "reward_model_path =\"/train/f_model\"\n",
        "dataset_train_name = \"/train/traindataset\"\n",
        "# dataset_test_name = \"/content/drive/MyDrive/Policy_Dataset/Data_Test_generation/datasets/llama3_ultrafeedback_armorm\"\n",
        "output_dir  = \"/train/output_model/llama3-8b-sympo-5e-5_0.1\"\n",
        "device_map = \"auto\"\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "lr = 5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "415bd7890d414750b40a37f29021b4b2",
            "a0389836bf0849e7b240b387a3eb8326",
            "2e711b1d8d3840cc815eea7c92635126",
            "50a80b48efc94855b254f06870f401f9",
            "c0f3572168a24856a7efbd71d160f6f3",
            "17497d002c8c4815b1195ba29d01c658",
            "6d8713e140bf4d42bef4f516c92e56e1",
            "b0015e340b4143b79e86013ee412ba46",
            "c5ed2bbc6f774c13833afdd652b58ed4",
            "a4e82862bfbe410eb344d714fda34058",
            "ed1dc297703d4cd6b153977f83fcb00d",
            "d1669527c6e6478c8ac45d26d42cb615",
            "1bd2f35def6a44d68c5b673666a20c72",
            "7ec2a848a0374d8e8a2dc4ce9e3ffb67",
            "ae015869cc824c2e81d51c90ca0f051f",
            "daab7a72be6d4a798f85611ab6452845",
            "7c04bdd4e924443a99ccd32f8f99d931",
            "d1fbff2e6a764c0f846a12ac44953784",
            "25a46d3bc2f74fcbbc5c70241c33b306",
            "c0d6d37ccf154e2b8441e8ea446aec5f",
            "9487db326e4b4db0a7418695e32b3e08",
            "32e93f32ed864dbb868a51dba727ac75"
          ]
        },
        "id": "5Q2u_QF1YRJb",
        "outputId": "b4608e4b-d08b-40ea-f593-babb819cba41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading policy model (for training)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading reward model (frozen)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\n",
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /train/f_model and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# 4-bit 量化配置\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# LoRA 配置\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loading policy model (for training)...\")\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(\n",
        "    sft_model_path,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=device_map,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "policy_model = prepare_model_for_kbit_training(policy_model)\n",
        "policy_model = get_peft_model(policy_model, lora_config)\n",
        "\n",
        "# print(\"Loading reference model (frozen)...\")\n",
        "# ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     sft_model_path,\n",
        "#     quantization_config=quantization_config,\n",
        "#     device_map=device_map,\n",
        "#     attn_implementation=\"flash_attention_2\",\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "# )\n",
        "print(\"Loading reward model (frozen)...\")\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    reward_model_path,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=device_map,\n",
        "    num_labels=1,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "if reward_model.config.pad_token_id is None:\n",
        "    reward_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     sft_model_path,\n",
        "#     num_labels=1,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     device_map=\"auto\"\n",
        "# )\n",
        "#\n",
        "# # 设置填充符号\n",
        "# if reward_model.config.pad_token_id is None:\n",
        "#     reward_model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# reward_model = PeftModel.from_pretrained(reward_model, reward_model_path)\n",
        "\n",
        "# for param in ref_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "for param in reward_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_train = load_from_disk(dataset_train_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFOXdC9nYUpy",
        "outputId": "34afe633-de10-4139-ef72-bf2af7f1b38e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing dataset...\n"
          ]
        }
      ],
      "source": [
        "def preprocess_ultrafeedback(example: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        example: 数据集中的一个样本字典。\n",
        "\n",
        "    Returns:\n",
        "        一个包含 'prompt', 'chosen', 'rejected' 键的新字典。\n",
        "    \"\"\"\n",
        "\n",
        "    chosen_response = example['chosen'][-1]['content']+ \"<|eot_id|>\\n\"\n",
        "    rejected_response = example['rejected'][-1]['content']+ \"<|eot_id|>\\n\"\n",
        "    conversation_history = example['chosen'][:-1]\n",
        "\n",
        "    full_prompt = tokenizer.apply_chat_template(\n",
        "        conversation_history,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    if not full_prompt or not chosen_response or not rejected_response:\n",
        "        return {\"prompt\": \"\", \"chosen\": \"\", \"rejected\": \"\"}\n",
        "\n",
        "    return {\n",
        "        \"prompt\": full_prompt,\n",
        "        \"chosen\": chosen_response,\n",
        "        \"rejected\": rejected_response\n",
        "    }\n",
        "\n",
        "\n",
        "dataset_train = load_from_disk(dataset_train_name)\n",
        "\n",
        "print(\"Preprocessing dataset...\")\n",
        "processed_dataset_train = dataset_train.map(\n",
        "    preprocess_ultrafeedback,\n",
        "    num_proc=4,\n",
        "    remove_columns=dataset_train.column_names\n",
        ")\n",
        "\n",
        "processed_dataset_train = processed_dataset_train.filter(\n",
        "    lambda x: x['prompt'] and x['chosen'] and x['rejected']\n",
        ")\n",
        "\n",
        "# dataset_test = load_from_disk(dataset_test_name)\n",
        "\n",
        "# print(\"Preprocessing dataset...\")\n",
        "# processed_dataset_test = dataset_test.map(\n",
        "#     preprocess_ultrafeedback,\n",
        "#     num_proc=4,\n",
        "#     remove_columns=dataset_test.column_names\n",
        "# )\n",
        "\n",
        "# processed_dataset_test = processed_dataset_test.filter(\n",
        "#     lambda x: x['prompt'] and x['chosen'] and x['rejected']\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjov5jIJYZH_",
        "outputId": "17f83939-7866-4f34-a9e8-fd20f4129900"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_190118/2046757411.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(model=model, args=args, **kwargs)\n",
            "WARNING:accelerate.utils.other:Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing CustomSymPOTrainer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    gradient_checkpointing=False,\n",
        "    # group_by_length=True, # 启用长度分组\n",
        "    # length_column_name=\"prompt_length\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    learning_rate=lr,\n",
        "    max_grad_norm=1.0,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,\n",
        "    bf16=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=300,\n",
        "    # eval_strategy=\"no\",\n",
        "    eval_strategy=\"no\",\n",
        "    remove_unused_columns=False,\n",
        "    save_total_limit=20,\n",
        "    # metric_for_best_model=\"eval_accuracy\",\n",
        "    # greater_is_better=True,\n",
        "    # remove_unused_columns=False,\n",
        ")\n",
        "data_collator = DataCollatorForCustomSymPO()\n",
        "printing_callback = PrintingCallback()\n",
        "\n",
        "print(\"Initializing CustomSymPOTrainer...\")\n",
        "trainer = CustomSymPOTrainer(\n",
        "    model=policy_model,\n",
        "    ref_model=policy_model,\n",
        "    reward_model=reward_model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=processed_dataset_train,\n",
        "    # eval_dataset=processed_dataset_test,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[PrintingCallback()],\n",
        "    # 传递自定义参数\n",
        "    beta_kl=0.1,\n",
        "    log_ratio_clip_min=-2.3,\n",
        "    log_ratio_clip_max=2.3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXS0AtbzYu2Z",
        "outputId": "fa8f9037-c7af-4371-8b69-77887d1baa0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzmisource\u001b[0m (\u001b[33mzmisource-zhejiang-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (0.0s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/rlhf_code/code/wandb/run-20251024_164252-exn60krz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zmisource-zhejiang-university/huggingface/runs/exn60krz' target=\"_blank\">robust-grass-164</a></strong> to <a href='https://wandb.ai/zmisource-zhejiang-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/zmisource-zhejiang-university/huggingface' target=\"_blank\">https://wandb.ai/zmisource-zhejiang-university/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/zmisource-zhejiang-university/huggingface/runs/exn60krz' target=\"_blank\">https://wandb.ai/zmisource-zhejiang-university/huggingface/runs/exn60krz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Casting fp32 inputs back to torch.bfloat16 for flash-attn compatibility.\n",
            "/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22' max='3753' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  22/3753 05:14 < 16:18:36, 0.06 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.013500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.112900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0, 'grad_norm': 69.2486572265625, 'learning_rate': 0.0, 'mean_ratio_chosen': 1.0, 'mean_ratio_rejected': 1.0, 'mean_weight_chosen': 0.0, 'mean_weight_rejected': 0.0, 'mean_logprob_pi_chosen': -280.10357666015625, 'mean_logprob_ref_chosen': -280.10357666015625, 'debug_ratio_chosen_max': 1.0, 'debug_ratio_rejected_max': 1.0, 'debug_weight_chosen_max': 0.0, 'debug_weight_rejected_max': 0.0, 'debug_logprob_pi_chosen_max': -280.10357666015625, 'debug_logprob_ref_chosen_max': -280.10357666015625, 'debug_objective_mean': 0.0, 'epoch': 0.0002664978846730404}\n",
            "{'loss': 0.0135, 'grad_norm': 42.615970611572266, 'learning_rate': 1.1968085106382979e-06, 'mean_ratio_chosen': 2.0598974227905273, 'mean_ratio_rejected': 0.9574445486068726, 'mean_weight_chosen': -0.072265625, 'mean_weight_rejected': -0.0043487548828125, 'mean_logprob_pi_chosen': -213.90457153320312, 'mean_logprob_ref_chosen': -214.62722778320312, 'debug_ratio_chosen_max': 2.0598974227905273, 'debug_ratio_rejected_max': 0.9574445486068726, 'debug_weight_chosen_max': -0.072265625, 'debug_weight_rejected_max': -0.0043487548828125, 'debug_logprob_pi_chosen_max': -213.90457153320312, 'debug_logprob_ref_chosen_max': -214.62722778320312, 'debug_objective_mean': -0.14469607174396515, 'epoch': 0.0026649788467304042}\n",
            "{'loss': 0.1129, 'grad_norm': 68.96123504638672, 'learning_rate': 2.526595744680851e-06, 'mean_ratio_chosen': 0.9403847455978394, 'mean_ratio_rejected': 0.6587445735931396, 'mean_weight_chosen': 0.006146621890366077, 'mean_weight_rejected': -0.04174194484949112, 'mean_logprob_pi_chosen': -46.892269134521484, 'mean_logprob_ref_chosen': -46.83080291748047, 'debug_ratio_chosen_max': 0.9403847455978394, 'debug_ratio_rejected_max': 0.6587445735931396, 'debug_weight_chosen_max': 0.006146621890366077, 'debug_weight_rejected_max': -0.04174194484949112, 'debug_logprob_pi_chosen_max': -46.892269134521484, 'debug_logprob_ref_chosen_max': -46.83080291748047, 'debug_objective_mean': 0.033277470618486404, 'epoch': 0.0053299576934608085}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# trainer.train(resume_from_checkpoint=True)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/advan/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/advan/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/advan/lib/python3.12/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/advan/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/advan/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/advan/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print(\"Starting training...\")\n",
        "# trainer.train(resume_from_checkpoint=True)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g991TbGmdrBH"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from peft import PeftModel\n",
        "# import os\n",
        "\n",
        "# # --- 1. 请在这里配置您的路径 ---\n",
        "\n",
        "# # 您微调时使用的原始基础模型的Hugging Face ID\n",
        "# # (根据您的目录名称，很可能就是这个)\n",
        "# base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# # 您的LoRA适配器所在的目录\n",
        "# adapter_path = \"/content/drive/My Drive/Colab_outputs/f_model_llama3_8b_lr2e-4_ultrafeedback_armorm\"\n",
        "\n",
        "# # 您想将合并后的完整模型保存到的【新】目录\n",
        "# merged_model_path = \"/content/drive/My Drive/Colab_outputs/merged_model_llama3_8b_ultrafeedback_new\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoH_RmFAduWC"
      },
      "outputs": [],
      "source": [
        "# # --- 2. 执行合并的代码 ---\n",
        "\n",
        "# # 确保目标目录存在\n",
        "# os.makedirs(merged_model_path, exist_ok=True)\n",
        "\n",
        "# print(f\"正在加载基础模型: {base_model_id}\")\n",
        "# # 加载基础模型，请确保您的Colab有足够的内存和显存\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     base_model_id,\n",
        "#     torch_dtype=torch.bfloat16, # 在T4/A100 GPU上建议使用 bfloat16\n",
        "#     device_map=\"auto\"\n",
        "# )\n",
        "\n",
        "# print(f\"正在加载分词器: {base_model_id}\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQIe8MMPdm7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# print(f\"正在加载LoRA适配器: {adapter_path}\")\n",
        "# # 将LoRA适配器加载到基础模型上\n",
        "# merged_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "# print(\"正在合并权重...\")\n",
        "# # 合并权重，然后卸载适配器，得到一个独立的、完整的模型\n",
        "# merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "# print(f\"正在将合并后的完整模型保存到: {merged_model_path}\")\n",
        "# # 将这个新模型和分词器一起保存到新目录\n",
        "# merged_model.save_pretrained(merged_model_path)\n",
        "# tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "# print(\"\\n模型合并并保存完毕！\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "advan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17497d002c8c4815b1195ba29d01c658": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bd2f35def6a44d68c5b673666a20c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c04bdd4e924443a99ccd32f8f99d931",
            "placeholder": "​",
            "style": "IPY_MODEL_d1fbff2e6a764c0f846a12ac44953784",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "25a46d3bc2f74fcbbc5c70241c33b306": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e711b1d8d3840cc815eea7c92635126": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0015e340b4143b79e86013ee412ba46",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5ed2bbc6f774c13833afdd652b58ed4",
            "value": 4
          }
        },
        "32e93f32ed864dbb868a51dba727ac75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "415bd7890d414750b40a37f29021b4b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0389836bf0849e7b240b387a3eb8326",
              "IPY_MODEL_2e711b1d8d3840cc815eea7c92635126",
              "IPY_MODEL_50a80b48efc94855b254f06870f401f9"
            ],
            "layout": "IPY_MODEL_c0f3572168a24856a7efbd71d160f6f3"
          }
        },
        "50a80b48efc94855b254f06870f401f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4e82862bfbe410eb344d714fda34058",
            "placeholder": "​",
            "style": "IPY_MODEL_ed1dc297703d4cd6b153977f83fcb00d",
            "value": " 4/4 [00:17&lt;00:00,  3.72s/it]"
          }
        },
        "6d8713e140bf4d42bef4f516c92e56e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c04bdd4e924443a99ccd32f8f99d931": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec2a848a0374d8e8a2dc4ce9e3ffb67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25a46d3bc2f74fcbbc5c70241c33b306",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0d6d37ccf154e2b8441e8ea446aec5f",
            "value": 4
          }
        },
        "9487db326e4b4db0a7418695e32b3e08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0389836bf0849e7b240b387a3eb8326": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17497d002c8c4815b1195ba29d01c658",
            "placeholder": "​",
            "style": "IPY_MODEL_6d8713e140bf4d42bef4f516c92e56e1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a4e82862bfbe410eb344d714fda34058": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae015869cc824c2e81d51c90ca0f051f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9487db326e4b4db0a7418695e32b3e08",
            "placeholder": "​",
            "style": "IPY_MODEL_32e93f32ed864dbb868a51dba727ac75",
            "value": " 4/4 [00:23&lt;00:00,  4.92s/it]"
          }
        },
        "b0015e340b4143b79e86013ee412ba46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0d6d37ccf154e2b8441e8ea446aec5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0f3572168a24856a7efbd71d160f6f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5ed2bbc6f774c13833afdd652b58ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1669527c6e6478c8ac45d26d42cb615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bd2f35def6a44d68c5b673666a20c72",
              "IPY_MODEL_7ec2a848a0374d8e8a2dc4ce9e3ffb67",
              "IPY_MODEL_ae015869cc824c2e81d51c90ca0f051f"
            ],
            "layout": "IPY_MODEL_daab7a72be6d4a798f85611ab6452845"
          }
        },
        "d1fbff2e6a764c0f846a12ac44953784": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daab7a72be6d4a798f85611ab6452845": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed1dc297703d4cd6b153977f83fcb00d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
