nohup: ignoring input
[W1215 12:22:40.907633191 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
设置随机种子为: 42
设置随机种子为: 42
设置随机种子为: 42
设置随机种子为: 42
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
加载用于训练的策略模型 (Policy Model)...
加载用于训练的策略模型 (Policy Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 89.63it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 91.86it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 93.32it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 91.30it/s]
加载作为参考的SFT模型 (Reference Model)...
加载作为参考的SFT模型 (Reference Model)...
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 91.40it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 93.83it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 94.77it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 94.04it/s]
[W1215 12:22:45.292106318 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1215 12:22:45.296164918 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1215 12:22:45.297694002 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1215 12:22:45.304899820 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_kl_term2.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_kl_term2.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...初始化 CustomSymPOTrainer...

/rlhf_code/code/implement_final_v3_no_kl_term2.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
/rlhf_code/code/implement_final_v3_no_kl_term2.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in LlamaForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in LlamaDecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1968: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.
  warnings.warn(
wandb: Tracking run with wandb version 0.21.3
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /wandb/offline-run-20251215_122256-2vdb9rm9
  0%|          | 0/3743 [00:00<?, ?it/s]--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -50.5
Sample 0 - pi_logp_chosen:  -50.17200469970703
Difference: 0.32799530029296875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -48.5
Sample 0 - pi_logp_chosen:  -48.19110107421875
Difference: 0.30889892578125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -72.5
Sample 0 - pi_logp_chosen:  -72.23435974121094
Difference: 0.2656402587890625
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -164.0
Sample 0 - pi_logp_chosen:  -164.82467651367188
Difference: -0.824676513671875
---------------------------------
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
--- Sanity Check at Step 0 ---
--- Sanity Check at Step 0 ---
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -202.0
Sample 0 - pi_logp_chosen:  -201.9729766845703
Difference: 0.0270233154296875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -210.0
Sample 0 - pi_logp_chosen:  -209.87814331054688
Difference: 0.121856689453125
---------------------------------
Sample 0 - ref_logp_chosen: -193.0
Sample 0 - pi_logp_chosen:  -191.9381103515625
Difference: 1.0618896484375
---------------------------------
Sample 0 - ref_logp_chosen: -340.0
Sample 0 - pi_logp_chosen:  -339.3873291015625
Difference: 0.6126708984375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -39.5
Sample 0 - pi_logp_chosen:  -39.76247787475586
Difference: -0.2624778747558594
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -250.0
Sample 0 - pi_logp_chosen:  -251.0724639892578
Difference: -1.0724639892578125
---------------------------------
--- Sanity Check at Step 0 ---
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -55.75
Sample 0 - pi_logp_chosen:  -56.33271026611328
Difference: -0.5827102661132812
---------------------------------
Sample 0 - ref_logp_chosen: -118.0
Sample 0 - pi_logp_chosen:  -117.84983825683594
Difference: 0.1501617431640625
---------------------------------
--- Sanity Check at Step 0 ---
--- Sanity Check at Step 0 ---
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -77.5
Sample 0 - ref_logp_chosen: -69.0Sample 0 - pi_logp_chosen:  -78.01214599609375
Sample 0 - pi_logp_chosen:  -69.32749938964844

Difference: -0.3274993896484375
---------------------------------
Sample 0 - ref_logp_chosen: -69.5
Difference: -0.51214599609375Sample 0 - pi_logp_chosen:  -69.90039825439453
Difference: -0.40039825439453125
---------------------------------

---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -89.0
Sample 0 - pi_logp_chosen:  -89.27076721191406
Difference: -0.2707672119140625
---------------------------------
  0%|          | 1/3743 [00:04<5:08:01,  4.94s/it]{'loss': 0.425, 'grad_norm': 2618.370849609375, 'learning_rate': 0.0, 'mean_ratio_chosen': 0.5992082953453064, 'mean_ratio_rejected': 1.012220025062561, 'weight_chosen': 0.7500918507575989, 'weight_rejected': 0.49713146686553955, 'kl_term_chosen': -0.256072998046875, 'epoch': 0.00026721891909947226}
                                                  {'loss': 0.425, 'grad_norm': 2618.370849609375, 'learning_rate': 0.0, 'mean_ratio_chosen': 0.5992082953453064, 'mean_ratio_rejected': 1.012220025062561, 'weight_chosen': 0.7500918507575989, 'weight_rejected': 0.49713146686553955, 'kl_term_chosen': -0.256072998046875, 'epoch': 0.0}
  0%|          | 1/3743 [00:04<5:08:01,  4.94s/it]  0%|          | 2/3743 [00:09<5:02:21,  4.85s/it]  0%|          | 3/3743 [00:15<5:15:59,  5.07s/it]  0%|          | 4/3743 [00:19<5:11:25,  5.00s/it]  0%|          | 5/3743 [00:24<5:06:22,  4.92s/it]  0%|          | 6/3743 [00:29<5:07:03,  4.93s/it]  0%|          | 7/3743 [00:33<4:53:42,  4.72s/it]  0%|          | 8/3743 [00:37<4:39:48,  4.49s/it]  0%|          | 9/3743 [00:41<4:11:59,  4.05s/it]  0%|          | 10/3743 [00:44<3:55:28,  3.78s/it]{'loss': 0.3662, 'grad_norm': 3348.409423828125, 'learning_rate': 2.4e-08, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 0.12505777180194855, 'weight_chosen': -0.8935995101928711, 'weight_rejected': 0.4968875050544739, 'kl_term_chosen': 1.3848114013671875, 'epoch': 0.0026721891909947224}
                                                   {'loss': 0.3662, 'grad_norm': 3348.409423828125, 'learning_rate': 2.4e-08, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 0.12505777180194855, 'weight_chosen': -0.8935995101928711, 'weight_rejected': 0.4968875050544739, 'kl_term_chosen': 1.3848114013671875, 'epoch': 0.0}
  0%|          | 10/3743 [00:44<3:55:28,  3.78s/it]  0%|          | 11/3743 [00:48<4:07:13,  3.97s/it]  0%|          | 12/3743 [00:54<4:40:43,  4.51s/it]  0%|          | 13/3743 [00:57<4:12:21,  4.06s/it]  0%|          | 14/3743 [01:03<4:51:15,  4.69s/it]  0%|          | 15/3743 [01:07<4:28:50,  4.33s/it]  0%|          | 16/3743 [01:11<4:32:30,  4.39s/it]  0%|          | 17/3743 [01:20<5:51:00,  5.65s/it]  0%|          | 18/3743 [01:28<6:44:23,  6.51s/it]  1%|          | 19/3743 [01:33<6:15:57,  6.06s/it]  1%|          | 20/3743 [01:38<6:00:10,  5.80s/it]{'loss': 0.2609, 'grad_norm': 1350.9942626953125, 'learning_rate': 5.0666666666666664e-08, 'mean_ratio_chosen': 0.3752039074897766, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 0.988128662109375, 'weight_rejected': 0.4986572265625, 'kl_term_chosen': -0.490142822265625, 'epoch': 0.005344378381989445}
                                                   {'loss': 0.2609, 'grad_norm': 1350.9942626953125, 'learning_rate': 5.0666666666666664e-08, 'mean_ratio_chosen': 0.3752039074897766, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 0.988128662109375, 'weight_rejected': 0.4986572265625, 'kl_term_chosen': -0.490142822265625, 'epoch': 0.01}
  1%|          | 20/3743 [01:38<6:00:10,  5.80s/it]  1%|          | 21/3743 [01:46<6:38:19,  6.42s/it]  1%|          | 22/3743 [01:52<6:30:47,  6.30s/it]  1%|          | 23/3743 [01:56<5:46:26,  5.59s/it]  1%|          | 24/3743 [02:01<5:23:50,  5.22s/it]  1%|          | 25/3743 [02:05<5:01:13,  4.86s/it]  1%|          | 26/3743 [02:09<4:48:47,  4.66s/it]  1%|          | 27/3743 [02:14<5:02:46,  4.89s/it]  1%|          | 28/3743 [02:17<4:32:54,  4.41s/it]  1%|          | 29/3743 [02:22<4:42:15,  4.56s/it]  1%|          | 30/3743 [02:27<4:36:13,  4.46s/it]{'loss': 0.1102, 'grad_norm': 1545.396728515625, 'learning_rate': 7.733333333333334e-08, 'mean_ratio_chosen': 0.416213721036911, 'mean_ratio_rejected': 0.6395887732505798, 'weight_chosen': 0.9349213242530823, 'weight_rejected': 0.49633798003196716, 'kl_term_chosen': -0.4382781982421875, 'epoch': 0.008016567572984168}
                                                   {'loss': 0.1102, 'grad_norm': 1545.396728515625, 'learning_rate': 7.733333333333334e-08, 'mean_ratio_chosen': 0.416213721036911, 'mean_ratio_rejected': 0.6395887732505798, 'weight_chosen': 0.9349213242530823, 'weight_rejected': 0.49633798003196716, 'kl_term_chosen': -0.4382781982421875, 'epoch': 0.01}
  1%|          | 30/3743 [02:27<4:36:13,  4.46s/it]  1%|          | 31/3743 [02:30<4:25:02,  4.28s/it]  1%|          | 32/3743 [02:36<4:41:17,  4.55s/it]  1%|          | 33/3743 [02:41<4:47:33,  4.65s/it]  1%|          | 34/3743 [02:44<4:33:14,  4.42s/it]  1%|          | 35/3743 [02:53<5:48:22,  5.64s/it]  1%|          | 36/3743 [02:59<5:49:02,  5.65s/it]  1%|          | 37/3743 [03:02<5:13:30,  5.08s/it]  1%|          | 38/3743 [03:07<5:13:00,  5.07s/it]  1%|          | 39/3743 [03:12<5:07:56,  4.99s/it]  1%|          | 40/3743 [03:16<4:53:48,  4.76s/it]{'loss': -0.0134, 'grad_norm': 970.7284545898438, 'learning_rate': 1.0399999999999999e-07, 'mean_ratio_chosen': 0.21594862639904022, 'mean_ratio_rejected': 0.3752468526363373, 'weight_chosen': 1.26318359375, 'weight_rejected': 0.4967651963233948, 'kl_term_chosen': -0.766357421875, 'epoch': 0.01068875676397889}
                                                   {'loss': -0.0134, 'grad_norm': 970.7284545898438, 'learning_rate': 1.0399999999999999e-07, 'mean_ratio_chosen': 0.21594862639904022, 'mean_ratio_rejected': 0.3752468526363373, 'weight_chosen': 1.26318359375, 'weight_rejected': 0.4967651963233948, 'kl_term_chosen': -0.766357421875, 'epoch': 0.01}
  1%|          | 40/3743 [03:17<4:53:48,  4.76s/it]  1%|          | 41/3743 [03:20<4:31:13,  4.40s/it]  1%|          | 42/3743 [03:24<4:28:24,  4.35s/it]  1%|          | 43/3743 [03:32<5:40:10,  5.52s/it]  1%|          | 44/3743 [03:38<5:39:24,  5.51s/it]  1%|          | 45/3743 [03:42<5:14:33,  5.10s/it]  1%|          | 46/3743 [03:49<5:48:21,  5.65s/it]  1%|▏         | 47/3743 [03:54<5:36:50,  5.47s/it]  1%|▏         | 48/3743 [04:00<5:37:49,  5.49s/it]  1%|▏         | 49/3743 [04:04<5:09:27,  5.03s/it]  1%|▏         | 50/3743 [04:08<5:06:32,  4.98s/it]{'loss': -0.0654, 'grad_norm': 1155.168701171875, 'learning_rate': 1.3066666666666665e-07, 'mean_ratio_chosen': 1.0820293426513672, 'mean_ratio_rejected': 0.9461166858673096, 'weight_chosen': 0.46003150939941406, 'weight_rejected': 0.499481201171875, 'kl_term_chosen': 0.03941917419433594, 'epoch': 0.013360945954973611}
                                                   {'loss': -0.0654, 'grad_norm': 1155.168701171875, 'learning_rate': 1.3066666666666665e-07, 'mean_ratio_chosen': 1.0820293426513672, 'mean_ratio_rejected': 0.9461166858673096, 'weight_chosen': 0.46003150939941406, 'weight_rejected': 0.499481201171875, 'kl_term_chosen': 0.03941917419433594, 'epoch': 0.01}
  1%|▏         | 50/3743 [04:09<5:06:32,  4.98s/it]  1%|▏         | 51/3743 [04:20<7:06:27,  6.93s/it]  1%|▏         | 52/3743 [04:24<6:13:45,  6.08s/it]  1%|▏         | 53/3743 [04:28<5:33:57,  5.43s/it]  1%|▏         | 54/3743 [04:32<5:01:18,  4.90s/it]  1%|▏         | 55/3743 [04:35<4:38:15,  4.53s/it]  1%|▏         | 56/3743 [04:42<5:24:06,  5.27s/it]  2%|▏         | 57/3743 [04:46<5:01:12,  4.90s/it]  2%|▏         | 58/3743 [04:51<4:57:54,  4.85s/it]  2%|▏         | 59/3743 [04:56<4:57:59,  4.85s/it]  2%|▏         | 60/3743 [05:01<5:09:03,  5.03s/it]{'loss': 0.0538, 'grad_norm': 944.7415771484375, 'learning_rate': 1.573333333333333e-07, 'mean_ratio_chosen': 1.2774395942687988, 'mean_ratio_rejected': 0.8878996968269348, 'weight_chosen': 0.37586212158203125, 'weight_rejected': 0.498046875, 'kl_term_chosen': 0.12242889404296875, 'epoch': 0.016033135145968335}
                                                   {'loss': 0.0538, 'grad_norm': 944.7415771484375, 'learning_rate': 1.573333333333333e-07, 'mean_ratio_chosen': 1.2774395942687988, 'mean_ratio_rejected': 0.8878996968269348, 'weight_chosen': 0.37586212158203125, 'weight_rejected': 0.498046875, 'kl_term_chosen': 0.12242889404296875, 'epoch': 0.02}
  2%|▏         | 60/3743 [05:01<5:09:03,  5.03s/it]  2%|▏         | 61/3743 [05:10<6:12:55,  6.08s/it]  2%|▏         | 62/3743 [05:13<5:21:46,  5.24s/it]  2%|▏         | 63/3743 [05:17<4:53:20,  4.78s/it]  2%|▏         | 64/3743 [05:22<4:52:09,  4.76s/it]  2%|▏         | 65/3743 [05:26<4:40:12,  4.57s/it]  2%|▏         | 66/3743 [05:31<4:45:52,  4.66s/it]  2%|▏         | 67/3743 [05:34<4:29:24,  4.40s/it]  2%|▏         | 68/3743 [05:39<4:33:51,  4.47s/it]  2%|▏         | 69/3743 [05:45<5:08:11,  5.03s/it]  2%|▏         | 70/3743 [05:51<5:18:43,  5.21s/it]{'loss': 0.0371, 'grad_norm': 989.1879272460938, 'learning_rate': 1.8399999999999998e-07, 'mean_ratio_chosen': 0.45630601048469543, 'mean_ratio_rejected': 2.25140380859375, 'weight_chosen': 0.8892441391944885, 'weight_rejected': 0.4975586235523224, 'kl_term_chosen': -0.39229583740234375, 'epoch': 0.018705324336963057}
                                                   {'loss': 0.0371, 'grad_norm': 989.1879272460938, 'learning_rate': 1.8399999999999998e-07, 'mean_ratio_chosen': 0.45630601048469543, 'mean_ratio_rejected': 2.25140380859375, 'weight_chosen': 0.8892441391944885, 'weight_rejected': 0.4975586235523224, 'kl_term_chosen': -0.39229583740234375, 'epoch': 0.02}
  2%|▏         | 70/3743 [05:51<5:18:43,  5.21s/it]  2%|▏         | 71/3743 [05:55<4:54:55,  4.82s/it]  2%|▏         | 72/3743 [05:59<4:47:26,  4.70s/it]  2%|▏         | 73/3743 [06:07<5:51:37,  5.75s/it]  2%|▏         | 74/3743 [06:16<6:34:09,  6.45s/it]  2%|▏         | 75/3743 [06:21<6:19:52,  6.21s/it]  2%|▏         | 76/3743 [06:25<5:31:57,  5.43s/it]  2%|▏         | 77/3743 [06:29<5:11:06,  5.09s/it]  2%|▏         | 78/3743 [06:35<5:22:10,  5.27s/it]  2%|▏         | 79/3743 [06:40<5:25:58,  5.34s/it]  2%|▏         | 80/3743 [06:45<5:07:32,  5.04s/it]{'loss': -0.0041, 'grad_norm': 1202.029052734375, 'learning_rate': 2.1066666666666665e-07, 'mean_ratio_chosen': 1.0028728246688843, 'mean_ratio_rejected': 0.7362969517707825, 'weight_chosen': 0.4950562119483948, 'weight_rejected': 0.4978637993335724, 'kl_term_chosen': 0.001434326171875, 'epoch': 0.02137751352795778}
                                                   {'loss': -0.0041, 'grad_norm': 1202.029052734375, 'learning_rate': 2.1066666666666665e-07, 'mean_ratio_chosen': 1.0028728246688843, 'mean_ratio_rejected': 0.7362969517707825, 'weight_chosen': 0.4950562119483948, 'weight_rejected': 0.4978637993335724, 'kl_term_chosen': 0.001434326171875, 'epoch': 0.02}
  2%|▏         | 80/3743 [06:45<5:07:32,  5.04s/it]  2%|▏         | 81/3743 [06:48<4:45:23,  4.68s/it]  2%|▏         | 82/3743 [06:54<4:55:10,  4.84s/it]  2%|▏         | 83/3743 [06:58<4:38:49,  4.57s/it]  2%|▏         | 84/3743 [07:02<4:31:37,  4.45s/it]  2%|▏         | 85/3743 [07:06<4:31:00,  4.45s/it]  2%|▏         | 86/3743 [07:11<4:36:42,  4.54s/it]  2%|▏         | 87/3743 [07:15<4:29:51,  4.43s/it]  2%|▏         | 88/3743 [07:20<4:29:49,  4.43s/it]  2%|▏         | 89/3743 [07:24<4:24:21,  4.34s/it]  2%|▏         | 90/3743 [07:27<4:07:09,  4.06s/it]{'loss': 0.0784, 'grad_norm': 902.8264770507812, 'learning_rate': 2.3733333333333334e-07, 'mean_ratio_chosen': 0.5733717679977417, 'mean_ratio_rejected': 0.7321349382400513, 'weight_chosen': 0.7744484543800354, 'weight_rejected': 0.49664315581321716, 'kl_term_chosen': -0.2781105041503906, 'epoch': 0.0240497027189525}
                                                   {'loss': 0.0784, 'grad_norm': 902.8264770507812, 'learning_rate': 2.3733333333333334e-07, 'mean_ratio_chosen': 0.5733717679977417, 'mean_ratio_rejected': 0.7321349382400513, 'weight_chosen': 0.7744484543800354, 'weight_rejected': 0.49664315581321716, 'kl_term_chosen': -0.2781105041503906, 'epoch': 0.02}
  2%|▏         | 90/3743 [07:27<4:07:09,  4.06s/it]  2%|▏         | 91/3743 [07:32<4:19:09,  4.26s/it]  2%|▏         | 92/3743 [07:37<4:42:44,  4.65s/it]  2%|▏         | 93/3743 [07:41<4:22:12,  4.31s/it]  3%|▎         | 94/3743 [07:46<4:38:54,  4.59s/it]  3%|▎         | 95/3743 [07:51<4:36:03,  4.54s/it]  3%|▎         | 96/3743 [07:55<4:34:58,  4.52s/it]  3%|▎         | 97/3743 [07:59<4:20:31,  4.29s/it]  3%|▎         | 98/3743 [08:03<4:13:34,  4.17s/it]  3%|▎         | 99/3743 [08:07<4:17:41,  4.24s/it]  3%|▎         | 100/3743 [08:12<4:35:21,  4.54s/it]{'loss': 0.0668, 'grad_norm': 977.9578247070312, 'learning_rate': 2.64e-07, 'mean_ratio_chosen': 0.34032633900642395, 'mean_ratio_rejected': 0.15409384667873383, 'weight_chosen': 1.029099941253662, 'weight_rejected': 0.49334806203842163, 'kl_term_chosen': -0.5389251708984375, 'epoch': 0.026721891909947223}
                                                    {'loss': 0.0668, 'grad_norm': 977.9578247070312, 'learning_rate': 2.64e-07, 'mean_ratio_chosen': 0.34032633900642395, 'mean_ratio_rejected': 0.15409384667873383, 'weight_chosen': 1.029099941253662, 'weight_rejected': 0.49334806203842163, 'kl_term_chosen': -0.5389251708984375, 'epoch': 0.03}
  3%|▎         | 100/3743 [08:12<4:35:21,  4.54s/it]  3%|▎         | 101/3743 [08:16<4:21:27,  4.31s/it]  3%|▎         | 102/3743 [08:20<4:15:47,  4.22s/it]  3%|▎         | 103/3743 [08:27<4:57:02,  4.90s/it]  3%|▎         | 104/3743 [08:30<4:36:37,  4.56s/it]  3%|▎         | 105/3743 [08:34<4:21:26,  4.31s/it]  3%|▎         | 106/3743 [08:39<4:27:07,  4.41s/it]  3%|▎         | 107/3743 [08:43<4:20:01,  4.29s/it]  3%|▎         | 108/3743 [08:47<4:18:36,  4.27s/it]  3%|▎         | 109/3743 [08:52<4:26:41,  4.40s/it]  3%|▎         | 110/3743 [08:56<4:21:22,  4.32s/it]{'loss': 0.0285, 'grad_norm': 2233.56494140625, 'learning_rate': 2.906666666666667e-07, 'mean_ratio_chosen': 0.6428861021995544, 'mean_ratio_rejected': 1.3642958402633667, 'weight_chosen': 0.7192306518554688, 'weight_rejected': 0.498992919921875, 'kl_term_chosen': -0.22089385986328125, 'epoch': 0.029394081100941948}
                                                    {'loss': 0.0285, 'grad_norm': 2233.56494140625, 'learning_rate': 2.906666666666667e-07, 'mean_ratio_chosen': 0.6428861021995544, 'mean_ratio_rejected': 1.3642958402633667, 'weight_chosen': 0.7192306518554688, 'weight_rejected': 0.498992919921875, 'kl_term_chosen': -0.22089385986328125, 'epoch': 0.03}
  3%|▎         | 110/3743 [08:56<4:21:22,  4.32s/it]  3%|▎         | 111/3743 [09:00<4:17:49,  4.26s/it]  3%|▎         | 112/3743 [09:04<4:12:12,  4.17s/it]  3%|▎         | 113/3743 [09:08<4:17:48,  4.26s/it]  3%|▎         | 114/3743 [09:12<4:08:57,  4.12s/it]  3%|▎         | 115/3743 [09:16<4:03:42,  4.03s/it]  3%|▎         | 116/3743 [09:20<4:07:59,  4.10s/it]  3%|▎         | 117/3743 [09:24<4:06:16,  4.08s/it]  3%|▎         | 118/3743 [09:28<3:59:05,  3.96s/it]  3%|▎         | 119/3743 [09:32<3:52:22,  3.85s/it]  3%|▎         | 120/3743 [09:35<3:45:16,  3.73s/it]{'loss': 0.0136, 'grad_norm': 2057.1787109375, 'learning_rate': 3.173333333333333e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 5.0289130210876465, 'weight_chosen': 1.74481201171875, 'weight_rejected': 0.4970093071460724, 'kl_term_chosen': -1.24639892578125, 'epoch': 0.03206627029193667}
                                                    {'loss': 0.0136, 'grad_norm': 2057.1787109375, 'learning_rate': 3.173333333333333e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 5.0289130210876465, 'weight_chosen': 1.74481201171875, 'weight_rejected': 0.4970093071460724, 'kl_term_chosen': -1.24639892578125, 'epoch': 0.03}
  3%|▎         | 120/3743 [09:35<3:45:16,  3.73s/it]  3%|▎         | 121/3743 [09:40<4:00:06,  3.98s/it]  3%|▎         | 122/3743 [09:44<4:03:12,  4.03s/it]  3%|▎         | 123/3743 [09:50<4:41:19,  4.66s/it]  3%|▎         | 124/3743 [09:53<4:19:55,  4.31s/it]  3%|▎         | 125/3743 [09:58<4:29:55,  4.48s/it]  3%|▎         | 126/3743 [10:02<4:14:42,  4.23s/it]  3%|▎         | 127/3743 [10:06<4:15:40,  4.24s/it]  3%|▎         | 128/3743 [10:10<4:04:55,  4.07s/it]  3%|▎         | 129/3743 [10:14<3:59:59,  3.98s/it]  3%|▎         | 130/3743 [10:17<3:59:01,  3.97s/it]{'loss': 0.029, 'grad_norm': 717.0049438476562, 'learning_rate': 3.4399999999999996e-07, 'mean_ratio_chosen': 0.8325214982032776, 'mean_ratio_rejected': 0.7077333331108093, 'weight_chosen': 0.5887184739112854, 'weight_rejected': 0.4977417290210724, 'kl_term_chosen': -0.09164810180664062, 'epoch': 0.03473845948293139}
                                                    {'loss': 0.029, 'grad_norm': 717.0049438476562, 'learning_rate': 3.4399999999999996e-07, 'mean_ratio_chosen': 0.8325214982032776, 'mean_ratio_rejected': 0.7077333331108093, 'weight_chosen': 0.5887184739112854, 'weight_rejected': 0.4977417290210724, 'kl_term_chosen': -0.09164810180664062, 'epoch': 0.03}
  3%|▎         | 130/3743 [10:18<3:59:01,  3.97s/it]  3%|▎         | 131/3743 [10:22<4:00:27,  3.99s/it]  4%|▎         | 132/3743 [10:26<4:08:08,  4.12s/it]  4%|▎         | 133/3743 [10:31<4:16:53,  4.27s/it]  4%|▎         | 134/3743 [10:35<4:14:09,  4.23s/it]  4%|▎         | 135/3743 [10:38<4:05:08,  4.08s/it]  4%|▎         | 136/3743 [10:42<3:52:05,  3.86s/it]  4%|▎         | 137/3743 [10:47<4:08:55,  4.14s/it]  4%|▎         | 138/3743 [10:53<4:41:27,  4.68s/it]  4%|▎         | 139/3743 [10:57<4:32:48,  4.54s/it]  4%|▎         | 140/3743 [11:02<4:47:22,  4.79s/it]{'loss': 0.0991, 'grad_norm': 776.31884765625, 'learning_rate': 3.7066666666666665e-07, 'mean_ratio_chosen': 1.1272850036621094, 'mean_ratio_rejected': 0.4962819218635559, 'weight_chosen': 0.439453125, 'weight_rejected': 0.499114990234375, 'kl_term_chosen': 0.059906005859375, 'epoch': 0.037410648673926114}
                                                    {'loss': 0.0991, 'grad_norm': 776.31884765625, 'learning_rate': 3.7066666666666665e-07, 'mean_ratio_chosen': 1.1272850036621094, 'mean_ratio_rejected': 0.4962819218635559, 'weight_chosen': 0.439453125, 'weight_rejected': 0.499114990234375, 'kl_term_chosen': 0.059906005859375, 'epoch': 0.04}
  4%|▎         | 140/3743 [11:02<4:47:22,  4.79s/it]  4%|▍         | 141/3743 [11:05<4:15:17,  4.25s/it]  4%|▍         | 142/3743 [11:10<4:18:20,  4.30s/it]  4%|▍         | 143/3743 [11:14<4:22:16,  4.37s/it]  4%|▍         | 144/3743 [11:18<4:13:53,  4.23s/it]  4%|▍         | 145/3743 [11:23<4:24:59,  4.42s/it]  4%|▍         | 146/3743 [11:29<4:54:55,  4.92s/it]  4%|▍         | 147/3743 [11:33<4:39:56,  4.67s/it]  4%|▍         | 148/3743 [11:38<4:41:00,  4.69s/it]  4%|▍         | 149/3743 [11:43<4:47:38,  4.80s/it]  4%|▍         | 150/3743 [11:47<4:34:26,  4.58s/it]{'loss': 0.0723, 'grad_norm': 847.326171875, 'learning_rate': 3.973333333333333e-07, 'mean_ratio_chosen': 0.7387896776199341, 'mean_ratio_rejected': 0.6659250855445862, 'weight_chosen': 0.6494179368019104, 'weight_rejected': 0.49584969878196716, 'kl_term_chosen': -0.15137100219726562, 'epoch': 0.04008283786492084}
                                                    {'loss': 0.0723, 'grad_norm': 847.326171875, 'learning_rate': 3.973333333333333e-07, 'mean_ratio_chosen': 0.7387896776199341, 'mean_ratio_rejected': 0.6659250855445862, 'weight_chosen': 0.6494179368019104, 'weight_rejected': 0.49584969878196716, 'kl_term_chosen': -0.15137100219726562, 'epoch': 0.04}
  4%|▍         | 150/3743 [11:47<4:34:26,  4.58s/it]  4%|▍         | 151/3743 [11:51<4:25:32,  4.44s/it]  4%|▍         | 152/3743 [11:55<4:22:22,  4.38s/it]  4%|▍         | 153/3743 [12:00<4:36:03,  4.61s/it]  4%|▍         | 154/3743 [12:04<4:24:40,  4.42s/it]  4%|▍         | 155/3743 [12:09<4:20:57,  4.36s/it]  4%|▍         | 156/3743 [12:12<4:07:14,  4.14s/it]  4%|▍         | 157/3743 [12:17<4:13:21,  4.24s/it]  4%|▍         | 158/3743 [12:21<4:12:11,  4.22s/it]  4%|▍         | 159/3743 [12:25<4:08:46,  4.16s/it]  4%|▍         | 160/3743 [12:29<4:07:09,  4.14s/it]{'loss': 0.153, 'grad_norm': 898.8118286132812, 'learning_rate': 4.24e-07, 'mean_ratio_chosen': 0.211952805519104, 'mean_ratio_rejected': 0.18267710506916046, 'weight_chosen': 1.270263910293579, 'weight_rejected': 0.49642959237098694, 'kl_term_chosen': -0.77569580078125, 'epoch': 0.04275502705591556}
                                                    {'loss': 0.153, 'grad_norm': 898.8118286132812, 'learning_rate': 4.24e-07, 'mean_ratio_chosen': 0.211952805519104, 'mean_ratio_rejected': 0.18267710506916046, 'weight_chosen': 1.270263910293579, 'weight_rejected': 0.49642959237098694, 'kl_term_chosen': -0.77569580078125, 'epoch': 0.04}
  4%|▍         | 160/3743 [12:29<4:07:09,  4.14s/it]  4%|▍         | 161/3743 [12:34<4:14:26,  4.26s/it]  4%|▍         | 162/3743 [12:37<4:09:33,  4.18s/it]  4%|▍         | 163/3743 [12:41<3:48:46,  3.83s/it]  4%|▍         | 164/3743 [12:44<3:47:37,  3.82s/it]  4%|▍         | 165/3743 [12:50<4:24:28,  4.43s/it]  4%|▍         | 166/3743 [12:54<4:16:24,  4.30s/it]  4%|▍         | 167/3743 [13:00<4:40:51,  4.71s/it]  4%|▍         | 168/3743 [13:04<4:36:53,  4.65s/it]  5%|▍         | 169/3743 [13:09<4:31:30,  4.56s/it]  5%|▍         | 170/3743 [13:13<4:26:48,  4.48s/it]{'loss': 0.0826, 'grad_norm': 3420.8271484375, 'learning_rate': 4.506666666666666e-07, 'mean_ratio_chosen': 2.342733383178711, 'mean_ratio_rejected': 0.46048519015312195, 'weight_chosen': 0.06933629512786865, 'weight_rejected': 0.49310359358787537, 'kl_term_chosen': 0.4256591796875, 'epoch': 0.04542721624691028}
                                                    {'loss': 0.0826, 'grad_norm': 3420.8271484375, 'learning_rate': 4.506666666666666e-07, 'mean_ratio_chosen': 2.342733383178711, 'mean_ratio_rejected': 0.46048519015312195, 'weight_chosen': 0.06933629512786865, 'weight_rejected': 0.49310359358787537, 'kl_term_chosen': 0.4256591796875, 'epoch': 0.05}
  5%|▍         | 170/3743 [13:13<4:26:48,  4.48s/it]  5%|▍         | 171/3743 [13:18<4:31:03,  4.55s/it]  5%|▍         | 172/3743 [13:22<4:26:13,  4.47s/it]  5%|▍         | 173/3743 [13:31<5:39:29,  5.71s/it]  5%|▍         | 174/3743 [13:34<4:56:06,  4.98s/it]  5%|▍         | 175/3743 [13:38<4:43:46,  4.77s/it]  5%|▍         | 176/3743 [13:46<5:42:38,  5.76s/it]  5%|▍         | 177/3743 [13:52<5:36:02,  5.65s/it]  5%|▍         | 178/3743 [13:57<5:35:47,  5.65s/it]  5%|▍         | 179/3743 [14:02<5:21:44,  5.42s/it]  5%|▍         | 180/3743 [14:05<4:42:26,  4.76s/it]{'loss': 0.1726, 'grad_norm': 1546.280517578125, 'learning_rate': 4.773333333333333e-07, 'mean_ratio_chosen': 0.6063455939292908, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 0.7461854219436646, 'weight_rejected': 0.49603283405303955, 'kl_term_chosen': -0.250152587890625, 'epoch': 0.048099405437905}
                                                    {'loss': 0.1726, 'grad_norm': 1546.280517578125, 'learning_rate': 4.773333333333333e-07, 'mean_ratio_chosen': 0.6063455939292908, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 0.7461854219436646, 'weight_rejected': 0.49603283405303955, 'kl_term_chosen': -0.250152587890625, 'epoch': 0.05}
  5%|▍         | 180/3743 [14:05<4:42:26,  4.76s/it]  5%|▍         | 181/3743 [14:14<5:44:36,  5.80s/it]  5%|▍         | 182/3743 [14:21<6:18:37,  6.38s/it]  5%|▍         | 183/3743 [14:26<5:50:17,  5.90s/it]  5%|▍         | 184/3743 [14:31<5:30:05,  5.56s/it]  5%|▍         | 185/3743 [14:35<5:03:38,  5.12s/it]  5%|▍         | 186/3743 [14:39<4:40:17,  4.73s/it]  5%|▍         | 187/3743 [14:43<4:32:22,  4.60s/it]  5%|▌         | 188/3743 [14:55<6:50:30,  6.93s/it]  5%|▌         | 189/3743 [15:00<6:04:02,  6.15s/it]  5%|▌         | 190/3743 [15:04<5:21:54,  5.44s/it]{'loss': 0.1028, 'grad_norm': 692.625244140625, 'learning_rate': 5.04e-07, 'mean_ratio_chosen': 0.44180828332901, 'mean_ratio_rejected': 0.5688793659210205, 'weight_chosen': 0.8998810648918152, 'weight_rejected': 0.4889930784702301, 'kl_term_chosen': -0.40843963623046875, 'epoch': 0.05077159462889973}
                                                    {'loss': 0.1028, 'grad_norm': 692.625244140625, 'learning_rate': 5.04e-07, 'mean_ratio_chosen': 0.44180828332901, 'mean_ratio_rejected': 0.5688793659210205, 'weight_chosen': 0.8998810648918152, 'weight_rejected': 0.4889930784702301, 'kl_term_chosen': -0.40843963623046875, 'epoch': 0.05}
  5%|▌         | 190/3743 [15:04<5:21:54,  5.44s/it]  5%|▌         | 191/3743 [15:07<4:53:28,  4.96s/it]  5%|▌         | 192/3743 [15:11<4:31:56,  4.59s/it]  5%|▌         | 193/3743 [15:19<5:27:19,  5.53s/it]  5%|▌         | 194/3743 [15:24<5:17:09,  5.36s/it]  5%|▌         | 195/3743 [15:29<5:07:44,  5.20s/it]  5%|▌         | 196/3743 [15:35<5:21:42,  5.44s/it]  5%|▌         | 197/3743 [15:38<4:51:58,  4.94s/it]  5%|▌         | 198/3743 [15:46<5:44:39,  5.83s/it]  5%|▌         | 199/3743 [15:51<5:30:38,  5.60s/it]  5%|▌         | 200/3743 [15:57<5:23:10,  5.47s/it]{'loss': 0.0673, 'grad_norm': 572.6414794921875, 'learning_rate': 5.306666666666665e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.6871185302734375, 'weight_rejected': 0.4971924126148224, 'kl_term_chosen': -2.1882781982421875, 'epoch': 0.053443783819894446}
                                                    {'loss': 0.0673, 'grad_norm': 572.6414794921875, 'learning_rate': 5.306666666666665e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.6871185302734375, 'weight_rejected': 0.4971924126148224, 'kl_term_chosen': -2.1882781982421875, 'epoch': 0.05}
  5%|▌         | 200/3743 [15:57<5:23:10,  5.47s/it]  5%|▌         | 201/3743 [16:01<5:08:19,  5.22s/it]  5%|▌         | 202/3743 [16:05<4:40:07,  4.75s/it]  5%|▌         | 203/3743 [16:09<4:21:18,  4.43s/it]  5%|▌         | 204/3743 [16:12<3:59:57,  4.07s/it]  5%|▌         | 205/3743 [16:16<4:02:37,  4.11s/it]  6%|▌         | 206/3743 [16:24<5:12:19,  5.30s/it]  6%|▌         | 207/3743 [16:28<4:46:17,  4.86s/it]  6%|▌         | 208/3743 [16:34<5:06:53,  5.21s/it]  6%|▌         | 209/3743 [16:38<4:48:00,  4.89s/it]  6%|▌         | 210/3743 [16:42<4:39:43,  4.75s/it]{'loss': 0.1951, 'grad_norm': 775.7261962890625, 'learning_rate': 5.573333333333333e-07, 'mean_ratio_chosen': 0.41083934903144836, 'mean_ratio_rejected': 0.1634698510169983, 'weight_chosen': 0.9403210878372192, 'weight_rejected': 0.4977417588233948, 'kl_term_chosen': -0.4447765350341797, 'epoch': 0.05611597301088917}
                                                    {'loss': 0.1951, 'grad_norm': 775.7261962890625, 'learning_rate': 5.573333333333333e-07, 'mean_ratio_chosen': 0.41083934903144836, 'mean_ratio_rejected': 0.1634698510169983, 'weight_chosen': 0.9403210878372192, 'weight_rejected': 0.4977417588233948, 'kl_term_chosen': -0.4447765350341797, 'epoch': 0.06}
  6%|▌         | 210/3743 [16:43<4:39:43,  4.75s/it]  6%|▌         | 211/3743 [16:46<4:15:19,  4.34s/it]  6%|▌         | 212/3743 [16:50<4:11:43,  4.28s/it]  6%|▌         | 213/3743 [16:54<4:06:04,  4.18s/it]  6%|▌         | 214/3743 [16:58<4:06:05,  4.18s/it]  6%|▌         | 215/3743 [17:04<4:32:39,  4.64s/it]  6%|▌         | 216/3743 [17:08<4:25:10,  4.51s/it]  6%|▌         | 217/3743 [17:12<4:07:33,  4.21s/it]  6%|▌         | 218/3743 [17:16<4:03:49,  4.15s/it]  6%|▌         | 219/3743 [17:19<3:49:44,  3.91s/it]  6%|▌         | 220/3743 [17:24<4:14:04,  4.33s/it]{'loss': 0.3282, 'grad_norm': 740.694580078125, 'learning_rate': 5.839999999999999e-07, 'mean_ratio_chosen': 0.17534811794757843, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 1.3699417114257812, 'weight_rejected': 0.499481201171875, 'kl_term_chosen': -0.8704910278320312, 'epoch': 0.058788162201883896}
                                                    {'loss': 0.3282, 'grad_norm': 740.694580078125, 'learning_rate': 5.839999999999999e-07, 'mean_ratio_chosen': 0.17534811794757843, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 1.3699417114257812, 'weight_rejected': 0.499481201171875, 'kl_term_chosen': -0.8704910278320312, 'epoch': 0.06}
  6%|▌         | 220/3743 [17:24<4:14:04,  4.33s/it]  6%|▌         | 221/3743 [17:29<4:27:00,  4.55s/it]  6%|▌         | 222/3743 [17:34<4:22:44,  4.48s/it]  6%|▌         | 223/3743 [17:37<4:10:36,  4.27s/it]  6%|▌         | 224/3743 [17:41<4:00:04,  4.09s/it]  6%|▌         | 225/3743 [17:45<4:01:02,  4.11s/it]  6%|▌         | 226/3743 [17:49<3:58:06,  4.06s/it]  6%|▌         | 227/3743 [17:55<4:20:46,  4.45s/it]  6%|▌         | 228/3743 [17:59<4:14:04,  4.34s/it]  6%|▌         | 229/3743 [18:03<4:07:23,  4.22s/it]  6%|▌         | 230/3743 [18:06<3:57:46,  4.06s/it]{'loss': 0.4859, 'grad_norm': 1865.2418212890625, 'learning_rate': 6.106666666666666e-07, 'mean_ratio_chosen': 0.7290524840354919, 'mean_ratio_rejected': 0.1604108214378357, 'weight_chosen': 0.6556243896484375, 'weight_rejected': 0.4978790581226349, 'kl_term_chosen': -0.1580047607421875, 'epoch': 0.061460351392878615}
                                                    {'loss': 0.4859, 'grad_norm': 1865.2418212890625, 'learning_rate': 6.106666666666666e-07, 'mean_ratio_chosen': 0.7290524840354919, 'mean_ratio_rejected': 0.1604108214378357, 'weight_chosen': 0.6556243896484375, 'weight_rejected': 0.4978790581226349, 'kl_term_chosen': -0.1580047607421875, 'epoch': 0.06}
  6%|▌         | 230/3743 [18:06<3:57:46,  4.06s/it]  6%|▌         | 231/3743 [18:13<4:53:52,  5.02s/it]  6%|▌         | 232/3743 [18:17<4:29:09,  4.60s/it]  6%|▌         | 233/3743 [18:22<4:34:32,  4.69s/it]  6%|▋         | 234/3743 [18:26<4:25:36,  4.54s/it]  6%|▋         | 235/3743 [18:31<4:23:51,  4.51s/it]  6%|▋         | 236/3743 [18:34<4:05:43,  4.20s/it]  6%|▋         | 237/3743 [18:38<4:04:07,  4.18s/it]  6%|▋         | 238/3743 [18:42<4:02:54,  4.16s/it]  6%|▋         | 239/3743 [18:51<5:21:55,  5.51s/it]  6%|▋         | 240/3743 [18:54<4:45:22,  4.89s/it]{'loss': 1.9924, 'grad_norm': 10012.1337890625, 'learning_rate': 6.373333333333333e-07, 'mean_ratio_chosen': 8.107863426208496, 'mean_ratio_rejected': 0.5787026286125183, 'weight_chosen': -0.548187255859375, 'weight_rejected': 0.4990234375, 'kl_term_chosen': 1.046417236328125, 'epoch': 0.06413254058387334}
                                                    {'loss': 1.9924, 'grad_norm': 10012.1337890625, 'learning_rate': 6.373333333333333e-07, 'mean_ratio_chosen': 8.107863426208496, 'mean_ratio_rejected': 0.5787026286125183, 'weight_chosen': -0.548187255859375, 'weight_rejected': 0.4990234375, 'kl_term_chosen': 1.046417236328125, 'epoch': 0.06}
  6%|▋         | 240/3743 [18:55<4:45:22,  4.89s/it]  6%|▋         | 241/3743 [18:58<4:26:29,  4.57s/it]  6%|▋         | 242/3743 [19:03<4:21:14,  4.48s/it]  6%|▋         | 243/3743 [19:08<4:40:20,  4.81s/it]  7%|▋         | 244/3743 [19:13<4:34:55,  4.71s/it]  7%|▋         | 245/3743 [19:17<4:29:04,  4.62s/it]  7%|▋         | 246/3743 [19:21<4:23:01,  4.51s/it]  7%|▋         | 247/3743 [19:24<3:59:06,  4.10s/it]  7%|▋         | 248/3743 [19:29<4:08:08,  4.26s/it]  7%|▋         | 249/3743 [19:34<4:12:29,  4.34s/it]  7%|▋         | 250/3743 [19:39<4:25:43,  4.56s/it]{'loss': 0.0626, 'grad_norm': 648.0829467773438, 'learning_rate': 6.64e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 3.035329818725586, 'weight_rejected': 0.498504638671875, 'kl_term_chosen': -2.538106918334961, 'epoch': 0.06680472977486807}
                                                    {'loss': 0.0626, 'grad_norm': 648.0829467773438, 'learning_rate': 6.64e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 3.035329818725586, 'weight_rejected': 0.498504638671875, 'kl_term_chosen': -2.538106918334961, 'epoch': 0.07}
  7%|▋         | 250/3743 [19:39<4:25:43,  4.56s/it]  7%|▋         | 251/3743 [19:42<4:10:17,  4.30s/it]  7%|▋         | 252/3743 [19:47<4:11:49,  4.33s/it]  7%|▋         | 253/3743 [19:53<4:42:01,  4.85s/it]  7%|▋         | 254/3743 [19:57<4:22:51,  4.52s/it]  7%|▋         | 255/3743 [20:03<4:54:29,  5.07s/it]  7%|▋         | 256/3743 [20:06<4:25:22,  4.57s/it]  7%|▋         | 257/3743 [20:13<5:03:18,  5.22s/it]  7%|▋         | 258/3743 [20:17<4:43:56,  4.89s/it]  7%|▋         | 259/3743 [20:22<4:51:05,  5.01s/it]  7%|▋         | 260/3743 [20:27<4:48:58,  4.98s/it]{'loss': 0.3966, 'grad_norm': 361.5814208984375, 'learning_rate': 6.906666666666666e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 1.7447662353515625, 'weight_rejected': 0.49951171875, 'kl_term_chosen': -1.2451934814453125, 'epoch': 0.06947691896586278}
                                                    {'loss': 0.3966, 'grad_norm': 361.5814208984375, 'learning_rate': 6.906666666666666e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 1.7447662353515625, 'weight_rejected': 0.49951171875, 'kl_term_chosen': -1.2451934814453125, 'epoch': 0.07}
  7%|▋         | 260/3743 [20:27<4:48:58,  4.98s/it]  7%|▋         | 261/3743 [20:31<4:33:04,  4.71s/it]  7%|▋         | 262/3743 [20:37<4:56:36,  5.11s/it]  7%|▋         | 263/3743 [20:41<4:30:33,  4.66s/it]  7%|▋         | 264/3743 [20:46<4:41:43,  4.86s/it]  7%|▋         | 265/3743 [20:51<4:34:04,  4.73s/it]  7%|▋         | 266/3743 [20:55<4:30:44,  4.67s/it]  7%|▋         | 267/3743 [21:00<4:24:33,  4.57s/it]  7%|▋         | 268/3743 [21:03<4:10:24,  4.32s/it]  7%|▋         | 269/3743 [21:08<4:13:59,  4.39s/it]  7%|▋         | 270/3743 [21:12<4:04:31,  4.22s/it]{'loss': 0.1828, 'grad_norm': 1665.79052734375, 'learning_rate': 7.173333333333333e-07, 'mean_ratio_chosen': 1.5039520263671875, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 0.2933273911476135, 'weight_rejected': 0.4965210556983948, 'kl_term_chosen': 0.20404815673828125, 'epoch': 0.0721491081568575}
                                                    {'loss': 0.1828, 'grad_norm': 1665.79052734375, 'learning_rate': 7.173333333333333e-07, 'mean_ratio_chosen': 1.5039520263671875, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 0.2933273911476135, 'weight_rejected': 0.4965210556983948, 'kl_term_chosen': 0.20404815673828125, 'epoch': 0.07}
  7%|▋         | 270/3743 [21:12<4:04:31,  4.22s/it]  7%|▋         | 271/3743 [21:18<4:32:45,  4.71s/it]  7%|▋         | 272/3743 [21:24<4:51:50,  5.04s/it]  7%|▋         | 273/3743 [21:27<4:33:15,  4.72s/it]  7%|▋         | 274/3743 [21:32<4:23:25,  4.56s/it]  7%|▋         | 275/3743 [21:36<4:17:58,  4.46s/it]  7%|▋         | 276/3743 [21:41<4:35:21,  4.77s/it]  7%|▋         | 277/3743 [21:46<4:40:50,  4.86s/it]  7%|▋         | 278/3743 [21:51<4:40:44,  4.86s/it]  7%|▋         | 279/3743 [21:55<4:19:05,  4.49s/it]  7%|▋         | 280/3743 [22:00<4:23:30,  4.57s/it]{'loss': 0.2093, 'grad_norm': 2227.078857421875, 'learning_rate': 7.44e-07, 'mean_ratio_chosen': 0.7551161050796509, 'mean_ratio_rejected': 1.8021658658981323, 'weight_chosen': 0.6373901963233948, 'weight_rejected': 0.4972534775733948, 'kl_term_chosen': -0.14044189453125, 'epoch': 0.07482129734785223}
                                                    {'loss': 0.2093, 'grad_norm': 2227.078857421875, 'learning_rate': 7.44e-07, 'mean_ratio_chosen': 0.7551161050796509, 'mean_ratio_rejected': 1.8021658658981323, 'weight_chosen': 0.6373901963233948, 'weight_rejected': 0.4972534775733948, 'kl_term_chosen': -0.14044189453125, 'epoch': 0.07}
  7%|▋         | 280/3743 [22:00<4:23:30,  4.57s/it]  8%|▊         | 281/3743 [22:05<4:38:27,  4.83s/it]  8%|▊         | 282/3743 [22:10<4:40:41,  4.87s/it]  8%|▊         | 283/3743 [22:14<4:29:00,  4.66s/it]  8%|▊         | 284/3743 [22:18<4:17:46,  4.47s/it]  8%|▊         | 285/3743 [22:22<4:06:57,  4.28s/it]  8%|▊         | 286/3743 [22:26<4:07:23,  4.29s/it]  8%|▊         | 287/3743 [22:30<3:58:40,  4.14s/it]  8%|▊         | 288/3743 [22:35<4:02:30,  4.21s/it]  8%|▊         | 289/3743 [22:38<3:49:29,  3.99s/it]  8%|▊         | 290/3743 [22:42<3:40:07,  3.82s/it]{'loss': 0.8951, 'grad_norm': 305.8030090332031, 'learning_rate': 7.706666666666667e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.0975494384765625, 'weight_rejected': 0.4989013671875, 'kl_term_chosen': -1.5998077392578125, 'epoch': 0.07749348653884695}
                                                    {'loss': 0.8951, 'grad_norm': 305.8030090332031, 'learning_rate': 7.706666666666667e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.0975494384765625, 'weight_rejected': 0.4989013671875, 'kl_term_chosen': -1.5998077392578125, 'epoch': 0.08}
  8%|▊         | 290/3743 [22:42<3:40:07,  3.82s/it]  8%|▊         | 291/3743 [22:46<3:45:47,  3.92s/it]  8%|▊         | 292/3743 [22:50<3:54:28,  4.08s/it]  8%|▊         | 293/3743 [22:53<3:39:33,  3.82s/it]  8%|▊         | 294/3743 [22:57<3:36:50,  3.77s/it]  8%|▊         | 295/3743 [23:01<3:38:13,  3.80s/it]  8%|▊         | 296/3743 [23:06<4:09:05,  4.34s/it]  8%|▊         | 297/3743 [23:10<4:00:19,  4.18s/it]  8%|▊         | 298/3743 [23:14<3:57:55,  4.14s/it]  8%|▊         | 299/3743 [23:19<4:05:31,  4.28s/it]  8%|▊         | 300/3743 [23:23<4:01:48,  4.21s/it]{'loss': 0.6766, 'grad_norm': 341.198974609375, 'learning_rate': 7.973333333333333e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.463472366333008, 'weight_rejected': 0.498748779296875, 'kl_term_chosen': -1.9661884307861328, 'epoch': 0.08016567572984168}
                                                    {'loss': 0.6766, 'grad_norm': 341.198974609375, 'learning_rate': 7.973333333333333e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.463472366333008, 'weight_rejected': 0.498748779296875, 'kl_term_chosen': -1.9661884307861328, 'epoch': 0.08}
  8%|▊         | 300/3743 [23:23<4:01:48,  4.21s/it]  8%|▊         | 301/3743 [23:27<4:06:06,  4.29s/it]  8%|▊         | 302/3743 [23:32<4:10:28,  4.37s/it]  8%|▊         | 303/3743 [23:36<3:58:39,  4.16s/it]  8%|▊         | 304/3743 [23:39<3:50:34,  4.02s/it]  8%|▊         | 305/3743 [23:44<4:06:40,  4.31s/it]  8%|▊         | 306/3743 [23:48<3:51:45,  4.05s/it]  8%|▊         | 307/3743 [23:52<3:50:43,  4.03s/it]  8%|▊         | 308/3743 [23:56<3:47:49,  3.98s/it]  8%|▊         | 309/3743 [24:02<4:27:03,  4.67s/it]  8%|▊         | 310/3743 [24:10<5:27:12,  5.72s/it]{'loss': 1.0826, 'grad_norm': 812.761962890625, 'learning_rate': 8.24e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 1.9327316284179688, 'weight_rejected': 0.49920654296875, 'kl_term_chosen': -1.4337692260742188, 'epoch': 0.08283786492083639}
                                                    {'loss': 1.0826, 'grad_norm': 812.761962890625, 'learning_rate': 8.24e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 1.9327316284179688, 'weight_rejected': 0.49920654296875, 'kl_term_chosen': -1.4337692260742188, 'epoch': 0.08}
  8%|▊         | 310/3743 [24:10<5:27:12,  5.72s/it]  8%|▊         | 311/3743 [24:16<5:30:46,  5.78s/it]  8%|▊         | 312/3743 [24:22<5:29:19,  5.76s/it]  8%|▊         | 313/3743 [24:26<5:09:46,  5.42s/it]  8%|▊         | 314/3743 [24:33<5:22:48,  5.65s/it]  8%|▊         | 315/3743 [24:36<4:47:12,  5.03s/it]  8%|▊         | 316/3743 [24:40<4:33:12,  4.78s/it]  8%|▊         | 317/3743 [24:45<4:24:04,  4.62s/it]  8%|▊         | 318/3743 [24:49<4:13:42,  4.44s/it]  9%|▊         | 319/3743 [24:52<3:51:14,  4.05s/it]  9%|▊         | 320/3743 [24:56<3:51:09,  4.05s/it]{'loss': -0.3216, 'grad_norm': 0.0, 'learning_rate': 8.506666666666667e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 7.5064849853515625, 'weight_rejected': 0.4979248344898224, 'kl_term_chosen': -7.0083770751953125, 'epoch': 0.08551005411183112}
                                                    {'loss': -0.3216, 'grad_norm': 0.0, 'learning_rate': 8.506666666666667e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 7.5064849853515625, 'weight_rejected': 0.4979248344898224, 'kl_term_chosen': -7.0083770751953125, 'epoch': 0.09}
  9%|▊         | 320/3743 [24:56<3:51:09,  4.05s/it]  9%|▊         | 321/3743 [25:00<3:46:52,  3.98s/it]  9%|▊         | 322/3743 [25:04<3:49:18,  4.02s/it]  9%|▊         | 323/3743 [25:07<3:33:55,  3.75s/it]  9%|▊         | 324/3743 [25:15<4:40:49,  4.93s/it]  9%|▊         | 325/3743 [25:18<4:24:28,  4.64s/it]  9%|▊         | 326/3743 [25:23<4:22:32,  4.61s/it]  9%|▊         | 327/3743 [25:27<4:10:13,  4.40s/it]  9%|▉         | 328/3743 [25:31<4:02:38,  4.26s/it]  9%|▉         | 329/3743 [25:35<3:56:18,  4.15s/it]  9%|▉         | 330/3743 [25:41<4:25:02,  4.66s/it]{'loss': -0.6778, 'grad_norm': 0.0, 'learning_rate': 8.773333333333332e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 9.03180980682373, 'weight_rejected': 0.47481569647789, 'kl_term_chosen': -8.544891357421875, 'epoch': 0.08818224330282584}
                                                    {'loss': -0.6778, 'grad_norm': 0.0, 'learning_rate': 8.773333333333332e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 9.03180980682373, 'weight_rejected': 0.47481569647789, 'kl_term_chosen': -8.544891357421875, 'epoch': 0.09}
  9%|▉         | 330/3743 [25:41<4:25:02,  4.66s/it]  9%|▉         | 331/3743 [25:44<4:03:14,  4.28s/it]  9%|▉         | 332/3743 [25:49<4:13:36,  4.46s/it]  9%|▉         | 333/3743 [25:54<4:19:43,  4.57s/it]  9%|▉         | 334/3743 [25:57<3:59:40,  4.22s/it]  9%|▉         | 335/3743 [26:01<4:00:20,  4.23s/it]  9%|▉         | 336/3743 [26:05<3:45:42,  3.97s/it]  9%|▉         | 337/3743 [26:11<4:21:00,  4.60s/it]  9%|▉         | 338/3743 [26:18<5:06:47,  5.41s/it]  9%|▉         | 339/3743 [26:22<4:45:21,  5.03s/it]  9%|▉         | 340/3743 [26:26<4:31:53,  4.79s/it]{'loss': -0.7934, 'grad_norm': 0.0, 'learning_rate': 9.039999999999999e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 10.215217590332031, 'weight_rejected': 0.49951171875, 'kl_term_chosen': -9.716255187988281, 'epoch': 0.09085443249382057}
                                                    {'loss': -0.7934, 'grad_norm': 0.0, 'learning_rate': 9.039999999999999e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 10.215217590332031, 'weight_rejected': 0.49951171875, 'kl_term_chosen': -9.716255187988281, 'epoch': 0.09}
  9%|▉         | 340/3743 [26:27<4:31:53,  4.79s/it]  9%|▉         | 341/3743 [26:31<4:26:06,  4.69s/it]  9%|▉         | 342/3743 [26:34<4:03:51,  4.30s/it]  9%|▉         | 343/3743 [26:39<4:11:28,  4.44s/it]  9%|▉         | 344/3743 [26:42<3:50:06,  4.06s/it]  9%|▉         | 345/3743 [26:47<3:55:30,  4.16s/it]  9%|▉         | 346/3743 [26:51<3:55:02,  4.15s/it]  9%|▉         | 347/3743 [26:55<3:50:00,  4.06s/it]  9%|▉         | 348/3743 [27:01<4:24:21,  4.67s/it]  9%|▉         | 349/3743 [27:04<4:05:47,  4.35s/it]  9%|▉         | 350/3743 [27:08<4:01:09,  4.26s/it]{'loss': -0.8395, 'grad_norm': 0.0, 'learning_rate': 9.306666666666666e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 10.957059860229492, 'weight_rejected': 0.49871826171875, 'kl_term_chosen': -10.458524703979492, 'epoch': 0.09352662168481528}
                                                    {'loss': -0.8395, 'grad_norm': 0.0, 'learning_rate': 9.306666666666666e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 10.957059860229492, 'weight_rejected': 0.49871826171875, 'kl_term_chosen': -10.458524703979492, 'epoch': 0.09}
  9%|▉         | 350/3743 [27:08<4:01:09,  4.26s/it]  9%|▉         | 351/3743 [27:12<3:48:16,  4.04s/it]  9%|▉         | 352/3743 [27:15<3:38:26,  3.87s/it]  9%|▉         | 353/3743 [27:23<4:48:53,  5.11s/it]  9%|▉         | 354/3743 [27:28<4:36:56,  4.90s/it]  9%|▉         | 355/3743 [27:33<4:41:46,  4.99s/it] 10%|▉         | 356/3743 [27:37<4:32:55,  4.83s/it] 10%|▉         | 357/3743 [27:42<4:34:15,  4.86s/it] 10%|▉         | 358/3743 [27:46<4:12:11,  4.47s/it] 10%|▉         | 359/3743 [27:50<4:04:23,  4.33s/it] 10%|▉         | 360/3743 [27:56<4:27:34,  4.75s/it]{'loss': -0.833, 'grad_norm': 0.0, 'learning_rate': 9.573333333333333e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 11.358806610107422, 'weight_rejected': 0.49603283405303955, 'kl_term_chosen': -10.862468719482422, 'epoch': 0.09619881087581}
                                                    {'loss': -0.833, 'grad_norm': 0.0, 'learning_rate': 9.573333333333333e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 11.358806610107422, 'weight_rejected': 0.49603283405303955, 'kl_term_chosen': -10.862468719482422, 'epoch': 0.1}
 10%|▉         | 360/3743 [27:56<4:27:34,  4.75s/it] 10%|▉         | 361/3743 [28:00<4:14:05,  4.51s/it] 10%|▉         | 362/3743 [28:07<5:09:33,  5.49s/it] 10%|▉         | 363/3743 [28:12<4:54:13,  5.22s/it] 10%|▉         | 364/3743 [28:17<4:46:56,  5.10s/it] 10%|▉         | 365/3743 [28:21<4:28:26,  4.77s/it] 10%|▉         | 366/3743 [28:27<4:48:06,  5.12s/it] 10%|▉         | 367/3743 [28:31<4:27:22,  4.75s/it] 10%|▉         | 368/3743 [28:35<4:22:13,  4.66s/it] 10%|▉         | 369/3743 [28:39<4:12:03,  4.48s/it] 10%|▉         | 370/3743 [28:45<4:28:51,  4.78s/it]{'loss': -0.8197, 'grad_norm': 0.0, 'learning_rate': 9.84e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 7.576447010040283, 'weight_rejected': 0.4968873858451843, 'kl_term_chosen': -7.083404541015625, 'epoch': 0.09887100006680473}
                                                    {'loss': -0.8197, 'grad_norm': 0.0, 'learning_rate': 9.84e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 7.576447010040283, 'weight_rejected': 0.4968873858451843, 'kl_term_chosen': -7.083404541015625, 'epoch': 0.1}
 10%|▉         | 370/3743 [28:45<4:28:51,  4.78s/it] 10%|▉         | 371/3743 [28:51<4:47:43,  5.12s/it] 10%|▉         | 372/3743 [28:54<4:22:56,  4.68s/it] 10%|▉         | 373/3743 [28:57<3:58:07,  4.24s/it] 10%|▉         | 374/3743 [29:02<4:01:15,  4.30s/it] 10%|█         | 375/3743 [29:06<3:50:28,  4.11s/it] 10%|█         | 376/3743 [29:09<3:37:21,  3.87s/it] 10%|█         | 377/3743 [29:14<3:59:01,  4.26s/it] 10%|█         | 378/3743 [29:18<3:51:02,  4.12s/it] 10%|█         | 379/3743 [29:22<3:55:16,  4.20s/it] 10%|█         | 380/3743 [29:27<3:59:32,  4.27s/it]{'loss': -0.8279, 'grad_norm': 0.0, 'learning_rate': 9.999965197129363e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 9.453685760498047, 'weight_rejected': 0.4977417588233948, 'kl_term_chosen': -8.958446502685547, 'epoch': 0.10154318925779945}
                                                    {'loss': -0.8279, 'grad_norm': 0.0, 'learning_rate': 9.999965197129363e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 9.453685760498047, 'weight_rejected': 0.4977417588233948, 'kl_term_chosen': -8.958446502685547, 'epoch': 0.1}
 10%|█         | 380/3743 [29:27<3:59:32,  4.27s/it] 10%|█         | 381/3743 [29:33<4:32:06,  4.86s/it] 10%|█         | 382/3743 [29:37<4:16:07,  4.57s/it] 10%|█         | 383/3743 [29:40<3:59:43,  4.28s/it] 10%|█         | 384/3743 [29:45<4:04:54,  4.37s/it] 10%|█         | 385/3743 [29:49<4:00:21,  4.29s/it] 10%|█         | 386/3743 [29:53<3:53:08,  4.17s/it] 10%|█         | 387/3743 [29:57<3:47:12,  4.06s/it] 10%|█         | 388/3743 [30:01<3:48:04,  4.08s/it] 10%|█         | 389/3743 [30:05<3:48:26,  4.09s/it] 10%|█         | 390/3743 [30:13<4:56:09,  5.30s/it]{'loss': -0.8421, 'grad_norm': 0.0, 'learning_rate': 9.999573670398804e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 6.654624938964844, 'weight_rejected': 0.4986572265625, 'kl_term_chosen': -6.156333923339844, 'epoch': 0.10421537844879418}
                                                    {'loss': -0.8421, 'grad_norm': 0.0, 'learning_rate': 9.999573670398804e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 6.654624938964844, 'weight_rejected': 0.4986572265625, 'kl_term_chosen': -6.156333923339844, 'epoch': 0.1}
 10%|█         | 390/3743 [30:13<4:56:09,  5.30s/it] 10%|█         | 391/3743 [30:18<4:43:51,  5.08s/it] 10%|█         | 392/3743 [30:26<5:38:14,  6.06s/it] 10%|█         | 393/3743 [30:30<4:57:54,  5.34s/it] 11%|█         | 394/3743 [30:34<4:38:56,  5.00s/it] 11%|█         | 395/3743 [30:39<4:37:19,  4.97s/it] 11%|█         | 396/3743 [30:43<4:17:36,  4.62s/it] 11%|█         | 397/3743 [30:46<3:54:23,  4.20s/it] 11%|█         | 398/3743 [30:51<4:06:18,  4.42s/it] 11%|█         | 399/3743 [30:55<3:56:25,  4.24s/it] 11%|█         | 400/3743 [30:59<3:52:04,  4.17s/it]{'loss': -0.8702, 'grad_norm': 0.0, 'learning_rate': 9.998747147528373e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 10.218605041503906, 'weight_rejected': 0.4935917556285858, 'kl_term_chosen': -9.723793029785156, 'epoch': 0.10688756763978889}
                                                    {'loss': -0.8702, 'grad_norm': 0.0, 'learning_rate': 9.998747147528373e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 10.218605041503906, 'weight_rejected': 0.4935917556285858, 'kl_term_chosen': -9.723793029785156, 'epoch': 0.11}
 11%|█         | 400/3743 [30:59<3:52:04,  4.17s/it] 11%|█         | 401/3743 [31:02<3:48:04,  4.09s/it] 11%|█         | 402/3743 [31:06<3:40:36,  3.96s/it] 11%|█         | 403/3743 [31:10<3:43:08,  4.01s/it] 11%|█         | 404/3743 [31:15<3:55:10,  4.23s/it] 11%|█         | 405/3743 [31:19<3:46:39,  4.07s/it] 11%|█         | 406/3743 [31:25<4:17:12,  4.62s/it] 11%|█         | 407/3743 [31:29<4:08:07,  4.46s/it] 11%|█         | 408/3743 [31:33<4:05:16,  4.41s/it] 11%|█         | 409/3743 [31:37<4:01:53,  4.35s/it] 11%|█         | 410/3743 [31:41<4:00:45,  4.33s/it]{'loss': -0.8431, 'grad_norm': 0.0, 'learning_rate': 9.997485700431054e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 12.573348999023438, 'weight_rejected': 0.49945068359375, 'kl_term_chosen': -12.074020385742188, 'epoch': 0.10955975683078362}
                                                    {'loss': -0.8431, 'grad_norm': 0.0, 'learning_rate': 9.997485700431054e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 12.573348999023438, 'weight_rejected': 0.49945068359375, 'kl_term_chosen': -12.074020385742188, 'epoch': 0.11}
 11%|█         | 410/3743 [31:42<4:00:45,  4.33s/it] 11%|█         | 411/3743 [31:46<4:04:39,  4.41s/it] 11%|█         | 412/3743 [31:50<3:55:31,  4.24s/it] 11%|█         | 413/3743 [31:55<4:05:20,  4.42s/it] 11%|█         | 414/3743 [32:00<4:13:28,  4.57s/it] 11%|█         | 415/3743 [32:06<4:40:11,  5.05s/it] 11%|█         | 416/3743 [32:11<4:35:48,  4.97s/it] 11%|█         | 417/3743 [32:17<4:53:38,  5.30s/it] 11%|█         | 418/3743 [32:22<4:53:56,  5.30s/it] 11%|█         | 419/3743 [32:27<4:43:05,  5.11s/it] 11%|█         | 420/3743 [32:30<4:21:43,  4.73s/it]{'loss': -0.8171, 'grad_norm': 0.0, 'learning_rate': 9.995789438861126e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 6.0643157958984375, 'weight_rejected': 0.4952394962310791, 'kl_term_chosen': -5.5680999755859375, 'epoch': 0.11223194602177834}
                                                    {'loss': -0.8171, 'grad_norm': 0.0, 'learning_rate': 9.995789438861126e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 6.0643157958984375, 'weight_rejected': 0.4952394962310791, 'kl_term_chosen': -5.5680999755859375, 'epoch': 0.11}
 11%|█         | 420/3743 [32:31<4:21:43,  4.73s/it] 11%|█         | 421/3743 [32:35<4:26:11,  4.81s/it] 11%|█▏        | 422/3743 [32:40<4:15:39,  4.62s/it] 11%|█▏        | 423/3743 [32:43<3:49:31,  4.15s/it] 11%|█▏        | 424/3743 [32:48<4:01:48,  4.37s/it] 11%|█▏        | 425/3743 [32:51<3:50:35,  4.17s/it] 11%|█▏        | 426/3743 [32:56<3:56:43,  4.28s/it] 11%|█▏        | 427/3743 [33:00<3:49:53,  4.16s/it] 11%|█▏        | 428/3743 [33:04<3:50:33,  4.17s/it] 11%|█▏        | 429/3743 [33:08<3:48:17,  4.13s/it] 11%|█▏        | 430/3743 [33:16<4:46:10,  5.18s/it]{'loss': -0.0591, 'grad_norm': 0.0, 'learning_rate': 9.993658510404621e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 8.857025146484375, 'weight_rejected': 0.4934543967247009, 'kl_term_chosen': -8.36077880859375, 'epoch': 0.11490413521277307}
                                                    {'loss': -0.0591, 'grad_norm': 0.0, 'learning_rate': 9.993658510404621e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 8.857025146484375, 'weight_rejected': 0.4934543967247009, 'kl_term_chosen': -8.36077880859375, 'epoch': 0.11}
 11%|█▏        | 430/3743 [33:16<4:46:10,  5.18s/it] 12%|█▏        | 431/3743 [33:20<4:31:41,  4.92s/it] 12%|█▏        | 432/3743 [33:24<4:14:06,  4.60s/it] 12%|█▏        | 433/3743 [33:28<4:14:14,  4.61s/it] 12%|█▏        | 434/3743 [33:32<4:05:48,  4.46s/it] 12%|█▏        | 435/3743 [33:37<3:59:08,  4.34s/it] 12%|█▏        | 436/3743 [33:41<4:07:27,  4.49s/it] 12%|█▏        | 437/3743 [33:45<3:56:45,  4.30s/it] 12%|█▏        | 438/3743 [33:49<3:52:16,  4.22s/it] 12%|█▏        | 439/3743 [33:53<3:38:57,  3.98s/it] 12%|█▏        | 440/3743 [34:00<4:31:57,  4.94s/it]{'loss': 0.5297, 'grad_norm': 345.8756408691406, 'learning_rate': 9.991093100466482e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 11.154193878173828, 'weight_rejected': 0.49572768807411194, 'kl_term_chosen': -10.658008575439453, 'epoch': 0.11757632440376779}
                                                    {'loss': 0.5297, 'grad_norm': 345.8756408691406, 'learning_rate': 9.991093100466482e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 11.154193878173828, 'weight_rejected': 0.49572768807411194, 'kl_term_chosen': -10.658008575439453, 'epoch': 0.12}
 12%|█▏        | 440/3743 [34:00<4:31:57,  4.94s/it] 12%|█▏        | 441/3743 [34:04<4:19:46,  4.72s/it] 12%|█▏        | 442/3743 [34:09<4:20:55,  4.74s/it] 12%|█▏        | 443/3743 [34:13<4:16:40,  4.67s/it] 12%|█▏        | 444/3743 [34:18<4:18:51,  4.71s/it] 12%|█▏        | 445/3743 [34:22<4:07:32,  4.50s/it] 12%|█▏        | 446/3743 [34:26<4:02:25,  4.41s/it] 12%|█▏        | 447/3743 [34:31<4:02:40,  4.42s/it] 12%|█▏        | 448/3743 [34:36<4:15:49,  4.66s/it] 12%|█▏        | 449/3743 [34:40<4:03:34,  4.44s/it] 12%|█▏        | 450/3743 [34:48<5:00:48,  5.48s/it]{'loss': 0.8349, 'grad_norm': 0.0, 'learning_rate': 9.98809343225442e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 5.7480316162109375, 'weight_rejected': 0.4948732852935791, 'kl_term_chosen': -5.2529754638671875, 'epoch': 0.1202485135947625}
                                                    {'loss': 0.8349, 'grad_norm': 0.0, 'learning_rate': 9.98809343225442e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 5.7480316162109375, 'weight_rejected': 0.4948732852935791, 'kl_term_chosen': -5.2529754638671875, 'epoch': 0.12}
 12%|█▏        | 450/3743 [34:48<5:00:48,  5.48s/it] 12%|█▏        | 451/3743 [34:52<4:37:25,  5.06s/it] 12%|█▏        | 452/3743 [34:57<4:30:37,  4.93s/it] 12%|█▏        | 453/3743 [35:00<4:11:26,  4.59s/it] 12%|█▏        | 454/3743 [35:04<3:57:05,  4.33s/it] 12%|█▏        | 455/3743 [35:08<3:55:30,  4.30s/it] 12%|█▏        | 456/3743 [35:16<4:56:18,  5.41s/it] 12%|█▏        | 457/3743 [35:20<4:25:00,  4.84s/it] 12%|█▏        | 458/3743 [35:23<3:59:54,  4.38s/it] 12%|█▏        | 459/3743 [35:29<4:16:35,  4.69s/it] 12%|█▏        | 460/3743 [35:32<3:58:46,  4.36s/it]{'loss': -0.8435, 'grad_norm': 0.0, 'learning_rate': 9.984659766759508e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 8.461196899414062, 'weight_rejected': 0.49896240234375, 'kl_term_chosen': -7.9626007080078125, 'epoch': 0.12292070278575723}
                                                    {'loss': -0.8435, 'grad_norm': 0.0, 'learning_rate': 9.984659766759508e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 8.461196899414062, 'weight_rejected': 0.49896240234375, 'kl_term_chosen': -7.9626007080078125, 'epoch': 0.12}
 12%|█▏        | 460/3743 [35:32<3:58:46,  4.36s/it] 12%|█▏        | 461/3743 [35:37<4:14:04,  4.64s/it] 12%|█▏        | 462/3743 [35:41<3:56:33,  4.33s/it] 12%|█▏        | 463/3743 [35:44<3:40:51,  4.04s/it] 12%|█▏        | 464/3743 [35:48<3:33:06,  3.90s/it] 12%|█▏        | 465/3743 [35:52<3:42:05,  4.07s/it] 12%|█▏        | 466/3743 [35:57<3:51:26,  4.24s/it] 12%|█▏        | 467/3743 [36:02<4:03:51,  4.47s/it] 13%|█▎        | 468/3743 [36:06<3:55:54,  4.32s/it] 13%|█▎        | 469/3743 [36:10<3:47:59,  4.18s/it] 13%|█▎        | 470/3743 [36:14<3:48:08,  4.18s/it]{'loss': -0.5383, 'grad_norm': 0.0, 'learning_rate': 9.98079240273347e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 8.12762451171875, 'weight_rejected': 0.49658212065696716, 'kl_term_chosen': -7.63092041015625, 'epoch': 0.12559289197675194}
                                                    {'loss': -0.5383, 'grad_norm': 0.0, 'learning_rate': 9.98079240273347e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 8.12762451171875, 'weight_rejected': 0.49658212065696716, 'kl_term_chosen': -7.63092041015625, 'epoch': 0.13}
 13%|█▎        | 470/3743 [36:14<3:48:08,  4.18s/it] 13%|█▎        | 471/3743 [36:19<3:58:52,  4.38s/it] 13%|█▎        | 472/3743 [36:23<3:58:25,  4.37s/it] 13%|█▎        | 473/3743 [36:30<4:35:14,  5.05s/it] 13%|█▎        | 474/3743 [36:33<4:08:19,  4.56s/it] 13%|█▎        | 475/3743 [36:38<4:06:36,  4.53s/it] 13%|█▎        | 476/3743 [36:42<4:00:01,  4.41s/it] 13%|█▎        | 477/3743 [36:46<3:55:59,  4.34s/it] 13%|█▎        | 478/3743 [36:50<3:54:51,  4.32s/it] 13%|█▎        | 479/3743 [36:54<3:49:20,  4.22s/it] 13%|█▎        | 480/3743 [36:57<3:29:11,  3.85s/it]{'loss': -0.8667, 'grad_norm': 0.0, 'learning_rate': 9.976491676662678e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 9.209716796875, 'weight_rejected': 0.4986877739429474, 'kl_term_chosen': -8.7137451171875, 'epoch': 0.12826508116774668}
                                                    {'loss': -0.8667, 'grad_norm': 0.0, 'learning_rate': 9.976491676662678e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 9.209716796875, 'weight_rejected': 0.4986877739429474, 'kl_term_chosen': -8.7137451171875, 'epoch': 0.13}
 13%|█▎        | 480/3743 [36:57<3:29:11,  3.85s/it] 13%|█▎        | 481/3743 [37:05<4:38:42,  5.13s/it] 13%|█▎        | 482/3743 [37:11<4:40:37,  5.16s/it] 13%|█▎        | 483/3743 [37:15<4:23:58,  4.86s/it] 13%|█▎        | 484/3743 [37:19<4:07:38,  4.56s/it] 13%|█▎        | 485/3743 [37:22<3:49:33,  4.23s/it] 13%|█▎        | 486/3743 [37:27<3:59:34,  4.41s/it] 13%|█▎        | 487/3743 [37:31<3:44:52,  4.14s/it] 13%|█▎        | 488/3743 [37:35<3:48:33,  4.21s/it] 13%|█▎        | 489/3743 [37:40<4:10:44,  4.62s/it] 13%|█▎        | 490/3743 [37:45<4:06:01,  4.54s/it]{'loss': -0.6772, 'grad_norm': 613.0901489257812, 'learning_rate': 9.971757962738888e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 10.310776710510254, 'weight_rejected': 0.4874446988105774, 'kl_term_chosen': -9.815078735351562, 'epoch': 0.1309372703587414}
                                                    {'loss': -0.6772, 'grad_norm': 613.0901489257812, 'learning_rate': 9.971757962738888e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 10.310776710510254, 'weight_rejected': 0.4874446988105774, 'kl_term_chosen': -9.815078735351562, 'epoch': 0.13}
 13%|█▎        | 490/3743 [37:45<4:06:01,  4.54s/it] 13%|█▎        | 491/3743 [37:51<4:40:15,  5.17s/it] 13%|█▎        | 492/3743 [37:56<4:23:00,  4.85s/it] 13%|█▎        | 493/3743 [37:59<4:03:20,  4.49s/it] 13%|█▎        | 494/3743 [38:11<5:55:30,  6.57s/it] 13%|█▎        | 495/3743 [38:16<5:28:12,  6.06s/it] 13%|█▎        | 496/3743 [38:20<4:55:53,  5.47s/it] 13%|█▎        | 497/3743 [38:26<5:06:45,  5.67s/it] 13%|█▎        | 498/3743 [38:30<4:39:36,  5.17s/it] 13%|█▎        | 499/3743 [38:34<4:19:01,  4.79s/it] 13%|█▎        | 500/3743 [38:37<3:58:58,  4.42s/it]{'loss': 0.0221, 'grad_norm': 0.0, 'learning_rate': 9.96659167282667e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 10.908821105957031, 'weight_rejected': 0.499420166015625, 'kl_term_chosen': -10.410530090332031, 'epoch': 0.13360945954973613}
                                                    {'loss': 0.0221, 'grad_norm': 0.0, 'learning_rate': 9.96659167282667e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 10.908821105957031, 'weight_rejected': 0.499420166015625, 'kl_term_chosen': -10.410530090332031, 'epoch': 0.13}
 13%|█▎        | 500/3743 [38:37<3:58:58,  4.42s/it]/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
 13%|█▎        | 501/3743 [39:38<19:07:46, 21.24s/it] 13%|█▎        | 502/3743 [39:42<14:26:40, 16.04s/it] 13%|█▎        | 503/3743 [39:50<12:29:33, 13.88s/it] 13%|█▎        | 504/3743 [39:55<9:58:17, 11.08s/it]  13%|█▎        | 505/3743 [40:03<9:11:58, 10.23s/it] 14%|█▎        | 506/3743 [40:09<7:58:09,  8.86s/it] 14%|█▎        | 507/3743 [40:13<6:36:57,  7.36s/it] 14%|█▎        | 508/3743 [40:18<5:55:45,  6.60s/it] 14%|█▎        | 509/3743 [40:23<5:28:56,  6.10s/it] 14%|█▎        | 510/3743 [40:29<5:40:42,  6.32s/it]{'loss': -0.9588, 'grad_norm': 0.0, 'learning_rate': 9.960993256427594e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 11.442619323730469, 'weight_rejected': 0.49658212065696716, 'kl_term_chosen': -10.946220397949219, 'epoch': 0.13628164874073084}
                                                    {'loss': -0.9588, 'grad_norm': 0.0, 'learning_rate': 9.960993256427594e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 11.442619323730469, 'weight_rejected': 0.49658212065696716, 'kl_term_chosen': -10.946220397949219, 'epoch': 0.14}
 14%|█▎        | 510/3743 [40:29<5:40:42,  6.32s/it] 14%|█▎        | 511/3743 [40:34<5:09:47,  5.75s/it] 14%|█▎        | 512/3743 [40:42<5:42:34,  6.36s/it] 14%|█▎        | 513/3743 [40:46<5:03:51,  5.64s/it] 14%|█▎        | 514/3743 [40:49<4:35:12,  5.11s/it] 14%|█▍        | 515/3743 [40:54<4:21:06,  4.85s/it] 14%|█▍        | 516/3743 [40:57<4:03:51,  4.53s/it] 14%|█▍        | 517/3743 [41:01<3:52:39,  4.33s/it] 14%|█▍        | 518/3743 [41:06<3:55:32,  4.38s/it] 14%|█▍        | 519/3743 [41:11<4:00:54,  4.48s/it] 14%|█▍        | 520/3743 [41:14<3:51:17,  4.31s/it]{'loss': -1.164, 'grad_norm': 0.0, 'learning_rate': 9.95496320064109e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 9.34256362915039, 'weight_rejected': 0.4928910434246063, 'kl_term_chosen': -8.856201171875, 'epoch': 0.13895383793172555}
                                                    {'loss': -1.164, 'grad_norm': 0.0, 'learning_rate': 9.95496320064109e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 9.34256362915039, 'weight_rejected': 0.4928910434246063, 'kl_term_chosen': -8.856201171875, 'epoch': 0.14}
 14%|█▍        | 520/3743 [41:15<3:51:17,  4.31s/it]