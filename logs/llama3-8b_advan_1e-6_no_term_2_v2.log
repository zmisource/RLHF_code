nohup: ignoring input
[W1021 18:55:21.849390856 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 95.47it/s]
加载作为参考的SFT模型 (Reference Model)...
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
`torch_dtype` is deprecated! Use `dtype` instead!
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 97.11it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][W1021 18:55:29.547793094 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 94.38it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 91.64it/s]
加载作为参考的SFT模型 (Reference Model)...
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 94.86it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 92.24it/s]
[W1021 18:55:30.681506783 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1021 18:55:30.687829815 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 77.53it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 86.84it/s]
[W1021 18:55:30.003005002 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_term2.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_term2.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_term2.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
WARNING:accelerate.utils.other:Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_term2.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in LlamaForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in LlamaDecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1968: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.
  warnings.warn(
wandb: Tracking run with wandb version 0.21.3
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /root/wandb/offline-run-20251021_185538-67g248aa
  0%|          | 0/3753 [00:00<?, ?it/s]--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -111.5
Sample 0 - pi_logp_chosen:  -112.77057647705078
Sample 0 - Difference (pi - ref): -1.2705764770507812
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -101.0
Sample 0 - pi_logp_chosen:  -100.28032684326172
Sample 0 - Difference (pi - ref): 0.7196731567382812
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -44.25
Sample 0 - pi_logp_chosen:  -44.228355407714844
Sample 0 - Difference (pi - ref): 0.02164459228515625
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -233.0
Sample 0 - pi_logp_chosen:  -233.77731323242188
Sample 0 - Difference (pi - ref): -0.777313232421875
---------------------------------
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -66.0
Sample 0 - pi_logp_chosen:  -66.13370513916016
Sample 0 - Difference (pi - ref): -0.13370513916015625
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -210.0
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -248.0
Sample 0 - pi_logp_chosen:  -246.30413818359375
Sample 0 - pi_logp_chosen:  -208.75222778320312Sample 0 - Difference (pi - ref): 1.69586181640625
---------------------------------

Sample 0 - Difference (pi - ref): 1.247772216796875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -380.0
Sample 0 - pi_logp_chosen:  -379.6788330078125
Sample 0 - Difference (pi - ref): 0.3211669921875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -70.5
Sample 0 - pi_logp_chosen:  -70.52078247070312
Sample 0 - Difference (pi - ref): -0.020782470703125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -167.0
Sample 0 - pi_logp_chosen:  -167.49078369140625
Sample 0 - Difference (pi - ref): -0.49078369140625
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -350.0
Sample 0 - pi_logp_chosen:  -349.9153747558594
Sample 0 - Difference (pi - ref): 0.084625244140625
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -454.0
Sample 0 - pi_logp_chosen:  -452.25799560546875
Sample 0 - Difference (pi - ref): 1.74200439453125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -81.5
Sample 0 - pi_logp_chosen:  -81.52965545654297
Sample 0 - Difference (pi - ref): -0.02965545654296875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -98.5
Sample 0 - pi_logp_chosen:  -98.7940673828125
Sample 0 - Difference (pi - ref): -0.2940673828125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -252.0
Sample 0 - pi_logp_chosen:  -251.3794403076172
Sample 0 - Difference (pi - ref): 0.6205596923828125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -243.0
Sample 0 - pi_logp_chosen:  -242.4936981201172
Sample 0 - Difference (pi - ref): 0.5063018798828125
---------------------------------
  0%|          | 1/3753 [00:02<3:01:52,  2.91s/it]{'loss': -0.6325, 'grad_norm': 2700.14013671875, 'learning_rate': 0.0, 'mean_ratio_chosen': 1.859968662261963, 'weight_chosen': 0.6116420030593872, 'kl_term_chosen': 0.31027984619140625, 'epoch': 0.0002664890073284477}
                                                  {'loss': -0.6325, 'grad_norm': 2700.14013671875, 'learning_rate': 0.0, 'mean_ratio_chosen': 1.859968662261963, 'weight_chosen': 0.6116420030593872, 'kl_term_chosen': 0.31027984619140625, 'epoch': 0.0}
  0%|          | 1/3753 [00:02<3:01:52,  2.91s/it]  0%|          | 2/3753 [00:05<3:02:35,  2.92s/it]  0%|          | 3/3753 [00:08<2:55:27,  2.81s/it]  0%|          | 4/3753 [00:11<2:47:39,  2.68s/it]  0%|          | 5/3753 [00:13<2:44:53,  2.64s/it]  0%|          | 6/3753 [00:16<2:48:12,  2.69s/it]  0%|          | 7/3753 [00:18<2:44:34,  2.64s/it]  0%|          | 8/3753 [00:21<2:44:42,  2.64s/it]  0%|          | 9/3753 [00:24<2:45:27,  2.65s/it]  0%|          | 10/3753 [00:26<2:43:26,  2.62s/it]{'loss': -0.7182, 'grad_norm': 2980.849609375, 'learning_rate': 2.3936170212765956e-08, 'mean_ratio_chosen': 0.9032455086708069, 'weight_chosen': 0.29596543312072754, 'kl_term_chosen': -0.05088043212890625, 'epoch': 0.002664890073284477}
                                                   {'loss': -0.7182, 'grad_norm': 2980.849609375, 'learning_rate': 2.3936170212765956e-08, 'mean_ratio_chosen': 0.9032455086708069, 'weight_chosen': 0.29596543312072754, 'kl_term_chosen': -0.05088043212890625, 'epoch': 0.0}
  0%|          | 10/3753 [00:26<2:43:26,  2.62s/it]  0%|          | 11/3753 [00:29<2:41:48,  2.59s/it]  0%|          | 12/3753 [00:32<2:49:10,  2.71s/it]  0%|          | 13/3753 [00:34<2:46:06,  2.66s/it]  0%|          | 14/3753 [00:37<2:42:51,  2.61s/it]  0%|          | 15/3753 [00:39<2:39:39,  2.56s/it]  0%|          | 16/3753 [00:42<2:34:58,  2.49s/it]  0%|          | 17/3753 [00:44<2:31:59,  2.44s/it]  0%|          | 18/3753 [00:47<2:39:12,  2.56s/it]  1%|          | 19/3753 [00:49<2:39:31,  2.56s/it]  1%|          | 20/3753 [00:52<2:42:44,  2.62s/it]{'loss': -0.6602, 'grad_norm': 2779.239990234375, 'learning_rate': 5.053191489361702e-08, 'mean_ratio_chosen': 1.4484567642211914, 'weight_chosen': 0.5473424792289734, 'kl_term_chosen': 0.18524932861328125, 'epoch': 0.005329780146568954}
                                                   {'loss': -0.6602, 'grad_norm': 2779.239990234375, 'learning_rate': 5.053191489361702e-08, 'mean_ratio_chosen': 1.4484567642211914, 'weight_chosen': 0.5473424792289734, 'kl_term_chosen': 0.18524932861328125, 'epoch': 0.01}
  1%|          | 20/3753 [00:52<2:42:44,  2.62s/it]  1%|          | 21/3753 [00:54<2:38:22,  2.55s/it]  1%|          | 22/3753 [00:58<2:51:35,  2.76s/it]  1%|          | 23/3753 [01:00<2:51:58,  2.77s/it]  1%|          | 24/3753 [01:03<2:42:31,  2.62s/it]  1%|          | 25/3753 [01:05<2:41:07,  2.59s/it]  1%|          | 26/3753 [01:08<2:34:26,  2.49s/it]  1%|          | 27/3753 [01:10<2:34:43,  2.49s/it]  1%|          | 28/3753 [01:13<2:35:26,  2.50s/it]  1%|          | 29/3753 [01:15<2:43:27,  2.63s/it]  1%|          | 30/3753 [01:18<2:42:27,  2.62s/it]{'loss': -0.2437, 'grad_norm': 2162.470458984375, 'learning_rate': 7.712765957446808e-08, 'mean_ratio_chosen': 9.006448745727539, 'weight_chosen': -0.4710192084312439, 'kl_term_chosen': 1.0989704132080078, 'epoch': 0.007994670219853431}
                                                   {'loss': -0.2437, 'grad_norm': 2162.470458984375, 'learning_rate': 7.712765957446808e-08, 'mean_ratio_chosen': 9.006448745727539, 'weight_chosen': -0.4710192084312439, 'kl_term_chosen': 1.0989704132080078, 'epoch': 0.01}
  1%|          | 30/3753 [01:18<2:42:27,  2.62s/it]  1%|          | 31/3753 [01:21<2:50:05,  2.74s/it]  1%|          | 32/3753 [01:24<2:43:32,  2.64s/it]  1%|          | 33/3753 [01:26<2:37:28,  2.54s/it]  1%|          | 34/3753 [01:28<2:35:17,  2.51s/it]  1%|          | 35/3753 [01:31<2:36:24,  2.52s/it]  1%|          | 36/3753 [01:34<2:49:25,  2.73s/it]  1%|          | 37/3753 [01:37<2:53:27,  2.80s/it]  1%|          | 38/3753 [01:40<2:52:09,  2.78s/it]  1%|          | 39/3753 [01:42<2:49:58,  2.75s/it]  1%|          | 40/3753 [01:45<2:46:13,  2.69s/it]{'loss': 3.4919, 'grad_norm': 2692.836669921875, 'learning_rate': 1.0372340425531915e-07, 'mean_ratio_chosen': 6.969287872314453, 'weight_chosen': -0.21296215057373047, 'kl_term_chosen': 0.9707565307617188, 'epoch': 0.010659560293137908}
                                                   {'loss': 3.4919, 'grad_norm': 2692.836669921875, 'learning_rate': 1.0372340425531915e-07, 'mean_ratio_chosen': 6.969287872314453, 'weight_chosen': -0.21296215057373047, 'kl_term_chosen': 0.9707565307617188, 'epoch': 0.01}
  1%|          | 40/3753 [01:45<2:46:13,  2.69s/it]  1%|          | 41/3753 [01:48<2:44:42,  2.66s/it]  1%|          | 42/3753 [01:50<2:35:38,  2.52s/it]  1%|          | 43/3753 [01:52<2:38:50,  2.57s/it]  1%|          | 44/3753 [01:55<2:36:45,  2.54s/it]  1%|          | 45/3753 [01:58<2:40:47,  2.60s/it]  1%|          | 46/3753 [02:01<2:46:03,  2.69s/it]  1%|▏         | 47/3753 [02:03<2:41:04,  2.61s/it]  1%|▏         | 48/3753 [02:06<2:41:36,  2.62s/it]  1%|▏         | 49/3753 [02:08<2:38:02,  2.56s/it]  1%|▏         | 50/3753 [02:10<2:33:21,  2.48s/it]{'loss': 1.1418, 'grad_norm': 1706.6456298828125, 'learning_rate': 1.3031914893617022e-07, 'mean_ratio_chosen': 1.4935808181762695, 'weight_chosen': 0.6628034114837646, 'kl_term_chosen': 0.20058822631835938, 'epoch': 0.013324450366422385}
                                                   {'loss': 1.1418, 'grad_norm': 1706.6456298828125, 'learning_rate': 1.3031914893617022e-07, 'mean_ratio_chosen': 1.4935808181762695, 'weight_chosen': 0.6628034114837646, 'kl_term_chosen': 0.20058822631835938, 'epoch': 0.01}
  1%|▏         | 50/3753 [02:10<2:33:21,  2.48s/it]  1%|▏         | 51/3753 [02:13<2:34:44,  2.51s/it]  1%|▏         | 52/3753 [02:16<2:37:04,  2.55s/it]  1%|▏         | 53/3753 [02:18<2:41:52,  2.63s/it]  1%|▏         | 54/3753 [02:21<2:37:14,  2.55s/it]  1%|▏         | 55/3753 [02:24<2:56:06,  2.86s/it]  1%|▏         | 56/3753 [02:27<2:50:55,  2.77s/it]  2%|▏         | 57/3753 [02:30<2:53:30,  2.82s/it]  2%|▏         | 58/3753 [02:32<2:45:36,  2.69s/it]  2%|▏         | 59/3753 [02:34<2:38:57,  2.58s/it]  2%|▏         | 60/3753 [02:37<2:33:44,  2.50s/it]{'loss': 0.7484, 'grad_norm': 4798.0361328125, 'learning_rate': 1.5691489361702126e-07, 'mean_ratio_chosen': 2.2808380126953125, 'weight_chosen': -0.3820568919181824, 'kl_term_chosen': 0.41227149963378906, 'epoch': 0.015989340439706862}
                                                   {'loss': 0.7484, 'grad_norm': 4798.0361328125, 'learning_rate': 1.5691489361702126e-07, 'mean_ratio_chosen': 2.2808380126953125, 'weight_chosen': -0.3820568919181824, 'kl_term_chosen': 0.41227149963378906, 'epoch': 0.02}
  2%|▏         | 60/3753 [02:37<2:33:44,  2.50s/it]  2%|▏         | 61/3753 [02:39<2:35:39,  2.53s/it]  2%|▏         | 62/3753 [02:42<2:37:46,  2.56s/it]  2%|▏         | 63/3753 [02:45<2:38:42,  2.58s/it]  2%|▏         | 64/3753 [02:47<2:43:15,  2.66s/it]  2%|▏         | 65/3753 [02:50<2:38:01,  2.57s/it]  2%|▏         | 66/3753 [02:53<2:40:31,  2.61s/it]  2%|▏         | 67/3753 [02:56<2:48:26,  2.74s/it]  2%|▏         | 68/3753 [02:58<2:42:20,  2.64s/it]  2%|▏         | 69/3753 [03:01<2:42:06,  2.64s/it]  2%|▏         | 70/3753 [03:03<2:37:27,  2.57s/it]{'loss': 1.8592, 'grad_norm': 3918.506103515625, 'learning_rate': 1.8351063829787234e-07, 'mean_ratio_chosen': 5.516947269439697, 'weight_chosen': 0.10931819677352905, 'kl_term_chosen': 0.853912353515625, 'epoch': 0.018654230512991338}
                                                   {'loss': 1.8592, 'grad_norm': 3918.506103515625, 'learning_rate': 1.8351063829787234e-07, 'mean_ratio_chosen': 5.516947269439697, 'weight_chosen': 0.10931819677352905, 'kl_term_chosen': 0.853912353515625, 'epoch': 0.02}
  2%|▏         | 70/3753 [03:03<2:37:27,  2.57s/it]  2%|▏         | 71/3753 [03:06<2:39:23,  2.60s/it]  2%|▏         | 72/3753 [03:08<2:38:03,  2.58s/it]  2%|▏         | 73/3753 [03:11<2:34:15,  2.52s/it]  2%|▏         | 74/3753 [03:13<2:33:00,  2.50s/it]  2%|▏         | 75/3753 [03:15<2:30:01,  2.45s/it]  2%|▏         | 76/3753 [03:19<2:45:46,  2.71s/it]  2%|▏         | 77/3753 [03:21<2:40:40,  2.62s/it]  2%|▏         | 78/3753 [03:24<2:40:38,  2.62s/it]  2%|▏         | 79/3753 [03:26<2:36:27,  2.56s/it]  2%|▏         | 80/3753 [03:29<2:38:52,  2.60s/it]{'loss': 1.5931, 'grad_norm': 1452.470947265625, 'learning_rate': 2.101063829787234e-07, 'mean_ratio_chosen': 6.990909099578857, 'weight_chosen': -0.043896496295928955, 'kl_term_chosen': 0.9723052978515625, 'epoch': 0.021319120586275817}
                                                   {'loss': 1.5931, 'grad_norm': 1452.470947265625, 'learning_rate': 2.101063829787234e-07, 'mean_ratio_chosen': 6.990909099578857, 'weight_chosen': -0.043896496295928955, 'kl_term_chosen': 0.9723052978515625, 'epoch': 0.02}
  2%|▏         | 80/3753 [03:29<2:38:52,  2.60s/it]  2%|▏         | 81/3753 [03:32<2:42:50,  2.66s/it]  2%|▏         | 82/3753 [03:34<2:39:03,  2.60s/it]  2%|▏         | 83/3753 [03:37<2:48:38,  2.76s/it]  2%|▏         | 84/3753 [03:40<2:48:58,  2.76s/it]  2%|▏         | 85/3753 [03:42<2:42:27,  2.66s/it]  2%|▏         | 86/3753 [03:45<2:43:49,  2.68s/it]  2%|▏         | 87/3753 [03:48<2:53:07,  2.83s/it]  2%|▏         | 88/3753 [03:51<2:42:37,  2.66s/it]  2%|▏         | 89/3753 [03:53<2:40:15,  2.62s/it]  2%|▏         | 90/3753 [03:56<2:37:46,  2.58s/it]{'loss': 1.6091, 'grad_norm': 1359.82421875, 'learning_rate': 2.3670212765957445e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.9937252998352051, 'kl_term_chosen': 1.782958984375, 'epoch': 0.023984010659560292}
                                                   {'loss': 1.6091, 'grad_norm': 1359.82421875, 'learning_rate': 2.3670212765957445e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.9937252998352051, 'kl_term_chosen': 1.782958984375, 'epoch': 0.02}
  2%|▏         | 90/3753 [03:56<2:37:46,  2.58s/it]  2%|▏         | 91/3753 [03:58<2:31:22,  2.48s/it]  2%|▏         | 92/3753 [04:00<2:29:48,  2.46s/it]  2%|▏         | 93/3753 [04:03<2:31:44,  2.49s/it]  3%|▎         | 94/3753 [04:05<2:28:32,  2.44s/it]  3%|▎         | 95/3753 [04:08<2:41:07,  2.64s/it]  3%|▎         | 96/3753 [04:11<2:47:58,  2.76s/it]  3%|▎         | 97/3753 [04:14<2:42:47,  2.67s/it]  3%|▎         | 98/3753 [04:17<2:49:13,  2.78s/it]  3%|▎         | 99/3753 [04:19<2:41:46,  2.66s/it]  3%|▎         | 100/3753 [04:22<2:36:17,  2.57s/it]{'loss': 1.3971, 'grad_norm': 1415.1361083984375, 'learning_rate': 2.632978723404255e-07, 'mean_ratio_chosen': 2.3443424701690674, 'weight_chosen': 0.20194870233535767, 'kl_term_chosen': 0.42600250244140625, 'epoch': 0.02664890073284477}
                                                    {'loss': 1.3971, 'grad_norm': 1415.1361083984375, 'learning_rate': 2.632978723404255e-07, 'mean_ratio_chosen': 2.3443424701690674, 'weight_chosen': 0.20194870233535767, 'kl_term_chosen': 0.42600250244140625, 'epoch': 0.03}
  3%|▎         | 100/3753 [04:22<2:36:17,  2.57s/it]  3%|▎         | 101/3753 [04:24<2:36:48,  2.58s/it]  3%|▎         | 102/3753 [04:27<2:33:15,  2.52s/it]  3%|▎         | 103/3753 [04:29<2:31:36,  2.49s/it]  3%|▎         | 104/3753 [04:32<2:42:11,  2.67s/it]  3%|▎         | 105/3753 [04:35<2:43:45,  2.69s/it]  3%|▎         | 106/3753 [04:38<2:51:37,  2.82s/it]  3%|▎         | 107/3753 [04:41<2:48:52,  2.78s/it]  3%|▎         | 108/3753 [04:43<2:43:06,  2.68s/it]  3%|▎         | 109/3753 [04:46<2:39:16,  2.62s/it]  3%|▎         | 110/3753 [04:48<2:38:08,  2.60s/it]{'loss': 1.5837, 'grad_norm': 1269.5206298828125, 'learning_rate': 2.8989361702127657e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.26456141471862793, 'kl_term_chosen': 1.1782379150390625, 'epoch': 0.029313790806129246}
                                                    {'loss': 1.5837, 'grad_norm': 1269.5206298828125, 'learning_rate': 2.8989361702127657e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.26456141471862793, 'kl_term_chosen': 1.1782379150390625, 'epoch': 0.03}
  3%|▎         | 110/3753 [04:48<2:38:08,  2.60s/it]  3%|▎         | 111/3753 [04:51<2:40:11,  2.64s/it]  3%|▎         | 112/3753 [04:53<2:35:39,  2.57s/it]  3%|▎         | 113/3753 [04:55<2:28:40,  2.45s/it]  3%|▎         | 114/3753 [04:58<2:26:37,  2.42s/it]  3%|▎         | 115/3753 [05:01<2:39:58,  2.64s/it]  3%|▎         | 116/3753 [05:03<2:39:02,  2.62s/it]  3%|▎         | 117/3753 [05:06<2:35:32,  2.57s/it]  3%|▎         | 118/3753 [05:08<2:33:55,  2.54s/it]  3%|▎         | 119/3753 [05:11<2:28:57,  2.46s/it]  3%|▎         | 120/3753 [05:13<2:33:36,  2.54s/it]{'loss': 1.5781, 'grad_norm': 2045.717041015625, 'learning_rate': 3.1648936170212764e-07, 'mean_ratio_chosen': 1.947171926498413, 'weight_chosen': 0.5033655166625977, 'kl_term_chosen': 0.3331890106201172, 'epoch': 0.031978680879413725}
                                                    {'loss': 1.5781, 'grad_norm': 2045.717041015625, 'learning_rate': 3.1648936170212764e-07, 'mean_ratio_chosen': 1.947171926498413, 'weight_chosen': 0.5033655166625977, 'kl_term_chosen': 0.3331890106201172, 'epoch': 0.03}
  3%|▎         | 120/3753 [05:13<2:33:36,  2.54s/it]  3%|▎         | 121/3753 [05:16<2:34:38,  2.55s/it]  3%|▎         | 122/3753 [05:19<2:36:19,  2.58s/it]  3%|▎         | 123/3753 [05:21<2:34:23,  2.55s/it]  3%|▎         | 124/3753 [05:24<2:34:37,  2.56s/it]  3%|▎         | 125/3753 [05:26<2:39:33,  2.64s/it]  3%|▎         | 126/3753 [05:29<2:37:56,  2.61s/it]  3%|▎         | 127/3753 [05:31<2:32:34,  2.52s/it]  3%|▎         | 128/3753 [05:34<2:29:38,  2.48s/it]  3%|▎         | 129/3753 [05:37<2:38:08,  2.62s/it]  3%|▎         | 130/3753 [05:39<2:39:30,  2.64s/it]{'loss': 1.3124, 'grad_norm': 1314.903076171875, 'learning_rate': 3.430851063829787e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.9185945391654968, 'kl_term_chosen': 1.216064453125, 'epoch': 0.034643570952698204}
                                                    {'loss': 1.3124, 'grad_norm': 1314.903076171875, 'learning_rate': 3.430851063829787e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.9185945391654968, 'kl_term_chosen': 1.216064453125, 'epoch': 0.03}
  3%|▎         | 130/3753 [05:39<2:39:30,  2.64s/it]  3%|▎         | 131/3753 [05:42<2:36:14,  2.59s/it]  4%|▎         | 132/3753 [05:44<2:31:56,  2.52s/it]  4%|▎         | 133/3753 [05:47<2:34:08,  2.55s/it]  4%|▎         | 134/3753 [05:50<2:43:50,  2.72s/it]  4%|▎         | 135/3753 [05:52<2:38:00,  2.62s/it]  4%|▎         | 136/3753 [05:55<2:32:27,  2.53s/it]  4%|▎         | 137/3753 [05:57<2:30:53,  2.50s/it]  4%|▎         | 138/3753 [06:00<2:33:28,  2.55s/it]  4%|▎         | 139/3753 [06:02<2:29:32,  2.48s/it]  4%|▎         | 140/3753 [06:05<2:32:12,  2.53s/it]{'loss': 1.9546, 'grad_norm': 2188.89013671875, 'learning_rate': 3.696808510638298e-07, 'mean_ratio_chosen': 5.819416522979736, 'weight_chosen': -0.020936250686645508, 'kl_term_chosen': 0.8805999755859375, 'epoch': 0.037308461025982675}
                                                    {'loss': 1.9546, 'grad_norm': 2188.89013671875, 'learning_rate': 3.696808510638298e-07, 'mean_ratio_chosen': 5.819416522979736, 'weight_chosen': -0.020936250686645508, 'kl_term_chosen': 0.8805999755859375, 'epoch': 0.04}
  4%|▎         | 140/3753 [06:05<2:32:12,  2.53s/it]  4%|▍         | 141/3753 [06:07<2:31:32,  2.52s/it]  4%|▍         | 142/3753 [06:10<2:32:49,  2.54s/it]  4%|▍         | 143/3753 [06:13<2:38:10,  2.63s/it]  4%|▍         | 144/3753 [06:16<2:49:16,  2.81s/it]  4%|▍         | 145/3753 [06:19<2:57:35,  2.95s/it]  4%|▍         | 146/3753 [06:22<2:52:51,  2.88s/it]  4%|▍         | 147/3753 [06:25<2:55:48,  2.93s/it]  4%|▍         | 148/3753 [06:28<2:54:16,  2.90s/it]  4%|▍         | 149/3753 [06:30<2:46:02,  2.76s/it]  4%|▍         | 150/3753 [06:33<2:53:17,  2.89s/it]{'loss': 1.2228, 'grad_norm': 3526.851806640625, 'learning_rate': 3.962765957446808e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.822422981262207, 'kl_term_chosen': 1.1578826904296875, 'epoch': 0.039973351099267154}
                                                    {'loss': 1.2228, 'grad_norm': 3526.851806640625, 'learning_rate': 3.962765957446808e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.822422981262207, 'kl_term_chosen': 1.1578826904296875, 'epoch': 0.04}
  4%|▍         | 150/3753 [06:33<2:53:17,  2.89s/it]  4%|▍         | 151/3753 [06:36<2:51:13,  2.85s/it]  4%|▍         | 152/3753 [06:39<2:43:19,  2.72s/it]  4%|▍         | 153/3753 [06:42<2:52:28,  2.87s/it]  4%|▍         | 154/3753 [06:44<2:42:00,  2.70s/it]  4%|▍         | 155/3753 [06:47<2:38:34,  2.64s/it]  4%|▍         | 156/3753 [06:49<2:35:35,  2.60s/it]  4%|▍         | 157/3753 [06:52<2:33:31,  2.56s/it]  4%|▍         | 158/3753 [06:54<2:35:36,  2.60s/it]  4%|▍         | 159/3753 [06:57<2:34:09,  2.57s/it]  4%|▍         | 160/3753 [06:59<2:36:31,  2.61s/it]{'loss': 1.8222, 'grad_norm': 2183.489013671875, 'learning_rate': 4.228723404255319e-07, 'mean_ratio_chosen': 5.018028259277344, 'weight_chosen': -0.1264892816543579, 'kl_term_chosen': 0.8065185546875, 'epoch': 0.04263824117255163}
                                                    {'loss': 1.8222, 'grad_norm': 2183.489013671875, 'learning_rate': 4.228723404255319e-07, 'mean_ratio_chosen': 5.018028259277344, 'weight_chosen': -0.1264892816543579, 'kl_term_chosen': 0.8065185546875, 'epoch': 0.04}
  4%|▍         | 160/3753 [07:00<2:36:31,  2.61s/it]  4%|▍         | 161/3753 [07:02<2:31:47,  2.54s/it]  4%|▍         | 162/3753 [07:04<2:30:49,  2.52s/it]  4%|▍         | 163/3753 [07:07<2:33:35,  2.57s/it]  4%|▍         | 164/3753 [07:10<2:45:35,  2.77s/it]  4%|▍         | 165/3753 [07:13<2:53:41,  2.90s/it]  4%|▍         | 166/3753 [07:16<2:41:39,  2.70s/it]  4%|▍         | 167/3753 [07:18<2:39:50,  2.67s/it]  4%|▍         | 168/3753 [07:21<2:41:31,  2.70s/it]  5%|▍         | 169/3753 [07:23<2:35:29,  2.60s/it]  5%|▍         | 170/3753 [07:26<2:29:22,  2.50s/it]{'loss': 0.7086, 'grad_norm': 2013.9498291015625, 'learning_rate': 4.4946808510638295e-07, 'mean_ratio_chosen': 2.9629387855529785, 'weight_chosen': 0.329256534576416, 'kl_term_chosen': 0.5430908203125, 'epoch': 0.04530313124583611}
                                                    {'loss': 0.7086, 'grad_norm': 2013.9498291015625, 'learning_rate': 4.4946808510638295e-07, 'mean_ratio_chosen': 2.9629387855529785, 'weight_chosen': 0.329256534576416, 'kl_term_chosen': 0.5430908203125, 'epoch': 0.05}
  5%|▍         | 170/3753 [07:26<2:29:22,  2.50s/it]  5%|▍         | 171/3753 [07:28<2:35:26,  2.60s/it]  5%|▍         | 172/3753 [07:31<2:41:25,  2.70s/it]  5%|▍         | 173/3753 [07:34<2:40:33,  2.69s/it]  5%|▍         | 174/3753 [07:37<2:36:30,  2.62s/it]  5%|▍         | 175/3753 [07:39<2:36:04,  2.62s/it]  5%|▍         | 176/3753 [07:41<2:30:08,  2.52s/it]  5%|▍         | 177/3753 [07:44<2:31:41,  2.55s/it]  5%|▍         | 178/3753 [07:47<2:35:17,  2.61s/it]  5%|▍         | 179/3753 [07:49<2:30:12,  2.52s/it]  5%|▍         | 180/3753 [07:52<2:30:09,  2.52s/it]{'loss': 2.2114, 'grad_norm': 1052.034423828125, 'learning_rate': 4.76063829787234e-07, 'mean_ratio_chosen': 8.352226257324219, 'weight_chosen': -0.12591755390167236, 'kl_term_chosen': 1.0612640380859375, 'epoch': 0.047968021319120584}
                                                    {'loss': 2.2114, 'grad_norm': 1052.034423828125, 'learning_rate': 4.76063829787234e-07, 'mean_ratio_chosen': 8.352226257324219, 'weight_chosen': -0.12591755390167236, 'kl_term_chosen': 1.0612640380859375, 'epoch': 0.05}
  5%|▍         | 180/3753 [07:52<2:30:09,  2.52s/it]  5%|▍         | 181/3753 [07:54<2:32:36,  2.56s/it]  5%|▍         | 182/3753 [07:58<2:58:18,  3.00s/it]  5%|▍         | 183/3753 [08:01<2:52:15,  2.89s/it]  5%|▍         | 184/3753 [08:04<2:51:41,  2.89s/it]  5%|▍         | 185/3753 [08:07<2:49:24,  2.85s/it]  5%|▍         | 186/3753 [08:09<2:44:38,  2.77s/it]  5%|▍         | 187/3753 [08:12<2:40:11,  2.70s/it]  5%|▌         | 188/3753 [08:14<2:37:11,  2.65s/it]  5%|▌         | 189/3753 [08:17<2:44:54,  2.78s/it]  5%|▌         | 190/3753 [08:20<2:39:48,  2.69s/it]{'loss': 1.4341, 'grad_norm': 1525.4385986328125, 'learning_rate': 5.026595744680851e-07, 'mean_ratio_chosen': 8.960356712341309, 'weight_chosen': -0.21560794115066528, 'kl_term_chosen': 1.096405029296875, 'epoch': 0.05063291139240506}
                                                    {'loss': 1.4341, 'grad_norm': 1525.4385986328125, 'learning_rate': 5.026595744680851e-07, 'mean_ratio_chosen': 8.960356712341309, 'weight_chosen': -0.21560794115066528, 'kl_term_chosen': 1.096405029296875, 'epoch': 0.05}
  5%|▌         | 190/3753 [08:20<2:39:48,  2.69s/it]  5%|▌         | 191/3753 [08:23<2:40:02,  2.70s/it]  5%|▌         | 192/3753 [08:25<2:38:25,  2.67s/it]  5%|▌         | 193/3753 [08:28<2:36:12,  2.63s/it]  5%|▌         | 194/3753 [08:30<2:38:26,  2.67s/it]  5%|▌         | 195/3753 [08:33<2:34:19,  2.60s/it]  5%|▌         | 196/3753 [08:35<2:27:21,  2.49s/it]  5%|▌         | 197/3753 [08:39<2:49:14,  2.86s/it]  5%|▌         | 198/3753 [08:41<2:40:49,  2.71s/it]  5%|▌         | 199/3753 [08:44<2:44:15,  2.77s/it]  5%|▌         | 200/3753 [08:47<2:41:37,  2.73s/it]{'loss': 2.099, 'grad_norm': 827.1688232421875, 'learning_rate': 5.292553191489362e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -1.9610103368759155, 'kl_term_chosen': 2.90435791015625, 'epoch': 0.05329780146568954}
                                                    {'loss': 2.099, 'grad_norm': 827.1688232421875, 'learning_rate': 5.292553191489362e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -1.9610103368759155, 'kl_term_chosen': 2.90435791015625, 'epoch': 0.05}
  5%|▌         | 200/3753 [08:47<2:41:37,  2.73s/it]  5%|▌         | 201/3753 [08:50<2:47:52,  2.84s/it]  5%|▌         | 202/3753 [08:52<2:39:23,  2.69s/it]  5%|▌         | 203/3753 [08:55<2:39:41,  2.70s/it]  5%|▌         | 204/3753 [08:57<2:33:55,  2.60s/it]  5%|▌         | 205/3753 [09:00<2:29:40,  2.53s/it]  5%|▌         | 206/3753 [09:03<2:38:16,  2.68s/it]  6%|▌         | 207/3753 [09:05<2:34:44,  2.62s/it]  6%|▌         | 208/3753 [09:08<2:32:31,  2.58s/it]  6%|▌         | 209/3753 [09:10<2:31:23,  2.56s/it]  6%|▌         | 210/3753 [09:13<2:35:55,  2.64s/it]{'loss': 4.1222, 'grad_norm': 1379.110107421875, 'learning_rate': 5.558510638297872e-07, 'mean_ratio_chosen': 3.6699516773223877, 'weight_chosen': 0.16864752769470215, 'kl_term_chosen': 0.6500892639160156, 'epoch': 0.05596269153897402}
                                                    {'loss': 4.1222, 'grad_norm': 1379.110107421875, 'learning_rate': 5.558510638297872e-07, 'mean_ratio_chosen': 3.6699516773223877, 'weight_chosen': 0.16864752769470215, 'kl_term_chosen': 0.6500892639160156, 'epoch': 0.06}
  6%|▌         | 210/3753 [09:13<2:35:55,  2.64s/it]  6%|▌         | 211/3753 [09:16<2:43:51,  2.78s/it]  6%|▌         | 212/3753 [09:18<2:36:00,  2.64s/it]  6%|▌         | 213/3753 [09:21<2:35:47,  2.64s/it]  6%|▌         | 214/3753 [09:24<2:36:30,  2.65s/it]  6%|▌         | 215/3753 [09:26<2:33:42,  2.61s/it]  6%|▌         | 216/3753 [09:29<2:33:42,  2.61s/it]  6%|▌         | 217/3753 [09:31<2:28:32,  2.52s/it]  6%|▌         | 218/3753 [09:34<2:31:55,  2.58s/it]  6%|▌         | 219/3753 [09:36<2:28:55,  2.53s/it]  6%|▌         | 220/3753 [09:39<2:33:36,  2.61s/it]{'loss': 5.8576, 'grad_norm': 491.3707580566406, 'learning_rate': 5.824468085106384e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -1.063672661781311, 'kl_term_chosen': 1.7453994750976562, 'epoch': 0.05862758161225849}
                                                    {'loss': 5.8576, 'grad_norm': 491.3707580566406, 'learning_rate': 5.824468085106384e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -1.063672661781311, 'kl_term_chosen': 1.7453994750976562, 'epoch': 0.06}
  6%|▌         | 220/3753 [09:39<2:33:36,  2.61s/it]  6%|▌         | 221/3753 [09:41<2:30:37,  2.56s/it]  6%|▌         | 222/3753 [09:44<2:30:48,  2.56s/it]  6%|▌         | 223/3753 [09:46<2:25:23,  2.47s/it]  6%|▌         | 224/3753 [09:49<2:24:09,  2.45s/it]  6%|▌         | 225/3753 [09:51<2:25:16,  2.47s/it]  6%|▌         | 226/3753 [09:54<2:24:27,  2.46s/it]  6%|▌         | 227/3753 [09:57<2:33:52,  2.62s/it]  6%|▌         | 228/3753 [09:59<2:27:43,  2.51s/it]  6%|▌         | 229/3753 [10:02<2:31:55,  2.59s/it]  6%|▌         | 230/3753 [10:05<2:50:12,  2.90s/it]{'loss': 4.5597, 'grad_norm': 1189.9407958984375, 'learning_rate': 6.090425531914894e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.4109795093536377, 'kl_term_chosen': 1.3481903076171875, 'epoch': 0.06129247168554297}
                                                    {'loss': 4.5597, 'grad_norm': 1189.9407958984375, 'learning_rate': 6.090425531914894e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.4109795093536377, 'kl_term_chosen': 1.3481903076171875, 'epoch': 0.06}
  6%|▌         | 230/3753 [10:05<2:50:12,  2.90s/it]  6%|▌         | 231/3753 [10:08<2:43:44,  2.79s/it]  6%|▌         | 232/3753 [10:10<2:39:18,  2.71s/it]  6%|▌         | 233/3753 [10:13<2:40:21,  2.73s/it]  6%|▌         | 234/3753 [10:16<2:41:38,  2.76s/it]  6%|▋         | 235/3753 [10:19<2:40:21,  2.73s/it]  6%|▋         | 236/3753 [10:21<2:37:33,  2.69s/it]  6%|▋         | 237/3753 [10:24<2:31:35,  2.59s/it]  6%|▋         | 238/3753 [10:26<2:31:52,  2.59s/it]  6%|▋         | 239/3753 [10:29<2:42:23,  2.77s/it]  6%|▋         | 240/3753 [10:32<2:36:57,  2.68s/it]{'loss': 4.0394, 'grad_norm': 1448.699951171875, 'learning_rate': 6.356382978723404e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.8753541707992554, 'kl_term_chosen': 1.7064971923828125, 'epoch': 0.06395736175882745}
                                                    {'loss': 4.0394, 'grad_norm': 1448.699951171875, 'learning_rate': 6.356382978723404e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.8753541707992554, 'kl_term_chosen': 1.7064971923828125, 'epoch': 0.06}
  6%|▋         | 240/3753 [10:32<2:36:57,  2.68s/it]  6%|▋         | 241/3753 [10:34<2:33:15,  2.62s/it]  6%|▋         | 242/3753 [10:37<2:31:53,  2.60s/it]  6%|▋         | 243/3753 [10:40<2:33:13,  2.62s/it]  7%|▋         | 244/3753 [10:43<2:39:45,  2.73s/it]  7%|▋         | 245/3753 [10:45<2:36:57,  2.68s/it]  7%|▋         | 246/3753 [10:47<2:29:37,  2.56s/it]  7%|▋         | 247/3753 [10:51<2:55:45,  3.01s/it]  7%|▋         | 248/3753 [10:54<2:52:08,  2.95s/it]  7%|▋         | 249/3753 [10:57<2:47:57,  2.88s/it]  7%|▋         | 250/3753 [11:00<2:52:09,  2.95s/it]{'loss': 7.2328, 'grad_norm': 621.608642578125, 'learning_rate': 6.622340425531915e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.3722851276397705, 'kl_term_chosen': 1.2670745849609375, 'epoch': 0.06662225183211193}
                                                    {'loss': 7.2328, 'grad_norm': 621.608642578125, 'learning_rate': 6.622340425531915e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -0.3722851276397705, 'kl_term_chosen': 1.2670745849609375, 'epoch': 0.07}
  7%|▋         | 250/3753 [11:00<2:52:09,  2.95s/it]  7%|▋         | 251/3753 [11:03<2:56:18,  3.02s/it]  7%|▋         | 252/3753 [11:06<2:49:27,  2.90s/it]  7%|▋         | 253/3753 [11:08<2:42:17,  2.78s/it]  7%|▋         | 254/3753 [11:11<2:33:14,  2.63s/it]  7%|▋         | 255/3753 [11:13<2:29:37,  2.57s/it]  7%|▋         | 256/3753 [11:15<2:25:19,  2.49s/it]  7%|▋         | 257/3753 [11:18<2:25:28,  2.50s/it]  7%|▋         | 258/3753 [11:20<2:24:27,  2.48s/it]  7%|▋         | 259/3753 [11:23<2:33:30,  2.64s/it]  7%|▋         | 260/3753 [11:26<2:35:28,  2.67s/it]{'loss': 19.3854, 'grad_norm': 0.0, 'learning_rate': 6.888297872340425e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -1.9110074043273926, 'kl_term_chosen': 2.7982120513916016, 'epoch': 0.06928714190539641}
                                                    {'loss': 19.3854, 'grad_norm': 0.0, 'learning_rate': 6.888297872340425e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -1.9110074043273926, 'kl_term_chosen': 2.7982120513916016, 'epoch': 0.07}
  7%|▋         | 260/3753 [11:26<2:35:28,  2.67s/it]  7%|▋         | 261/3753 [11:29<2:35:56,  2.68s/it]  7%|▋         | 262/3753 [11:31<2:33:37,  2.64s/it]  7%|▋         | 263/3753 [11:34<2:30:27,  2.59s/it]  7%|▋         | 264/3753 [11:36<2:28:55,  2.56s/it]  7%|▋         | 265/3753 [11:39<2:28:54,  2.56s/it]  7%|▋         | 266/3753 [11:41<2:26:19,  2.52s/it]  7%|▋         | 267/3753 [11:44<2:25:38,  2.51s/it]  7%|▋         | 268/3753 [11:47<2:34:22,  2.66s/it]  7%|▋         | 269/3753 [11:49<2:31:46,  2.61s/it]  7%|▋         | 270/3753 [11:52<2:32:22,  2.62s/it]{'loss': 43.9632, 'grad_norm': 0.0, 'learning_rate': 7.154255319148937e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.380013942718506, 'kl_term_chosen': 7.3555908203125, 'epoch': 0.07195203197868089}
                                                    {'loss': 43.9632, 'grad_norm': 0.0, 'learning_rate': 7.154255319148937e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.380013942718506, 'kl_term_chosen': 7.3555908203125, 'epoch': 0.07}
  7%|▋         | 270/3753 [11:52<2:32:22,  2.62s/it]  7%|▋         | 271/3753 [11:55<2:33:14,  2.64s/it]  7%|▋         | 272/3753 [11:57<2:31:05,  2.60s/it]  7%|▋         | 273/3753 [12:00<2:27:26,  2.54s/it]  7%|▋         | 274/3753 [12:02<2:25:57,  2.52s/it]  7%|▋         | 275/3753 [12:05<2:25:55,  2.52s/it]  7%|▋         | 276/3753 [12:07<2:23:20,  2.47s/it]  7%|▋         | 277/3753 [12:10<2:31:50,  2.62s/it]  7%|▋         | 278/3753 [12:13<2:34:02,  2.66s/it]  7%|▋         | 279/3753 [12:15<2:35:38,  2.69s/it]  7%|▋         | 280/3753 [12:18<2:35:02,  2.68s/it]{'loss': 50.8605, 'grad_norm': 0.0, 'learning_rate': 7.420212765957447e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -3.763009786605835, 'kl_term_chosen': 4.69346809387207, 'epoch': 0.07461692205196535}
                                                    {'loss': 50.8605, 'grad_norm': 0.0, 'learning_rate': 7.420212765957447e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -3.763009786605835, 'kl_term_chosen': 4.69346809387207, 'epoch': 0.07}
  7%|▋         | 280/3753 [12:18<2:35:02,  2.68s/it]  7%|▋         | 281/3753 [12:20<2:31:25,  2.62s/it]  8%|▊         | 282/3753 [12:23<2:35:18,  2.68s/it]  8%|▊         | 283/3753 [12:26<2:30:45,  2.61s/it]  8%|▊         | 284/3753 [12:28<2:26:15,  2.53s/it]  8%|▊         | 285/3753 [12:31<2:39:38,  2.76s/it]  8%|▊         | 286/3753 [12:34<2:30:37,  2.61s/it]  8%|▊         | 287/3753 [12:36<2:31:20,  2.62s/it]  8%|▊         | 288/3753 [12:39<2:39:37,  2.76s/it]  8%|▊         | 289/3753 [12:42<2:38:27,  2.74s/it]  8%|▊         | 290/3753 [12:45<2:44:40,  2.85s/it]{'loss': 55.354, 'grad_norm': 0.0, 'learning_rate': 7.686170212765957e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.685528755187988, 'kl_term_chosen': 7.438995361328125, 'epoch': 0.07728181212524983}
                                                    {'loss': 55.354, 'grad_norm': 0.0, 'learning_rate': 7.686170212765957e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.685528755187988, 'kl_term_chosen': 7.438995361328125, 'epoch': 0.08}
  8%|▊         | 290/3753 [12:45<2:44:40,  2.85s/it]  8%|▊         | 291/3753 [12:48<2:36:33,  2.71s/it]  8%|▊         | 292/3753 [12:50<2:39:18,  2.76s/it]  8%|▊         | 293/3753 [12:53<2:34:00,  2.67s/it]  8%|▊         | 294/3753 [12:55<2:28:47,  2.58s/it]  8%|▊         | 295/3753 [12:58<2:31:50,  2.63s/it]  8%|▊         | 296/3753 [13:00<2:28:12,  2.57s/it]  8%|▊         | 297/3753 [13:04<2:38:17,  2.75s/it]  8%|▊         | 298/3753 [13:07<2:40:34,  2.79s/it]  8%|▊         | 299/3753 [13:09<2:34:53,  2.69s/it]  8%|▊         | 300/3753 [13:11<2:28:48,  2.59s/it]{'loss': 56.0915, 'grad_norm': 0.0, 'learning_rate': 7.952127659574468e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.459471702575684, 'kl_term_chosen': 7.383613586425781, 'epoch': 0.07994670219853431}
                                                    {'loss': 56.0915, 'grad_norm': 0.0, 'learning_rate': 7.952127659574468e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.459471702575684, 'kl_term_chosen': 7.383613586425781, 'epoch': 0.08}
  8%|▊         | 300/3753 [13:11<2:28:48,  2.59s/it]/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
  8%|▊         | 301/3753 [14:07<17:51:52, 18.63s/it]  8%|▊         | 302/3753 [14:10<13:16:49, 13.85s/it]  8%|▊         | 303/3753 [14:13<10:03:42, 10.50s/it]  8%|▊         | 304/3753 [14:15<7:39:52,  8.00s/it]   8%|▊         | 305/3753 [14:17<6:03:06,  6.32s/it]  8%|▊         | 306/3753 [14:20<4:59:56,  5.22s/it]  8%|▊         | 307/3753 [14:23<4:13:38,  4.42s/it]  8%|▊         | 308/3753 [14:25<3:46:00,  3.94s/it]  8%|▊         | 309/3753 [14:28<3:24:09,  3.56s/it]  8%|▊         | 310/3753 [14:31<3:06:29,  3.25s/it]{'loss': 57.1837, 'grad_norm': 0.0, 'learning_rate': 8.218085106382978e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -5.1520466804504395, 'kl_term_chosen': 5.815715789794922, 'epoch': 0.08261159227181879}
                                                    {'loss': 57.1837, 'grad_norm': 0.0, 'learning_rate': 8.218085106382978e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -5.1520466804504395, 'kl_term_chosen': 5.815715789794922, 'epoch': 0.08}
  8%|▊         | 310/3753 [14:31<3:06:29,  3.25s/it]  8%|▊         | 311/3753 [14:33<2:57:14,  3.09s/it]  8%|▊         | 312/3753 [14:36<2:43:39,  2.85s/it]  8%|▊         | 313/3753 [14:38<2:37:06,  2.74s/it]  8%|▊         | 314/3753 [14:41<2:43:58,  2.86s/it]  8%|▊         | 315/3753 [14:44<2:35:48,  2.72s/it]  8%|▊         | 316/3753 [14:46<2:34:47,  2.70s/it]  8%|▊         | 317/3753 [14:49<2:31:38,  2.65s/it]  8%|▊         | 318/3753 [14:51<2:30:12,  2.62s/it]  8%|▊         | 319/3753 [14:54<2:29:54,  2.62s/it]  9%|▊         | 320/3753 [14:57<2:31:19,  2.64s/it]{'loss': 56.3074, 'grad_norm': 0.0, 'learning_rate': 8.48404255319149e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -7.086119651794434, 'kl_term_chosen': 8.023330688476562, 'epoch': 0.08527648234510327}
                                                    {'loss': 56.3074, 'grad_norm': 0.0, 'learning_rate': 8.48404255319149e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -7.086119651794434, 'kl_term_chosen': 8.023330688476562, 'epoch': 0.09}
  9%|▊         | 320/3753 [14:57<2:31:19,  2.64s/it]  9%|▊         | 321/3753 [15:00<2:35:11,  2.71s/it]  9%|▊         | 322/3753 [15:02<2:29:15,  2.61s/it]  9%|▊         | 323/3753 [15:04<2:23:33,  2.51s/it]  9%|▊         | 324/3753 [15:07<2:32:08,  2.66s/it]  9%|▊         | 325/3753 [15:10<2:36:12,  2.73s/it]  9%|▊         | 326/3753 [15:12<2:29:55,  2.62s/it]  9%|▊         | 327/3753 [15:15<2:27:03,  2.58s/it]  9%|▊         | 328/3753 [15:17<2:25:42,  2.55s/it]  9%|▉         | 329/3753 [15:20<2:27:23,  2.58s/it]  9%|▉         | 330/3753 [15:23<2:31:09,  2.65s/it]{'loss': 63.5682, 'grad_norm': 0.0, 'learning_rate': 8.75e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.95485258102417, 'kl_term_chosen': 7.8163909912109375, 'epoch': 0.08794137241838774}
                                                    {'loss': 63.5682, 'grad_norm': 0.0, 'learning_rate': 8.75e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.95485258102417, 'kl_term_chosen': 7.8163909912109375, 'epoch': 0.09}
  9%|▉         | 330/3753 [15:23<2:31:09,  2.65s/it]  9%|▉         | 331/3753 [15:26<2:36:54,  2.75s/it]  9%|▉         | 332/3753 [15:28<2:32:52,  2.68s/it]  9%|▉         | 333/3753 [15:31<2:33:16,  2.69s/it]  9%|▉         | 334/3753 [15:34<2:31:08,  2.65s/it]  9%|▉         | 335/3753 [15:37<2:35:14,  2.73s/it]  9%|▉         | 336/3753 [15:39<2:28:08,  2.60s/it]  9%|▉         | 337/3753 [15:42<2:28:42,  2.61s/it]  9%|▉         | 338/3753 [15:44<2:23:24,  2.52s/it]  9%|▉         | 339/3753 [15:46<2:20:44,  2.47s/it]  9%|▉         | 340/3753 [15:49<2:29:01,  2.62s/it]{'loss': 66.5642, 'grad_norm': 0.0, 'learning_rate': 9.01595744680851e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.797094821929932, 'kl_term_chosen': 7.040737152099609, 'epoch': 0.09060626249167222}
                                                    {'loss': 66.5642, 'grad_norm': 0.0, 'learning_rate': 9.01595744680851e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.797094821929932, 'kl_term_chosen': 7.040737152099609, 'epoch': 0.09}
  9%|▉         | 340/3753 [15:49<2:29:01,  2.62s/it]  9%|▉         | 341/3753 [15:52<2:26:10,  2.57s/it]  9%|▉         | 342/3753 [15:54<2:23:25,  2.52s/it]  9%|▉         | 343/3753 [15:57<2:26:31,  2.58s/it]  9%|▉         | 344/3753 [15:59<2:28:55,  2.62s/it]  9%|▉         | 345/3753 [16:02<2:31:30,  2.67s/it]  9%|▉         | 346/3753 [16:05<2:28:29,  2.62s/it]  9%|▉         | 347/3753 [16:07<2:29:09,  2.63s/it]  9%|▉         | 348/3753 [16:10<2:33:31,  2.71s/it]  9%|▉         | 349/3753 [16:13<2:28:08,  2.61s/it]  9%|▉         | 350/3753 [16:16<2:33:05,  2.70s/it]{'loss': 68.9832, 'grad_norm': 0.0, 'learning_rate': 9.281914893617021e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.3843278884887695, 'kl_term_chosen': 7.2187347412109375, 'epoch': 0.09327115256495669}
                                                    {'loss': 68.9832, 'grad_norm': 0.0, 'learning_rate': 9.281914893617021e-07, 'mean_ratio_chosen': 9.974181175231934, 'weight_chosen': -6.3843278884887695, 'kl_term_chosen': 7.2187347412109375, 'epoch': 0.09}
  9%|▉         | 350/3753 [16:16<2:33:05,  2.70s/it]  9%|▉         | 351/3753 [16:18<2:30:22,  2.65s/it]