nohup: ignoring input
[W1026 16:21:44.139723240 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
`torch_dtype` is deprecated! Use `dtype` instead!
加载用于训练的策略模型 (Policy Model)...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 56.28it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 57.84it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
`torch_dtype` is deprecated! Use `dtype` instead!
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 56.24it/s]
加载用于训练的策略模型 (Policy Model)...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 55.67it/s]
[W1026 16:21:54.725778306 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1026 16:21:54.728334373 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 57.78it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 56.14it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 57.85it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 64.89it/s]
[W1026 16:21:54.008036148 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1026 16:21:54.042596372 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
WARNING:accelerate.utils.other:Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in LlamaForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in LlamaDecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1968: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.
  warnings.warn(
wandb: Tracking run with wandb version 0.21.3
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /wandb/offline-run-20251026_162202-by3kleeu
  0%|          | 0/3753 [00:00<?, ?it/s]--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -199.0
Sample 0 - pi_logp_chosen:  -198.89137268066406
Sample 0 - Difference (pi - ref): 0.1086273193359375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -100.5
Sample 0 - pi_logp_chosen:  -99.98401641845703
Sample 0 - Difference (pi - ref): 0.5159835815429688
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -122.0
Sample 0 - pi_logp_chosen:  -121.78414916992188
Sample 0 - Difference (pi - ref): 0.215850830078125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -234.0
Sample 0 - pi_logp_chosen:  -233.8189239501953
Sample 0 - Difference (pi - ref): 0.1810760498046875
---------------------------------
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -227.0
Sample 0 - pi_logp_chosen:  -227.44651794433594
Sample 0 - Difference (pi - ref): -0.4465179443359375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -248.0
Sample 0 - pi_logp_chosen:  -248.48553466796875
Sample 0 - Difference (pi - ref): -0.48553466796875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -209.0
Sample 0 - pi_logp_chosen:  -208.68115234375
Sample 0 - Difference (pi - ref): 0.31884765625
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -728.0
Sample 0 - pi_logp_chosen:  -728.8134155273438
Sample 0 - Difference (pi - ref): -0.81341552734375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -240.0
Sample 0 - pi_logp_chosen:  -240.24868774414062
Sample 0 - Difference (pi - ref): -0.248687744140625
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -252.0
Sample 0 - pi_logp_chosen:  -252.66221618652344
Sample 0 - Difference (pi - ref): -0.6622161865234375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -350.0
Sample 0 - pi_logp_chosen:  -349.7069091796875
Sample 0 - Difference (pi - ref): 0.2930908203125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -454.0
Sample 0 - pi_logp_chosen:  -452.75640869140625
Sample 0 - Difference (pi - ref): 1.24359130859375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -98.5
Sample 0 - pi_logp_chosen:  -98.45634460449219
Sample 0 - Difference (pi - ref): 0.0436553955078125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -127.5
Sample 0 - pi_logp_chosen:  -127.38422393798828
Sample 0 - Difference (pi - ref): 0.11577606201171875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -252.0
Sample 0 - pi_logp_chosen:  -250.1884765625
Sample 0 - Difference (pi - ref): 1.8115234375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -516.0
Sample 0 - pi_logp_chosen:  -514.823974609375
Sample 0 - Difference (pi - ref): 1.176025390625
---------------------------------
  0%|          | 1/3753 [00:04<4:50:45,  4.65s/it]{'loss': 0.4374, 'grad_norm': 9107.3271484375, 'learning_rate': 0.0, 'mean_ratio_chosen': 6.1197638511657715, 'mean_ratio_rejected': 1.318693995475769, 'weight_chosen': 0.016160130500793457, 'weight_rejected': 0.1830017864704132, 'kl_term_chosen': 0.90576171875, 'epoch': 0.0002664890073284477}
                                                  {'loss': 0.4374, 'grad_norm': 9107.3271484375, 'learning_rate': 0.0, 'mean_ratio_chosen': 6.1197638511657715, 'mean_ratio_rejected': 1.318693995475769, 'weight_chosen': 0.016160130500793457, 'weight_rejected': 0.1830017864704132, 'kl_term_chosen': 0.90576171875, 'epoch': 0.0}
  0%|          | 1/3753 [00:04<4:50:45,  4.65s/it]  0%|          | 2/3753 [00:09<5:04:24,  4.87s/it]  0%|          | 3/3753 [00:14<4:52:15,  4.68s/it]  0%|          | 4/3753 [00:17<4:31:38,  4.35s/it]  0%|          | 5/3753 [00:21<4:21:54,  4.19s/it]  0%|          | 6/3753 [00:26<4:32:53,  4.37s/it]  0%|          | 7/3753 [00:30<4:25:42,  4.26s/it]  0%|          | 8/3753 [00:35<4:33:48,  4.39s/it]  0%|          | 9/3753 [00:39<4:36:46,  4.44s/it]  0%|          | 10/3753 [00:43<4:18:47,  4.15s/it]{'loss': 0.2395, 'grad_norm': 1707.7017822265625, 'learning_rate': 1.1968085106382979e-06, 'mean_ratio_chosen': 1.0208442211151123, 'mean_ratio_rejected': 1.0146942138671875, 'weight_chosen': 0.2347700595855713, 'weight_rejected': 0.4236156642436981, 'kl_term_chosen': 0.01031494140625, 'epoch': 0.002664890073284477}
                                                   {'loss': 0.2395, 'grad_norm': 1707.7017822265625, 'learning_rate': 1.1968085106382979e-06, 'mean_ratio_chosen': 1.0208442211151123, 'mean_ratio_rejected': 1.0146942138671875, 'weight_chosen': 0.2347700595855713, 'weight_rejected': 0.4236156642436981, 'kl_term_chosen': 0.01031494140625, 'epoch': 0.0}
  0%|          | 10/3753 [00:43<4:18:47,  4.15s/it]  0%|          | 11/3753 [00:47<4:19:08,  4.16s/it]  0%|          | 12/3753 [00:52<4:27:35,  4.29s/it]  0%|          | 13/3753 [00:56<4:23:39,  4.23s/it]  0%|          | 14/3753 [01:00<4:18:01,  4.14s/it]  0%|          | 15/3753 [01:03<4:09:32,  4.01s/it]  0%|          | 16/3753 [01:07<4:00:02,  3.85s/it]  0%|          | 17/3753 [01:11<4:13:56,  4.08s/it]  0%|          | 18/3753 [01:16<4:26:12,  4.28s/it]  1%|          | 19/3753 [01:20<4:09:47,  4.01s/it]  1%|          | 20/3753 [01:23<4:05:57,  3.95s/it]{'loss': 9.2109, 'grad_norm': 4475.45947265625, 'learning_rate': 2.526595744680851e-06, 'mean_ratio_chosen': 0.28418412804603577, 'mean_ratio_rejected': 0.34714987874031067, 'weight_chosen': 1.3616583347320557, 'weight_rejected': -0.2323450744152069, 'kl_term_chosen': -0.6290664672851562, 'epoch': 0.005329780146568954}
                                                   {'loss': 9.2109, 'grad_norm': 4475.45947265625, 'learning_rate': 2.526595744680851e-06, 'mean_ratio_chosen': 0.28418412804603577, 'mean_ratio_rejected': 0.34714987874031067, 'weight_chosen': 1.3616583347320557, 'weight_rejected': -0.2323450744152069, 'kl_term_chosen': -0.6290664672851562, 'epoch': 0.01}
  1%|          | 20/3753 [01:23<4:05:57,  3.95s/it]/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
  1%|          | 21/3753 [02:14<18:39:40, 18.00s/it]  1%|          | 22/3753 [02:21<15:06:34, 14.58s/it]  1%|          | 23/3753 [02:26<12:03:43, 11.64s/it]  1%|          | 24/3753 [02:29<9:35:21,  9.26s/it]   1%|          | 25/3753 [02:33<7:59:31,  7.72s/it]  1%|          | 26/3753 [02:37<6:37:12,  6.39s/it]  1%|          | 27/3753 [02:40<5:48:46,  5.62s/it]  1%|          | 28/3753 [02:44<5:10:08,  5.00s/it]  1%|          | 29/3753 [02:48<4:57:25,  4.79s/it]  1%|          | 30/3753 [02:52<4:45:38,  4.60s/it]{'loss': 2.4118, 'grad_norm': 0.0, 'learning_rate': 3.856382978723404e-06, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 15.266814231872559, 'weight_rejected': -14.751468658447266, 'kl_term_chosen': -14.638862609863281, 'epoch': 0.007994670219853431}
                                                   {'loss': 2.4118, 'grad_norm': 0.0, 'learning_rate': 3.856382978723404e-06, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 15.266814231872559, 'weight_rejected': -14.751468658447266, 'kl_term_chosen': -14.638862609863281, 'epoch': 0.01}
  1%|          | 30/3753 [02:53<4:45:38,  4.60s/it]  1%|          | 31/3753 [02:58<4:56:03,  4.77s/it]  1%|          | 32/3753 [03:01<4:32:21,  4.39s/it]  1%|          | 33/3753 [03:05<4:20:28,  4.20s/it]  1%|          | 34/3753 [03:10<4:42:51,  4.56s/it]  1%|          | 35/3753 [03:15<4:53:52,  4.74s/it]  1%|          | 36/3753 [03:21<5:14:55,  5.08s/it]  1%|          | 37/3753 [03:26<5:13:04,  5.06s/it]  1%|          | 38/3753 [03:31<5:07:56,  4.97s/it]  1%|          | 39/3753 [03:38<5:42:08,  5.53s/it]  1%|          | 40/3753 [03:42<5:11:08,  5.03s/it]{'loss': -4.5778, 'grad_norm': 0.0, 'learning_rate': 5.186170212765958e-06, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 17.156780242919922, 'weight_rejected': -23.17521095275879, 'kl_term_chosen': -16.39898681640625, 'epoch': 0.010659560293137908}
                                                   {'loss': -4.5778, 'grad_norm': 0.0, 'learning_rate': 5.186170212765958e-06, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 17.156780242919922, 'weight_rejected': -23.17521095275879, 'kl_term_chosen': -16.39898681640625, 'epoch': 0.01}
  1%|          | 40/3753 [03:42<5:11:08,  5.03s/it]/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
  1%|          | 41/3753 [04:35<20:06:35, 19.50s/it]  1%|          | 42/3753 [04:38<15:07:38, 14.67s/it]  1%|          | 43/3753 [04:43<11:58:53, 11.63s/it]  1%|          | 44/3753 [04:47<9:30:54,  9.24s/it]   1%|          | 45/3753 [04:51<8:01:55,  7.80s/it]W1026 16:26:55.357000 306930 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received 15 death signal, shutting down workers
W1026 16:26:55.358000 306930 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 307417 closing signal SIGTERM
W1026 16:26:55.362000 306930 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 307418 closing signal SIGTERM
W1026 16:26:55.367000 306930 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 307419 closing signal SIGTERM
W1026 16:26:55.370000 306930 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 307420 closing signal SIGTERM
Traceback (most recent call last):
  File "/root/miniconda3/envs/advan/bin/accelerate", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1222, in launch_command
    multi_gpu_launcher(args)
  File "/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/commands/launch.py", line 853, in multi_gpu_launcher
    distrib_run.run(args)
  File "/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 306930 got signal: 15
