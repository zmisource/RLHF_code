nohup: ignoring input
[W1114 03:56:29.839732546 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
⚙️  Running in WANDB offline mode
设置随机种子为: 42
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
设置随机种子为: 42
设置随机种子为: 42
设置随机种子为: 42
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]加载用于训练的策略模型 (Policy Model)...
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 96.16it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 95.96it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 97.87it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 92.34it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 94.99it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 96.08it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 97.27it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 95.60it/s]
[W1114 03:56:34.203875727 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1114 03:56:34.214722470 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1114 03:56:34.215698053 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1114 03:56:34.219202419 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_kl_term2.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_kl_term2.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_kl_term2.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_kl_term2.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in LlamaForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in LlamaDecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1968: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.
  warnings.warn(
wandb: Tracking run with wandb version 0.21.3
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /root/wandb/offline-run-20251114_035642-q7fxroh1
  0%|          | 0/3753 [00:00<?, ?it/s]--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -199.0
Sample 0 - pi_logp_chosen:  -198.81076049804688
Sample 0 - Difference (pi - ref): 0.189239501953125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -100.5
Sample 0 - pi_logp_chosen:  -100.34402465820312
Sample 0 - Difference (pi - ref): 0.155975341796875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -122.0
Sample 0 - pi_logp_chosen:  -121.57272338867188
Sample 0 - Difference (pi - ref): 0.427276611328125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -234.0
Sample 0 - pi_logp_chosen:  -233.57894897460938
Sample 0 - Difference (pi - ref): 0.421051025390625
---------------------------------
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -227.0
Sample 0 - pi_logp_chosen:  -227.50088500976562
Sample 0 - Difference (pi - ref): -0.500885009765625
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -248.0
Sample 0 - pi_logp_chosen:  -248.47735595703125
Sample 0 - Difference (pi - ref): -0.47735595703125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -209.0
Sample 0 - pi_logp_chosen:  -208.26136779785156
Sample 0 - Difference (pi - ref): 0.7386322021484375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -728.0
Sample 0 - pi_logp_chosen:  -730.50390625
Sample 0 - Difference (pi - ref): -2.50390625
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -240.0
Sample 0 - pi_logp_chosen:  -240.2412567138672
Sample 0 - Difference (pi - ref): -0.2412567138671875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -253.0
Sample 0 - pi_logp_chosen:  -253.00314331054688
Sample 0 - Difference (pi - ref): -0.003143310546875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -350.0
Sample 0 - pi_logp_chosen:  -349.3069152832031
Sample 0 - Difference (pi - ref): 0.693084716796875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -452.0
Sample 0 - pi_logp_chosen:  -453.64239501953125
Sample 0 - Difference (pi - ref): -1.64239501953125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -127.5
Sample 0 - pi_logp_chosen:  -127.59747314453125
Sample 0 - Difference (pi - ref): -0.09747314453125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -98.5
Sample 0 - pi_logp_chosen:  -98.78225708007812
Sample 0 - Difference (pi - ref): -0.282257080078125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -251.0
Sample 0 - pi_logp_chosen:  -249.6898956298828
Sample 0 - Difference (pi - ref): 1.3101043701171875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -516.0
Sample 0 - pi_logp_chosen:  -514.2531127929688
Sample 0 - Difference (pi - ref): 1.74688720703125
---------------------------------
  0%|          | 1/3753 [00:04<4:58:36,  4.78s/it]{'loss': 0.3808, 'grad_norm': 5779.43408203125, 'learning_rate': 0.0, 'mean_ratio_chosen': 3.7065606117248535, 'mean_ratio_rejected': 6.590037822723389, 'weight_chosen': -0.38818252086639404, 'weight_rejected': 0.044680867344141006, 'kl_term_chosen': 1.3101043701171875, 'epoch': 0.0002664890073284477}
                                                  {'loss': 0.3808, 'grad_norm': 5779.43408203125, 'learning_rate': 0.0, 'mean_ratio_chosen': 3.7065606117248535, 'mean_ratio_rejected': 6.590037822723389, 'weight_chosen': -0.38818252086639404, 'weight_rejected': 0.044680867344141006, 'kl_term_chosen': 1.3101043701171875, 'epoch': 0.0}
  0%|          | 1/3753 [00:04<4:58:36,  4.78s/it]  0%|          | 2/3753 [00:10<5:17:30,  5.08s/it]  0%|          | 3/3753 [00:14<5:06:20,  4.90s/it]  0%|          | 4/3753 [00:18<4:42:06,  4.51s/it]  0%|          | 5/3753 [00:22<4:33:02,  4.37s/it]  0%|          | 6/3753 [00:28<4:53:11,  4.69s/it]  0%|          | 7/3753 [00:32<4:42:18,  4.52s/it]  0%|          | 8/3753 [00:36<4:40:55,  4.50s/it]  0%|          | 9/3753 [00:40<4:23:25,  4.22s/it]  0%|          | 10/3753 [00:44<4:13:35,  4.07s/it]{'loss': 0.3667, 'grad_norm': 2507.893798828125, 'learning_rate': 2.3936170212765956e-08, 'mean_ratio_chosen': 0.639676570892334, 'mean_ratio_rejected': 0.7602492570877075, 'weight_chosen': 0.6918776035308838, 'weight_rejected': 0.4163219630718231, 'kl_term_chosen': -0.4467926025390625, 'epoch': 0.002664890073284477}
                                                   {'loss': 0.3667, 'grad_norm': 2507.893798828125, 'learning_rate': 2.3936170212765956e-08, 'mean_ratio_chosen': 0.639676570892334, 'mean_ratio_rejected': 0.7602492570877075, 'weight_chosen': 0.6918776035308838, 'weight_rejected': 0.4163219630718231, 'kl_term_chosen': -0.4467926025390625, 'epoch': 0.0}
  0%|          | 10/3753 [00:44<4:13:35,  4.07s/it]  0%|          | 11/3753 [00:48<4:20:23,  4.18s/it]  0%|          | 12/3753 [00:53<4:34:19,  4.40s/it]  0%|          | 13/3753 [00:57<4:32:28,  4.37s/it]  0%|          | 14/3753 [01:01<4:26:13,  4.27s/it]  0%|          | 15/3753 [01:05<4:16:18,  4.11s/it]  0%|          | 16/3753 [01:09<4:10:35,  4.02s/it]  0%|          | 17/3753 [01:13<4:04:54,  3.93s/it]  0%|          | 18/3753 [01:17<4:22:03,  4.21s/it]  1%|          | 19/3753 [01:21<4:05:58,  3.95s/it]  1%|          | 20/3753 [01:25<4:04:23,  3.93s/it]{'loss': 0.3705, 'grad_norm': 2533.5458984375, 'learning_rate': 5.053191489361702e-08, 'mean_ratio_chosen': 0.9904859066009521, 'mean_ratio_rejected': 1.1828093528747559, 'weight_chosen': 0.7421514391899109, 'weight_rejected': 0.29665425419807434, 'kl_term_chosen': -0.00955963134765625, 'epoch': 0.005329780146568954}
                                                   {'loss': 0.3705, 'grad_norm': 2533.5458984375, 'learning_rate': 5.053191489361702e-08, 'mean_ratio_chosen': 0.9904859066009521, 'mean_ratio_rejected': 1.1828093528747559, 'weight_chosen': 0.7421514391899109, 'weight_rejected': 0.29665425419807434, 'kl_term_chosen': -0.00955963134765625, 'epoch': 0.01}
  1%|          | 20/3753 [01:25<4:04:23,  3.93s/it]  1%|          | 21/3753 [01:29<4:07:19,  3.98s/it]  1%|          | 22/3753 [01:36<5:12:49,  5.03s/it]  1%|          | 23/3753 [01:41<5:12:34,  5.03s/it]  1%|          | 24/3753 [01:44<4:39:58,  4.50s/it]  1%|          | 25/3753 [01:49<4:34:25,  4.42s/it]  1%|          | 26/3753 [01:52<4:08:19,  4.00s/it]  1%|          | 27/3753 [01:56<4:08:22,  4.00s/it]  1%|          | 28/3753 [02:00<4:04:56,  3.95s/it]  1%|          | 29/3753 [02:03<4:02:37,  3.91s/it]  1%|          | 30/3753 [02:08<4:07:16,  3.99s/it]{'loss': 0.2464, 'grad_norm': 7114.0341796875, 'learning_rate': 7.712765957446808e-08, 'mean_ratio_chosen': 1.2378300428390503, 'mean_ratio_rejected': 0.8270012736320496, 'weight_chosen': 0.41459137201309204, 'weight_rejected': 0.5987815260887146, 'kl_term_chosen': 0.21335983276367188, 'epoch': 0.007994670219853431}
                                                   {'loss': 0.2464, 'grad_norm': 7114.0341796875, 'learning_rate': 7.712765957446808e-08, 'mean_ratio_chosen': 1.2378300428390503, 'mean_ratio_rejected': 0.8270012736320496, 'weight_chosen': 0.41459137201309204, 'weight_rejected': 0.5987815260887146, 'kl_term_chosen': 0.21335983276367188, 'epoch': 0.01}
  1%|          | 30/3753 [02:08<4:07:16,  3.99s/it]  1%|          | 31/3753 [02:13<4:30:39,  4.36s/it]  1%|          | 32/3753 [02:16<4:16:20,  4.13s/it]  1%|          | 33/3753 [02:20<4:06:17,  3.97s/it]  1%|          | 34/3753 [02:27<4:55:30,  4.77s/it]  1%|          | 35/3753 [02:32<5:10:07,  5.00s/it]  1%|          | 36/3753 [02:38<5:33:03,  5.38s/it]  1%|          | 37/3753 [02:44<5:39:18,  5.48s/it]  1%|          | 38/3753 [02:48<5:17:12,  5.12s/it]  1%|          | 39/3753 [02:56<6:12:07,  6.01s/it]  1%|          | 40/3753 [03:01<5:36:08,  5.43s/it]{'loss': 0.5999, 'grad_norm': 2117.30517578125, 'learning_rate': 1.0372340425531915e-07, 'mean_ratio_chosen': 0.7036308646202087, 'mean_ratio_rejected': 1.1427679061889648, 'weight_chosen': 1.1092958450317383, 'weight_rejected': 0.1276526302099228, 'kl_term_chosen': -0.35150146484375, 'epoch': 0.010659560293137908}
                                                   {'loss': 0.5999, 'grad_norm': 2117.30517578125, 'learning_rate': 1.0372340425531915e-07, 'mean_ratio_chosen': 0.7036308646202087, 'mean_ratio_rejected': 1.1427679061889648, 'weight_chosen': 1.1092958450317383, 'weight_rejected': 0.1276526302099228, 'kl_term_chosen': -0.35150146484375, 'epoch': 0.01}
  1%|          | 40/3753 [03:01<5:36:08,  5.43s/it]  1%|          | 41/3753 [03:07<5:56:03,  5.76s/it]  1%|          | 42/3753 [03:10<5:05:14,  4.94s/it]  1%|          | 43/3753 [03:15<5:01:08,  4.87s/it]  1%|          | 44/3753 [03:19<4:44:28,  4.60s/it]  1%|          | 45/3753 [03:24<4:49:55,  4.69s/it]  1%|          | 46/3753 [03:29<4:55:04,  4.78s/it]  1%|▏         | 47/3753 [03:32<4:20:55,  4.22s/it]  1%|▏         | 48/3753 [03:36<4:17:28,  4.17s/it]  1%|▏         | 49/3753 [03:40<4:19:55,  4.21s/it]  1%|▏         | 50/3753 [03:44<4:12:33,  4.09s/it]{'loss': 0.0998, 'grad_norm': 3454.649658203125, 'learning_rate': 1.3031914893617022e-07, 'mean_ratio_chosen': 2.712315559387207, 'mean_ratio_rejected': 1.348092794418335, 'weight_chosen': -0.13441109657287598, 'weight_rejected': 0.6851073503494263, 'kl_term_chosen': 0.997802734375, 'epoch': 0.013324450366422385}
                                                   {'loss': 0.0998, 'grad_norm': 3454.649658203125, 'learning_rate': 1.3031914893617022e-07, 'mean_ratio_chosen': 2.712315559387207, 'mean_ratio_rejected': 1.348092794418335, 'weight_chosen': -0.13441109657287598, 'weight_rejected': 0.6851073503494263, 'kl_term_chosen': 0.997802734375, 'epoch': 0.01}
  1%|▏         | 50/3753 [03:44<4:12:33,  4.09s/it]  1%|▏         | 51/3753 [03:48<4:13:21,  4.11s/it]  1%|▏         | 52/3753 [03:53<4:24:09,  4.28s/it]  1%|▏         | 53/3753 [03:58<4:53:21,  4.76s/it]  1%|▏         | 54/3753 [04:02<4:29:42,  4.37s/it]  1%|▏         | 55/3753 [04:10<5:34:49,  5.43s/it]  1%|▏         | 56/3753 [04:18<6:17:07,  6.12s/it]  2%|▏         | 57/3753 [04:22<5:51:14,  5.70s/it]  2%|▏         | 58/3753 [04:26<5:09:52,  5.03s/it]  2%|▏         | 59/3753 [04:33<5:56:54,  5.80s/it]  2%|▏         | 60/3753 [04:37<5:07:56,  5.00s/it]{'loss': 0.0919, 'grad_norm': 6931.0087890625, 'learning_rate': 1.5691489361702126e-07, 'mean_ratio_chosen': 0.892964243888855, 'mean_ratio_rejected': 0.9904821515083313, 'weight_chosen': 0.14342337846755981, 'weight_rejected': 0.937210738658905, 'kl_term_chosen': -0.11320877075195312, 'epoch': 0.015989340439706862}
                                                   {'loss': 0.0919, 'grad_norm': 6931.0087890625, 'learning_rate': 1.5691489361702126e-07, 'mean_ratio_chosen': 0.892964243888855, 'mean_ratio_rejected': 0.9904821515083313, 'weight_chosen': 0.14342337846755981, 'weight_rejected': 0.937210738658905, 'kl_term_chosen': -0.11320877075195312, 'epoch': 0.02}
  2%|▏         | 60/3753 [04:37<5:07:56,  5.00s/it]  2%|▏         | 61/3753 [04:42<5:15:03,  5.12s/it]  2%|▏         | 62/3753 [04:46<5:00:27,  4.88s/it]  2%|▏         | 63/3753 [04:51<4:52:42,  4.76s/it]  2%|▏         | 64/3753 [04:56<5:01:19,  4.90s/it]  2%|▏         | 65/3753 [05:00<4:43:01,  4.60s/it]  2%|▏         | 66/3753 [05:05<4:51:04,  4.74s/it]  2%|▏         | 67/3753 [05:09<4:46:44,  4.67s/it]  2%|▏         | 68/3753 [05:13<4:29:22,  4.39s/it]  2%|▏         | 69/3753 [05:18<4:32:25,  4.44s/it]  2%|▏         | 70/3753 [05:21<4:18:48,  4.22s/it]{'loss': 0.2502, 'grad_norm': 2243.493896484375, 'learning_rate': 1.8351063829787234e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 1.320264458656311, 'weight_chosen': -1.6182513236999512, 'weight_rejected': 0.044680867344141006, 'kl_term_chosen': 2.58148193359375, 'epoch': 0.018654230512991338}
                                                   {'loss': 0.2502, 'grad_norm': 2243.493896484375, 'learning_rate': 1.8351063829787234e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 1.320264458656311, 'weight_chosen': -1.6182513236999512, 'weight_rejected': 0.044680867344141006, 'kl_term_chosen': 2.58148193359375, 'epoch': 0.02}
  2%|▏         | 70/3753 [05:21<4:18:48,  4.22s/it]  2%|▏         | 71/3753 [05:26<4:28:08,  4.37s/it]  2%|▏         | 72/3753 [05:31<4:33:30,  4.46s/it]  2%|▏         | 73/3753 [05:35<4:20:39,  4.25s/it]  2%|▏         | 74/3753 [05:38<4:11:18,  4.10s/it]  2%|▏         | 75/3753 [05:42<4:03:14,  3.97s/it]  2%|▏         | 76/3753 [05:48<4:35:06,  4.49s/it]  2%|▏         | 77/3753 [05:51<4:11:09,  4.10s/it]  2%|▏         | 78/3753 [05:56<4:30:17,  4.41s/it]  2%|▏         | 79/3753 [06:00<4:28:20,  4.38s/it]  2%|▏         | 80/3753 [06:05<4:38:34,  4.55s/it]{'loss': 0.7343, 'grad_norm': 2021.669677734375, 'learning_rate': 2.101063829787234e-07, 'mean_ratio_chosen': 0.5732536315917969, 'mean_ratio_rejected': 0.8193581104278564, 'weight_chosen': 1.4848358631134033, 'weight_rejected': 0.10230471193790436, 'kl_term_chosen': -0.556427001953125, 'epoch': 0.021319120586275817}
                                                   {'loss': 0.7343, 'grad_norm': 2021.669677734375, 'learning_rate': 2.101063829787234e-07, 'mean_ratio_chosen': 0.5732536315917969, 'mean_ratio_rejected': 0.8193581104278564, 'weight_chosen': 1.4848358631134033, 'weight_rejected': 0.10230471193790436, 'kl_term_chosen': -0.556427001953125, 'epoch': 0.02}
  2%|▏         | 80/3753 [06:05<4:38:34,  4.55s/it]  2%|▏         | 81/3753 [06:10<4:42:14,  4.61s/it]  2%|▏         | 82/3753 [06:14<4:27:31,  4.37s/it]  2%|▏         | 83/3753 [06:20<5:09:32,  5.06s/it]  2%|▏         | 84/3753 [06:25<5:05:39,  5.00s/it]  2%|▏         | 85/3753 [06:29<4:38:25,  4.55s/it]  2%|▏         | 86/3753 [06:32<4:16:41,  4.20s/it]  2%|▏         | 87/3753 [06:39<5:06:50,  5.02s/it]  2%|▏         | 88/3753 [06:43<4:44:16,  4.65s/it]  2%|▏         | 89/3753 [06:47<4:37:16,  4.54s/it]  2%|▏         | 90/3753 [06:51<4:29:59,  4.42s/it]{'loss': -0.0076, 'grad_norm': 2252.399658203125, 'learning_rate': 2.3670212765957445e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -1.540844440460205, 'weight_rejected': 0.15002880990505219, 'kl_term_chosen': 2.330078125, 'epoch': 0.023984010659560292}
                                                   {'loss': -0.0076, 'grad_norm': 2252.399658203125, 'learning_rate': 2.3670212765957445e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -1.540844440460205, 'weight_rejected': 0.15002880990505219, 'kl_term_chosen': 2.330078125, 'epoch': 0.02}
  2%|▏         | 90/3753 [06:51<4:29:59,  4.42s/it]  2%|▏         | 91/3753 [06:55<4:09:38,  4.09s/it]  2%|▏         | 92/3753 [06:59<4:06:56,  4.05s/it]  2%|▏         | 93/3753 [07:04<4:33:29,  4.48s/it]  3%|▎         | 94/3753 [07:08<4:22:18,  4.30s/it]  3%|▎         | 95/3753 [07:15<5:04:10,  4.99s/it]  3%|▎         | 96/3753 [07:19<4:46:43,  4.70s/it]  3%|▎         | 97/3753 [07:23<4:38:23,  4.57s/it]  3%|▎         | 98/3753 [07:29<5:01:01,  4.94s/it]  3%|▎         | 99/3753 [07:32<4:33:38,  4.49s/it]  3%|▎         | 100/3753 [07:36<4:18:06,  4.24s/it]{'loss': 1.2974, 'grad_norm': 3565.03271484375, 'learning_rate': 2.632978723404255e-07, 'mean_ratio_chosen': 0.8193331360816956, 'mean_ratio_rejected': 0.5339888334274292, 'weight_chosen': 0.8272157311439514, 'weight_rejected': 0.14414885640144348, 'kl_term_chosen': -0.1992645263671875, 'epoch': 0.02664890073284477}
                                                    {'loss': 1.2974, 'grad_norm': 3565.03271484375, 'learning_rate': 2.632978723404255e-07, 'mean_ratio_chosen': 0.8193331360816956, 'mean_ratio_rejected': 0.5339888334274292, 'weight_chosen': 0.8272157311439514, 'weight_rejected': 0.14414885640144348, 'kl_term_chosen': -0.1992645263671875, 'epoch': 0.03}
  3%|▎         | 100/3753 [07:36<4:18:06,  4.24s/it]  3%|▎         | 101/3753 [07:40<4:24:03,  4.34s/it]  3%|▎         | 102/3753 [07:45<4:20:01,  4.27s/it]  3%|▎         | 103/3753 [07:48<4:09:50,  4.11s/it]  3%|▎         | 104/3753 [07:54<4:38:13,  4.57s/it]  3%|▎         | 105/3753 [07:58<4:29:55,  4.44s/it]  3%|▎         | 106/3753 [08:03<4:47:27,  4.73s/it]  3%|▎         | 107/3753 [08:08<4:45:29,  4.70s/it]  3%|▎         | 108/3753 [08:12<4:32:39,  4.49s/it]  3%|▎         | 109/3753 [08:16<4:22:36,  4.32s/it]  3%|▎         | 110/3753 [08:20<4:15:44,  4.21s/it]{'loss': 0.3204, 'grad_norm': 4003.90185546875, 'learning_rate': 2.8989361702127657e-07, 'mean_ratio_chosen': 3.2002809047698975, 'mean_ratio_rejected': 2.3013505935668945, 'weight_chosen': -0.24956202507019043, 'weight_rejected': 0.04146226868033409, 'kl_term_chosen': 1.163238525390625, 'epoch': 0.029313790806129246}
                                                    {'loss': 0.3204, 'grad_norm': 4003.90185546875, 'learning_rate': 2.8989361702127657e-07, 'mean_ratio_chosen': 3.2002809047698975, 'mean_ratio_rejected': 2.3013505935668945, 'weight_chosen': -0.24956202507019043, 'weight_rejected': 0.04146226868033409, 'kl_term_chosen': 1.163238525390625, 'epoch': 0.03}
  3%|▎         | 110/3753 [08:20<4:15:44,  4.21s/it]  3%|▎         | 111/3753 [08:26<4:43:57,  4.68s/it]  3%|▎         | 112/3753 [08:29<4:24:43,  4.36s/it]  3%|▎         | 113/3753 [08:33<4:05:26,  4.05s/it]  3%|▎         | 114/3753 [08:36<3:56:38,  3.90s/it]  3%|▎         | 115/3753 [08:40<3:55:19,  3.88s/it]  3%|▎         | 116/3753 [08:45<4:08:50,  4.11s/it]  3%|▎         | 117/3753 [08:49<4:09:22,  4.11s/it]  3%|▎         | 118/3753 [08:53<4:13:43,  4.19s/it]  3%|▎         | 119/3753 [09:01<5:22:34,  5.33s/it]  3%|▎         | 120/3753 [09:06<5:06:20,  5.06s/it]{'loss': 0.1416, 'grad_norm': 7343.951171875, 'learning_rate': 3.1648936170212764e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 1.455080270767212, 'weight_chosen': 4.69437313079834, 'weight_rejected': 0.49502578377723694, 'kl_term_chosen': -3.857818603515625, 'epoch': 0.031978680879413725}
                                                    {'loss': 0.1416, 'grad_norm': 7343.951171875, 'learning_rate': 3.1648936170212764e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 1.455080270767212, 'weight_chosen': 4.69437313079834, 'weight_rejected': 0.49502578377723694, 'kl_term_chosen': -3.857818603515625, 'epoch': 0.03}
  3%|▎         | 120/3753 [09:06<5:06:20,  5.06s/it]  3%|▎         | 121/3753 [09:10<4:52:03,  4.82s/it]  3%|▎         | 122/3753 [09:14<4:48:15,  4.76s/it]  3%|▎         | 123/3753 [09:19<4:39:11,  4.61s/it]  3%|▎         | 124/3753 [09:23<4:24:28,  4.37s/it]  3%|▎         | 125/3753 [09:27<4:32:33,  4.51s/it]  3%|▎         | 126/3753 [09:32<4:40:34,  4.64s/it]  3%|▎         | 127/3753 [09:36<4:22:13,  4.34s/it]  3%|▎         | 128/3753 [09:41<4:29:36,  4.46s/it]  3%|▎         | 129/3753 [09:46<4:52:26,  4.84s/it]  3%|▎         | 130/3753 [09:51<4:47:16,  4.76s/it]{'loss': 0.4868, 'grad_norm': 1777.521728515625, 'learning_rate': 3.430851063829787e-07, 'mean_ratio_chosen': 1.9339139461517334, 'mean_ratio_rejected': 0.23244743049144745, 'weight_chosen': -0.3620759844779968, 'weight_rejected': 0.10970578342676163, 'kl_term_chosen': 0.6595458984375, 'epoch': 0.034643570952698204}
                                                    {'loss': 0.4868, 'grad_norm': 1777.521728515625, 'learning_rate': 3.430851063829787e-07, 'mean_ratio_chosen': 1.9339139461517334, 'mean_ratio_rejected': 0.23244743049144745, 'weight_chosen': -0.3620759844779968, 'weight_rejected': 0.10970578342676163, 'kl_term_chosen': 0.6595458984375, 'epoch': 0.03}
  3%|▎         | 130/3753 [09:51<4:47:16,  4.76s/it]  3%|▎         | 131/3753 [09:55<4:32:09,  4.51s/it]  4%|▎         | 132/3753 [09:58<4:10:22,  4.15s/it]  4%|▎         | 133/3753 [10:03<4:19:23,  4.30s/it]  4%|▎         | 134/3753 [10:07<4:23:49,  4.37s/it]  4%|▎         | 135/3753 [10:11<4:12:14,  4.18s/it]  4%|▎         | 136/3753 [10:15<4:00:42,  3.99s/it]  4%|▎         | 137/3753 [10:18<3:55:58,  3.92s/it]  4%|▎         | 138/3753 [10:23<4:09:03,  4.13s/it]  4%|▎         | 139/3753 [10:27<4:01:37,  4.01s/it]  4%|▎         | 140/3753 [10:32<4:18:05,  4.29s/it]{'loss': 0.657, 'grad_norm': 4662.9443359375, 'learning_rate': 3.696808510638298e-07, 'mean_ratio_chosen': 0.7998039722442627, 'mean_ratio_rejected': 1.1400161981582642, 'weight_chosen': 1.083052396774292, 'weight_rejected': 0.14706450700759888, 'kl_term_chosen': -0.223388671875, 'epoch': 0.037308461025982675}
                                                    {'loss': 0.657, 'grad_norm': 4662.9443359375, 'learning_rate': 3.696808510638298e-07, 'mean_ratio_chosen': 0.7998039722442627, 'mean_ratio_rejected': 1.1400161981582642, 'weight_chosen': 1.083052396774292, 'weight_rejected': 0.14706450700759888, 'kl_term_chosen': -0.223388671875, 'epoch': 0.04}
  4%|▎         | 140/3753 [10:32<4:18:05,  4.29s/it]  4%|▍         | 141/3753 [10:36<4:10:10,  4.16s/it]  4%|▍         | 142/3753 [10:42<4:52:52,  4.87s/it]  4%|▍         | 143/3753 [10:46<4:39:10,  4.64s/it]  4%|▍         | 144/3753 [10:52<5:02:04,  5.02s/it]  4%|▍         | 145/3753 [10:59<5:43:41,  5.72s/it]  4%|▍         | 146/3753 [11:05<5:33:10,  5.54s/it]  4%|▍         | 147/3753 [11:12<5:59:17,  5.98s/it]  4%|▍         | 148/3753 [11:17<5:42:53,  5.71s/it]  4%|▍         | 149/3753 [11:21<5:11:01,  5.18s/it]  4%|▍         | 150/3753 [11:28<5:42:13,  5.70s/it]{'loss': 1.1783, 'grad_norm': 2639.762939453125, 'learning_rate': 3.962765957446808e-07, 'mean_ratio_chosen': 0.7463186979293823, 'mean_ratio_rejected': 0.16781693696975708, 'weight_chosen': 0.6280622482299805, 'weight_rejected': 0.5903069972991943, 'kl_term_chosen': -0.2926025390625, 'epoch': 0.039973351099267154}
                                                    {'loss': 1.1783, 'grad_norm': 2639.762939453125, 'learning_rate': 3.962765957446808e-07, 'mean_ratio_chosen': 0.7463186979293823, 'mean_ratio_rejected': 0.16781693696975708, 'weight_chosen': 0.6280622482299805, 'weight_rejected': 0.5903069972991943, 'kl_term_chosen': -0.2926025390625, 'epoch': 0.04}
  4%|▍         | 150/3753 [11:28<5:42:13,  5.70s/it]  4%|▍         | 151/3753 [11:33<5:33:32,  5.56s/it]  4%|▍         | 152/3753 [11:37<5:03:42,  5.06s/it]  4%|▍         | 153/3753 [11:42<4:59:55,  5.00s/it]  4%|▍         | 154/3753 [11:46<4:47:56,  4.80s/it]  4%|▍         | 155/3753 [11:50<4:39:17,  4.66s/it]  4%|▍         | 156/3753 [11:54<4:32:04,  4.54s/it]  4%|▍         | 157/3753 [11:58<4:18:36,  4.31s/it]  4%|▍         | 158/3753 [12:03<4:26:29,  4.45s/it]  4%|▍         | 159/3753 [12:07<4:19:07,  4.33s/it]  4%|▍         | 160/3753 [12:12<4:23:27,  4.40s/it]{'loss': 1.0135, 'grad_norm': 11883.3544921875, 'learning_rate': 4.228723404255319e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 2.944460391998291, 'weight_chosen': -1.850274682044983, 'weight_rejected': 0.3372035324573517, 'kl_term_chosen': 2.530303955078125, 'epoch': 0.04263824117255163}
                                                    {'loss': 1.0135, 'grad_norm': 11883.3544921875, 'learning_rate': 4.228723404255319e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 2.944460391998291, 'weight_chosen': -1.850274682044983, 'weight_rejected': 0.3372035324573517, 'kl_term_chosen': 2.530303955078125, 'epoch': 0.04}
  4%|▍         | 160/3753 [12:12<4:23:27,  4.40s/it]  4%|▍         | 161/3753 [12:16<4:15:02,  4.26s/it]  4%|▍         | 162/3753 [12:20<4:11:45,  4.21s/it]  4%|▍         | 163/3753 [12:23<3:56:51,  3.96s/it]  4%|▍         | 164/3753 [12:30<4:47:05,  4.80s/it]  4%|▍         | 165/3753 [12:36<5:19:27,  5.34s/it]  4%|▍         | 166/3753 [12:40<4:47:45,  4.81s/it]  4%|▍         | 167/3753 [12:45<4:48:13,  4.82s/it]  4%|▍         | 168/3753 [12:50<4:54:32,  4.93s/it]  5%|▍         | 169/3753 [12:53<4:28:04,  4.49s/it]  5%|▍         | 170/3753 [12:57<4:06:51,  4.13s/it]{'loss': 9.0998, 'grad_norm': 1391.4647216796875, 'learning_rate': 4.4946808510638295e-07, 'mean_ratio_chosen': 0.24252724647521973, 'mean_ratio_rejected': 3.7118241786956787, 'weight_chosen': 2.2889885902404785, 'weight_rejected': 0.13568955659866333, 'kl_term_chosen': -1.4166412353515625, 'epoch': 0.04530313124583611}
                                                    {'loss': 9.0998, 'grad_norm': 1391.4647216796875, 'learning_rate': 4.4946808510638295e-07, 'mean_ratio_chosen': 0.24252724647521973, 'mean_ratio_rejected': 3.7118241786956787, 'weight_chosen': 2.2889885902404785, 'weight_rejected': 0.13568955659866333, 'kl_term_chosen': -1.4166412353515625, 'epoch': 0.05}
  5%|▍         | 170/3753 [12:57<4:06:51,  4.13s/it]  5%|▍         | 171/3753 [13:01<4:13:56,  4.25s/it]  5%|▍         | 172/3753 [13:10<5:32:39,  5.57s/it]  5%|▍         | 173/3753 [13:14<4:59:01,  5.01s/it]  5%|▍         | 174/3753 [13:18<4:46:55,  4.81s/it]  5%|▍         | 175/3753 [13:22<4:39:59,  4.70s/it]  5%|▍         | 176/3753 [13:26<4:20:41,  4.37s/it]  5%|▍         | 177/3753 [13:31<4:22:49,  4.41s/it]  5%|▍         | 178/3753 [13:35<4:29:49,  4.53s/it]  5%|▍         | 179/3753 [13:39<4:12:10,  4.23s/it]  5%|▍         | 180/3753 [13:43<4:11:20,  4.22s/it]{'loss': 1.9823, 'grad_norm': 7613.7197265625, 'learning_rate': 4.76063829787234e-07, 'mean_ratio_chosen': 1.6298115253448486, 'mean_ratio_rejected': 1.166623592376709, 'weight_chosen': 0.44688212871551514, 'weight_rejected': 0.09268777817487717, 'kl_term_chosen': 0.48846435546875, 'epoch': 0.047968021319120584}
                                                    {'loss': 1.9823, 'grad_norm': 7613.7197265625, 'learning_rate': 4.76063829787234e-07, 'mean_ratio_chosen': 1.6298115253448486, 'mean_ratio_rejected': 1.166623592376709, 'weight_chosen': 0.44688212871551514, 'weight_rejected': 0.09268777817487717, 'kl_term_chosen': 0.48846435546875, 'epoch': 0.05}
  5%|▍         | 180/3753 [13:43<4:11:20,  4.22s/it]  5%|▍         | 181/3753 [13:48<4:17:05,  4.32s/it]  5%|▍         | 182/3753 [13:55<5:03:58,  5.11s/it]  5%|▍         | 183/3753 [13:59<4:54:10,  4.94s/it]  5%|▍         | 184/3753 [14:05<5:02:18,  5.08s/it]  5%|▍         | 185/3753 [14:09<4:57:57,  5.01s/it]  5%|▍         | 186/3753 [14:13<4:40:11,  4.71s/it]  5%|▍         | 187/3753 [14:17<4:26:12,  4.48s/it]  5%|▌         | 188/3753 [14:21<4:19:53,  4.37s/it]  5%|▌         | 189/3753 [14:28<5:05:33,  5.14s/it]  5%|▌         | 190/3753 [14:33<4:50:21,  4.89s/it]{'loss': 0.1069, 'grad_norm': 865.7190551757812, 'learning_rate': 5.026595744680851e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 1.4173449277877808, 'weight_chosen': 6.343138217926025, 'weight_rejected': 0.08509904146194458, 'kl_term_chosen': -5.46234130859375, 'epoch': 0.05063291139240506}
                                                    {'loss': 0.1069, 'grad_norm': 865.7190551757812, 'learning_rate': 5.026595744680851e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 1.4173449277877808, 'weight_chosen': 6.343138217926025, 'weight_rejected': 0.08509904146194458, 'kl_term_chosen': -5.46234130859375, 'epoch': 0.05}
  5%|▌         | 190/3753 [14:33<4:50:21,  4.89s/it]  5%|▌         | 191/3753 [14:36<4:29:00,  4.53s/it]  5%|▌         | 192/3753 [14:44<5:18:57,  5.37s/it]  5%|▌         | 193/3753 [14:48<4:59:33,  5.05s/it]  5%|▌         | 194/3753 [14:53<5:02:04,  5.09s/it]  5%|▌         | 195/3753 [14:57<4:44:49,  4.80s/it]  5%|▌         | 196/3753 [15:01<4:21:05,  4.40s/it]  5%|▌         | 197/3753 [15:10<5:37:35,  5.70s/it]  5%|▌         | 198/3753 [15:13<5:05:16,  5.15s/it]  5%|▌         | 199/3753 [15:19<5:12:58,  5.28s/it]  5%|▌         | 200/3753 [15:23<4:54:13,  4.97s/it]{'loss': 1.3338, 'grad_norm': 961.4284057617188, 'learning_rate': 5.292553191489362e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.47415024042129517, 'weight_chosen': 8.891407012939453, 'weight_rejected': 0.05582314357161522, 'kl_term_chosen': -7.94805908203125, 'epoch': 0.05329780146568954}
                                                    {'loss': 1.3338, 'grad_norm': 961.4284057617188, 'learning_rate': 5.292553191489362e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.47415024042129517, 'weight_chosen': 8.891407012939453, 'weight_rejected': 0.05582314357161522, 'kl_term_chosen': -7.94805908203125, 'epoch': 0.05}
  5%|▌         | 200/3753 [15:23<4:54:13,  4.97s/it]/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
  5%|▌         | 201/3753 [16:25<21:38:49, 21.94s/it]  5%|▌         | 202/3753 [16:28<16:13:50, 16.45s/it]  5%|▌         | 203/3753 [16:33<12:47:06, 12.97s/it]  5%|▌         | 204/3753 [16:37<10:03:03, 10.20s/it]  5%|▌         | 205/3753 [16:41<8:09:24,  8.28s/it]   5%|▌         | 206/3753 [16:48<7:44:58,  7.87s/it]  6%|▌         | 207/3753 [16:52<6:35:19,  6.69s/it]  6%|▌         | 208/3753 [16:57<6:11:12,  6.28s/it]  6%|▌         | 209/3753 [17:02<5:49:19,  5.91s/it]  6%|▌         | 210/3753 [17:06<5:23:11,  5.47s/it]{'loss': 2.3251, 'grad_norm': 8661.404296875, 'learning_rate': 5.558510638297872e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -1.9611704349517822, 'weight_rejected': 0.15304215252399445, 'kl_term_chosen': 2.7799072265625, 'epoch': 0.05596269153897402}
                                                    {'loss': 2.3251, 'grad_norm': 8661.404296875, 'learning_rate': 5.558510638297872e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -1.9611704349517822, 'weight_rejected': 0.15304215252399445, 'kl_term_chosen': 2.7799072265625, 'epoch': 0.06}
  6%|▌         | 210/3753 [17:07<5:23:11,  5.47s/it]  6%|▌         | 211/3753 [17:12<5:20:00,  5.42s/it]  6%|▌         | 212/3753 [17:15<4:46:18,  4.85s/it]  6%|▌         | 213/3753 [17:20<4:40:54,  4.76s/it]  6%|▌         | 214/3753 [17:25<4:56:01,  5.02s/it]  6%|▌         | 215/3753 [17:30<4:40:08,  4.75s/it]  6%|▌         | 216/3753 [17:34<4:32:12,  4.62s/it]  6%|▌         | 217/3753 [17:38<4:14:46,  4.32s/it]  6%|▌         | 218/3753 [17:42<4:22:39,  4.46s/it]  6%|▌         | 219/3753 [17:46<4:06:55,  4.19s/it]  6%|▌         | 220/3753 [17:50<3:59:02,  4.06s/it]{'loss': 4.7791, 'grad_norm': 341.7220153808594, 'learning_rate': 5.824468085106384e-07, 'mean_ratio_chosen': 0.4362148940563202, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 1.5113471746444702, 'weight_rejected': 0.1200256198644638, 'kl_term_chosen': -0.829620361328125, 'epoch': 0.05862758161225849}
                                                    {'loss': 4.7791, 'grad_norm': 341.7220153808594, 'learning_rate': 5.824468085106384e-07, 'mean_ratio_chosen': 0.4362148940563202, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 1.5113471746444702, 'weight_rejected': 0.1200256198644638, 'kl_term_chosen': -0.829620361328125, 'epoch': 0.06}
  6%|▌         | 220/3753 [17:50<3:59:02,  4.06s/it]  6%|▌         | 221/3753 [17:53<3:54:18,  3.98s/it]  6%|▌         | 222/3753 [17:58<3:57:22,  4.03s/it]  6%|▌         | 223/3753 [18:01<3:48:08,  3.88s/it]  6%|▌         | 224/3753 [18:05<3:46:46,  3.86s/it]  6%|▌         | 225/3753 [18:09<3:52:25,  3.95s/it]  6%|▌         | 226/3753 [18:13<3:49:50,  3.91s/it]  6%|▌         | 227/3753 [18:18<4:15:02,  4.34s/it]  6%|▌         | 228/3753 [18:22<3:57:24,  4.04s/it]  6%|▌         | 229/3753 [18:27<4:13:20,  4.31s/it]  6%|▌         | 230/3753 [18:33<4:50:43,  4.95s/it]{'loss': 0.9707, 'grad_norm': 7089.67431640625, 'learning_rate': 6.090425531914894e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': 9.621262550354004, 'weight_rejected': 0.1490352898836136, 'kl_term_chosen': -8.684051513671875, 'epoch': 0.06129247168554297}
                                                    {'loss': 0.9707, 'grad_norm': 7089.67431640625, 'learning_rate': 6.090425531914894e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': 9.621262550354004, 'weight_rejected': 0.1490352898836136, 'kl_term_chosen': -8.684051513671875, 'epoch': 0.06}
  6%|▌         | 230/3753 [18:33<4:50:43,  4.95s/it]  6%|▌         | 231/3753 [18:39<5:04:07,  5.18s/it]  6%|▌         | 232/3753 [18:43<4:47:46,  4.90s/it]  6%|▌         | 233/3753 [18:48<4:46:42,  4.89s/it]  6%|▌         | 234/3753 [18:53<4:54:27,  5.02s/it]  6%|▋         | 235/3753 [19:01<5:51:03,  5.99s/it]  6%|▋         | 236/3753 [19:05<5:17:40,  5.42s/it]  6%|▋         | 237/3753 [19:09<4:42:59,  4.83s/it]  6%|▋         | 238/3753 [19:13<4:36:20,  4.72s/it]  6%|▋         | 239/3753 [19:18<4:37:59,  4.75s/it]  6%|▋         | 240/3753 [19:23<4:39:36,  4.78s/it]{'loss': 1.3755, 'grad_norm': 1100.4610595703125, 'learning_rate': 6.356382978723404e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 11.9635591506958, 'weight_rejected': 0.17781086266040802, 'kl_term_chosen': -11.132415771484375, 'epoch': 0.06395736175882745}
                                                    {'loss': 1.3755, 'grad_norm': 1100.4610595703125, 'learning_rate': 6.356382978723404e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 11.9635591506958, 'weight_rejected': 0.17781086266040802, 'kl_term_chosen': -11.132415771484375, 'epoch': 0.06}
  6%|▋         | 240/3753 [19:23<4:39:36,  4.78s/it]  6%|▋         | 241/3753 [19:27<4:26:38,  4.56s/it]  6%|▋         | 242/3753 [19:31<4:19:03,  4.43s/it]  6%|▋         | 243/3753 [19:36<4:18:59,  4.43s/it]  7%|▋         | 244/3753 [19:41<4:44:37,  4.87s/it]  7%|▋         | 245/3753 [19:46<4:30:39,  4.63s/it]  7%|▋         | 246/3753 [19:49<4:14:36,  4.36s/it]  7%|▋         | 247/3753 [19:56<4:58:24,  5.11s/it]  7%|▋         | 248/3753 [20:01<4:56:30,  5.08s/it]  7%|▋         | 249/3753 [20:06<4:57:02,  5.09s/it]  7%|▋         | 250/3753 [20:12<5:02:16,  5.18s/it]{'loss': 1.0176, 'grad_norm': 4962.97802734375, 'learning_rate': 6.622340425531915e-07, 'mean_ratio_chosen': 2.451270580291748, 'mean_ratio_rejected': 0.5386041402816772, 'weight_chosen': -0.0018169879913330078, 'weight_rejected': 0.14804719388484955, 'kl_term_chosen': 0.8966064453125, 'epoch': 0.06662225183211193}
                                                    {'loss': 1.0176, 'grad_norm': 4962.97802734375, 'learning_rate': 6.622340425531915e-07, 'mean_ratio_chosen': 2.451270580291748, 'mean_ratio_rejected': 0.5386041402816772, 'weight_chosen': -0.0018169879913330078, 'weight_rejected': 0.14804719388484955, 'kl_term_chosen': 0.8966064453125, 'epoch': 0.07}
  7%|▋         | 250/3753 [20:12<5:02:16,  5.18s/it]  7%|▋         | 251/3753 [20:18<5:24:37,  5.56s/it]  7%|▋         | 252/3753 [20:23<5:14:52,  5.40s/it]  7%|▋         | 253/3753 [20:28<5:02:26,  5.18s/it]  7%|▋         | 254/3753 [20:31<4:32:23,  4.67s/it]  7%|▋         | 255/3753 [20:35<4:20:45,  4.47s/it]  7%|▋         | 256/3753 [20:39<4:05:31,  4.21s/it]  7%|▋         | 257/3753 [20:43<4:12:02,  4.33s/it]  7%|▋         | 258/3753 [20:47<4:01:42,  4.15s/it]  7%|▋         | 259/3753 [20:51<3:51:00,  3.97s/it]  7%|▋         | 260/3753 [20:55<4:01:31,  4.15s/it]{'loss': 3.3714, 'grad_norm': 843.2579956054688, 'learning_rate': 6.888297872340425e-07, 'mean_ratio_chosen': 0.18695068359375, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.564115047454834, 'weight_rejected': 0.13386748731136322, 'kl_term_chosen': -1.676910400390625, 'epoch': 0.06928714190539641}
                                                    {'loss': 3.3714, 'grad_norm': 843.2579956054688, 'learning_rate': 6.888297872340425e-07, 'mean_ratio_chosen': 0.18695068359375, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.564115047454834, 'weight_rejected': 0.13386748731136322, 'kl_term_chosen': -1.676910400390625, 'epoch': 0.07}
  7%|▋         | 260/3753 [20:55<4:01:31,  4.15s/it]  7%|▋         | 261/3753 [21:00<4:08:04,  4.26s/it]  7%|▋         | 262/3753 [21:04<4:09:44,  4.29s/it]  7%|▋         | 263/3753 [21:09<4:15:15,  4.39s/it]  7%|▋         | 264/3753 [21:13<4:05:11,  4.22s/it]  7%|▋         | 265/3753 [21:17<4:01:49,  4.16s/it]  7%|▋         | 266/3753 [21:20<3:54:34,  4.04s/it]  7%|▋         | 267/3753 [21:24<3:52:18,  4.00s/it]  7%|▋         | 268/3753 [21:29<4:10:06,  4.31s/it]  7%|▋         | 269/3753 [21:33<3:53:34,  4.02s/it]  7%|▋         | 270/3753 [21:37<4:01:23,  4.16s/it]{'loss': 0.9808, 'grad_norm': 4552.107421875, 'learning_rate': 7.154255319148937e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 3.446627378463745, 'weight_chosen': 17.965688705444336, 'weight_rejected': 0.05033063143491745, 'kl_term_chosen': -16.9901123046875, 'epoch': 0.07195203197868089}
                                                    {'loss': 0.9808, 'grad_norm': 4552.107421875, 'learning_rate': 7.154255319148937e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 3.446627378463745, 'weight_chosen': 17.965688705444336, 'weight_rejected': 0.05033063143491745, 'kl_term_chosen': -16.9901123046875, 'epoch': 0.07}
  7%|▋         | 270/3753 [21:37<4:01:23,  4.16s/it]  7%|▋         | 271/3753 [21:42<4:07:07,  4.26s/it]  7%|▋         | 272/3753 [21:46<4:09:17,  4.30s/it]  7%|▋         | 273/3753 [21:50<3:59:58,  4.14s/it]  7%|▋         | 274/3753 [21:54<3:54:37,  4.05s/it]  7%|▋         | 275/3753 [21:58<3:51:48,  4.00s/it]  7%|▋         | 276/3753 [22:01<3:44:33,  3.88s/it]  7%|▋         | 277/3753 [22:06<4:07:14,  4.27s/it]  7%|▋         | 278/3753 [22:10<3:53:16,  4.03s/it]  7%|▋         | 279/3753 [22:15<4:06:01,  4.25s/it]  7%|▋         | 280/3753 [22:19<4:16:44,  4.44s/it]{'loss': 2.6752, 'grad_norm': 301.6680908203125, 'learning_rate': 7.420212765957447e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 8.542434692382812, 'weight_rejected': 0.07263670116662979, 'kl_term_chosen': -7.611976623535156, 'epoch': 0.07461692205196535}
                                                    {'loss': 2.6752, 'grad_norm': 301.6680908203125, 'learning_rate': 7.420212765957447e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 8.542434692382812, 'weight_rejected': 0.07263670116662979, 'kl_term_chosen': -7.611976623535156, 'epoch': 0.07}
  7%|▋         | 280/3753 [22:20<4:16:44,  4.44s/it]  7%|▋         | 281/3753 [22:23<4:08:51,  4.30s/it]  8%|▊         | 282/3753 [22:29<4:35:43,  4.77s/it]  8%|▊         | 283/3753 [22:34<4:27:39,  4.63s/it]  8%|▊         | 284/3753 [22:38<4:22:01,  4.53s/it]  8%|▊         | 285/3753 [22:45<5:08:23,  5.34s/it]  8%|▊         | 286/3753 [22:49<4:41:51,  4.88s/it]  8%|▊         | 287/3753 [22:53<4:24:51,  4.58s/it]  8%|▊         | 288/3753 [23:02<5:36:15,  5.82s/it]  8%|▊         | 289/3753 [23:06<5:20:58,  5.56s/it]  8%|▊         | 290/3753 [23:15<6:15:56,  6.51s/it]{'loss': 1.0703, 'grad_norm': 2764.9306640625, 'learning_rate': 7.686170212765957e-07, 'mean_ratio_chosen': 0.17607469856739044, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.4903135299682617, 'weight_rejected': 0.20946967601776123, 'kl_term_chosen': -1.736846923828125, 'epoch': 0.07728181212524983}
                                                    {'loss': 1.0703, 'grad_norm': 2764.9306640625, 'learning_rate': 7.686170212765957e-07, 'mean_ratio_chosen': 0.17607469856739044, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.4903135299682617, 'weight_rejected': 0.20946967601776123, 'kl_term_chosen': -1.736846923828125, 'epoch': 0.08}
  8%|▊         | 290/3753 [23:15<6:15:56,  6.51s/it]  8%|▊         | 291/3753 [23:19<5:28:56,  5.70s/it]  8%|▊         | 292/3753 [23:25<5:31:55,  5.75s/it]  8%|▊         | 293/3753 [23:29<4:55:19,  5.12s/it]  8%|▊         | 294/3753 [23:33<4:36:32,  4.80s/it]  8%|▊         | 295/3753 [23:38<4:41:14,  4.88s/it]  8%|▊         | 296/3753 [23:42<4:23:38,  4.58s/it]  8%|▊         | 297/3753 [23:46<4:23:37,  4.58s/it]  8%|▊         | 298/3753 [23:52<4:42:36,  4.91s/it]  8%|▊         | 299/3753 [23:56<4:31:55,  4.72s/it]  8%|▊         | 300/3753 [24:00<4:22:17,  4.56s/it]{'loss': 0.3502, 'grad_norm': 4294.32958984375, 'learning_rate': 7.952127659574468e-07, 'mean_ratio_chosen': 1.035339117050171, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 0.8894128203392029, 'weight_rejected': 0.052618950605392456, 'kl_term_chosen': 0.03472900390625, 'epoch': 0.07994670219853431}
                                                    {'loss': 0.3502, 'grad_norm': 4294.32958984375, 'learning_rate': 7.952127659574468e-07, 'mean_ratio_chosen': 1.035339117050171, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 0.8894128203392029, 'weight_rejected': 0.052618950605392456, 'kl_term_chosen': 0.03472900390625, 'epoch': 0.08}
  8%|▊         | 300/3753 [24:00<4:22:17,  4.56s/it]  8%|▊         | 301/3753 [24:04<4:16:50,  4.46s/it]  8%|▊         | 302/3753 [24:10<4:26:28,  4.63s/it]  8%|▊         | 303/3753 [24:14<4:25:38,  4.62s/it]  8%|▊         | 304/3753 [24:17<3:58:41,  4.15s/it]  8%|▊         | 305/3753 [24:21<3:54:38,  4.08s/it]  8%|▊         | 306/3753 [24:25<3:54:36,  4.08s/it]  8%|▊         | 307/3753 [24:30<4:05:17,  4.27s/it]  8%|▊         | 308/3753 [24:36<4:30:14,  4.71s/it]  8%|▊         | 309/3753 [24:40<4:26:52,  4.65s/it]  8%|▊         | 310/3753 [24:44<4:15:43,  4.46s/it]{'loss': 1.3153, 'grad_norm': 2320.58203125, 'learning_rate': 8.218085106382978e-07, 'mean_ratio_chosen': 0.1680411398410797, 'mean_ratio_rejected': 0.45704811811447144, 'weight_chosen': 2.4472153186798096, 'weight_rejected': 0.22815649211406708, 'kl_term_chosen': -1.7835464477539062, 'epoch': 0.08261159227181879}
                                                    {'loss': 1.3153, 'grad_norm': 2320.58203125, 'learning_rate': 8.218085106382978e-07, 'mean_ratio_chosen': 0.1680411398410797, 'mean_ratio_rejected': 0.45704811811447144, 'weight_chosen': 2.4472153186798096, 'weight_rejected': 0.22815649211406708, 'kl_term_chosen': -1.7835464477539062, 'epoch': 0.08}
  8%|▊         | 310/3753 [24:44<4:15:43,  4.46s/it]  8%|▊         | 311/3753 [24:48<4:13:15,  4.41s/it]  8%|▊         | 312/3753 [24:53<4:08:46,  4.34s/it]  8%|▊         | 313/3753 [24:57<4:09:33,  4.35s/it]  8%|▊         | 314/3753 [25:04<4:56:49,  5.18s/it]  8%|▊         | 315/3753 [25:08<4:30:30,  4.72s/it]  8%|▊         | 316/3753 [25:11<4:11:44,  4.39s/it]  8%|▊         | 317/3753 [25:15<4:05:28,  4.29s/it]  8%|▊         | 318/3753 [25:20<4:11:34,  4.39s/it]  8%|▊         | 319/3753 [25:24<4:07:36,  4.33s/it]  9%|▊         | 320/3753 [25:29<4:12:57,  4.42s/it]{'loss': -0.3124, 'grad_norm': 746.1698608398438, 'learning_rate': 8.48404255319149e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 6.146012306213379, 'weight_rejected': 0.04084571450948715, 'kl_term_chosen': -5.20880126953125, 'epoch': 0.08527648234510327}
                                                    {'loss': -0.3124, 'grad_norm': 746.1698608398438, 'learning_rate': 8.48404255319149e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 6.146012306213379, 'weight_rejected': 0.04084571450948715, 'kl_term_chosen': -5.20880126953125, 'epoch': 0.09}
  9%|▊         | 320/3753 [25:29<4:12:57,  4.42s/it]  9%|▊         | 321/3753 [25:33<4:15:49,  4.47s/it]  9%|▊         | 322/3753 [25:37<4:04:49,  4.28s/it]  9%|▊         | 323/3753 [25:41<3:52:56,  4.07s/it]  9%|▊         | 324/3753 [25:48<4:48:19,  5.05s/it]  9%|▊         | 325/3753 [25:53<4:40:19,  4.91s/it]  9%|▊         | 326/3753 [25:56<4:17:29,  4.51s/it]  9%|▊         | 327/3753 [26:00<4:09:21,  4.37s/it]  9%|▊         | 328/3753 [26:04<4:02:13,  4.24s/it]  9%|▉         | 329/3753 [26:09<4:05:46,  4.31s/it]  9%|▉         | 330/3753 [26:14<4:17:15,  4.51s/it]{'loss': 9.4373, 'grad_norm': 811.368896484375, 'learning_rate': 8.75e-07, 'mean_ratio_chosen': 2.904477834701538, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': -0.20471543073654175, 'weight_rejected': 0.22135066986083984, 'kl_term_chosen': 1.066253662109375, 'epoch': 0.08794137241838774}
                                                    {'loss': 9.4373, 'grad_norm': 811.368896484375, 'learning_rate': 8.75e-07, 'mean_ratio_chosen': 2.904477834701538, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': -0.20471543073654175, 'weight_rejected': 0.22135066986083984, 'kl_term_chosen': 1.066253662109375, 'epoch': 0.09}
  9%|▉         | 330/3753 [26:14<4:17:15,  4.51s/it]  9%|▉         | 331/3753 [26:18<4:07:38,  4.34s/it]  9%|▉         | 332/3753 [26:22<3:59:06,  4.19s/it]  9%|▉         | 333/3753 [26:26<4:00:57,  4.23s/it]  9%|▉         | 334/3753 [26:34<5:02:07,  5.30s/it]  9%|▉         | 335/3753 [26:38<4:52:59,  5.14s/it]  9%|▉         | 336/3753 [26:42<4:26:07,  4.67s/it]  9%|▉         | 337/3753 [26:47<4:23:59,  4.64s/it]  9%|▉         | 338/3753 [26:50<4:03:00,  4.27s/it]  9%|▉         | 339/3753 [26:54<3:51:03,  4.06s/it]  9%|▉         | 340/3753 [26:58<3:56:23,  4.16s/it]{'loss': 0.2131, 'grad_norm': 2.6013405323028564, 'learning_rate': 9.01595744680851e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 18.26386070251465, 'weight_rejected': 0.7826625108718872, 'kl_term_chosen': -18.020217895507812, 'epoch': 0.09060626249167222}
                                                    {'loss': 0.2131, 'grad_norm': 2.6013405323028564, 'learning_rate': 9.01595744680851e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 18.26386070251465, 'weight_rejected': 0.7826625108718872, 'kl_term_chosen': -18.020217895507812, 'epoch': 0.09}
  9%|▉         | 340/3753 [26:58<3:56:23,  4.16s/it]  9%|▉         | 341/3753 [27:02<3:50:19,  4.05s/it]  9%|▉         | 342/3753 [27:06<3:49:47,  4.04s/it]  9%|▉         | 343/3753 [27:10<3:56:48,  4.17s/it]  9%|▉         | 344/3753 [27:14<3:50:34,  4.06s/it]  9%|▉         | 345/3753 [27:20<4:15:55,  4.51s/it]  9%|▉         | 346/3753 [27:24<4:09:46,  4.40s/it]  9%|▉         | 347/3753 [27:28<4:13:10,  4.46s/it]  9%|▉         | 348/3753 [27:34<4:25:27,  4.68s/it]  9%|▉         | 349/3753 [27:37<4:05:58,  4.34s/it]  9%|▉         | 350/3753 [27:42<4:07:39,  4.37s/it]{'loss': -2.6545, 'grad_norm': 0.0, 'learning_rate': 9.281914893617021e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 23.966060638427734, 'weight_rejected': 0.2186693549156189, 'kl_term_chosen': -23.13165283203125, 'epoch': 0.09327115256495669}
                                                    {'loss': -2.6545, 'grad_norm': 0.0, 'learning_rate': 9.281914893617021e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 23.966060638427734, 'weight_rejected': 0.2186693549156189, 'kl_term_chosen': -23.13165283203125, 'epoch': 0.09}
  9%|▉         | 350/3753 [27:42<4:07:39,  4.37s/it]  9%|▉         | 351/3753 [27:46<4:01:52,  4.27s/it]  9%|▉         | 352/3753 [27:49<3:46:59,  4.00s/it]  9%|▉         | 353/3753 [27:53<3:50:53,  4.07s/it]  9%|▉         | 354/3753 [27:57<3:53:54,  4.13s/it]  9%|▉         | 355/3753 [28:03<4:22:30,  4.64s/it]  9%|▉         | 356/3753 [28:08<4:16:34,  4.53s/it] 10%|▉         | 357/3753 [28:12<4:12:08,  4.45s/it] 10%|▉         | 358/3753 [28:16<4:12:21,  4.46s/it] 10%|▉         | 359/3753 [28:20<4:06:13,  4.35s/it] 10%|▉         | 360/3753 [28:24<3:53:12,  4.12s/it]{'loss': -3.6438, 'grad_norm': 0.0, 'learning_rate': 9.54787234042553e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 80.85724639892578, 'weight_rejected': 0.16667540371418, 'kl_term_chosen': -80.31292724609375, 'epoch': 0.09593604263824117}
                                                    {'loss': -3.6438, 'grad_norm': 0.0, 'learning_rate': 9.54787234042553e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 80.85724639892578, 'weight_rejected': 0.16667540371418, 'kl_term_chosen': -80.31292724609375, 'epoch': 0.1}
 10%|▉         | 360/3753 [28:24<3:53:12,  4.12s/it] 10%|▉         | 361/3753 [28:28<3:59:00,  4.23s/it] 10%|▉         | 362/3753 [28:33<4:00:39,  4.26s/it] 10%|▉         | 363/3753 [28:37<4:00:02,  4.25s/it] 10%|▉         | 364/3753 [28:42<4:07:19,  4.38s/it] 10%|▉         | 365/3753 [28:45<3:50:36,  4.08s/it] 10%|▉         | 366/3753 [28:48<3:37:28,  3.85s/it] 10%|▉         | 367/3753 [28:53<3:41:40,  3.93s/it] 10%|▉         | 368/3753 [28:57<3:56:13,  4.19s/it] 10%|▉         | 369/3753 [29:02<4:02:49,  4.31s/it] 10%|▉         | 370/3753 [29:06<4:01:54,  4.29s/it]{'loss': -3.7446, 'grad_norm': 0.0, 'learning_rate': 9.813829787234042e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 27.88837432861328, 'weight_rejected': 0.20561504364013672, 'kl_term_chosen': -27.024063110351562, 'epoch': 0.09860093271152565}
                                                    {'loss': -3.7446, 'grad_norm': 0.0, 'learning_rate': 9.813829787234042e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 27.88837432861328, 'weight_rejected': 0.20561504364013672, 'kl_term_chosen': -27.024063110351562, 'epoch': 0.1}
 10%|▉         | 370/3753 [29:06<4:01:54,  4.29s/it] 10%|▉         | 371/3753 [29:10<3:47:57,  4.04s/it] 10%|▉         | 372/3753 [29:14<3:56:13,  4.19s/it] 10%|▉         | 373/3753 [29:18<3:51:10,  4.10s/it] 10%|▉         | 374/3753 [29:25<4:43:33,  5.04s/it] 10%|▉         | 375/3753 [29:31<4:52:34,  5.20s/it] 10%|█         | 376/3753 [29:40<5:55:28,  6.32s/it] 10%|█         | 377/3753 [29:46<5:56:41,  6.34s/it] 10%|█         | 378/3753 [29:50<5:10:40,  5.52s/it] 10%|█         | 379/3753 [29:54<4:42:00,  5.02s/it] 10%|█         | 380/3753 [29:58<4:39:09,  4.97s/it]{'loss': -4.1339, 'grad_norm': 0.0, 'learning_rate': 9.999980527583044e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 30.18155288696289, 'weight_rejected': 0.02595735713839531, 'kl_term_chosen': -29.22119140625, 'epoch': 0.10126582278481013}
                                                    {'loss': -4.1339, 'grad_norm': 0.0, 'learning_rate': 9.999980527583044e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 30.18155288696289, 'weight_rejected': 0.02595735713839531, 'kl_term_chosen': -29.22119140625, 'epoch': 0.1}
 10%|█         | 380/3753 [29:59<4:39:09,  4.97s/it] 10%|█         | 381/3753 [30:03<4:25:55,  4.73s/it] 10%|█         | 382/3753 [30:07<4:23:11,  4.68s/it] 10%|█         | 383/3753 [30:11<4:08:08,  4.42s/it] 10%|█         | 384/3753 [30:16<4:20:47,  4.64s/it] 10%|█         | 385/3753 [30:26<5:54:47,  6.32s/it] 10%|█         | 386/3753 [30:30<5:07:05,  5.47s/it] 10%|█         | 387/3753 [30:34<4:48:26,  5.14s/it] 10%|█         | 388/3753 [30:39<4:35:40,  4.92s/it] 10%|█         | 389/3753 [30:43<4:27:41,  4.77s/it] 10%|█         | 390/3753 [30:49<4:48:53,  5.15s/it]{'loss': -4.1923, 'grad_norm': 0.0, 'learning_rate': 9.999634355500922e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 24.73794937133789, 'weight_rejected': 0.8164063692092896, 'kl_term_chosen': -24.09911346435547, 'epoch': 0.1039307128580946}
                                                    {'loss': -4.1923, 'grad_norm': 0.0, 'learning_rate': 9.999634355500922e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 24.73794937133789, 'weight_rejected': 0.8164063692092896, 'kl_term_chosen': -24.09911346435547, 'epoch': 0.1}
 10%|█         | 390/3753 [30:49<4:48:53,  5.15s/it] 10%|█         | 391/3753 [30:54<4:40:41,  5.01s/it] 10%|█         | 392/3753 [30:58<4:23:49,  4.71s/it] 10%|█         | 393/3753 [31:03<4:25:24,  4.74s/it] 10%|█         | 394/3753 [31:07<4:15:34,  4.57s/it] 11%|█         | 395/3753 [31:11<4:16:29,  4.58s/it] 11%|█         | 396/3753 [31:20<5:28:58,  5.88s/it] 11%|█         | 397/3753 [31:24<4:47:51,  5.15s/it] 11%|█         | 398/3753 [31:29<4:51:57,  5.22s/it] 11%|█         | 399/3753 [31:35<4:57:58,  5.33s/it] 11%|█         | 400/3753 [31:39<4:34:30,  4.91s/it]{'loss': -3.8733, 'grad_norm': 0.0, 'learning_rate': 9.998855497526103e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 28.363847732543945, 'weight_rejected': 0.25091278553009033, 'kl_term_chosen': -27.668716430664062, 'epoch': 0.10659560293137908}
                                                    {'loss': -3.8733, 'grad_norm': 0.0, 'learning_rate': 9.998855497526103e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 28.363847732543945, 'weight_rejected': 0.25091278553009033, 'kl_term_chosen': -27.668716430664062, 'epoch': 0.11}
 11%|█         | 400/3753 [31:39<4:34:30,  4.91s/it]/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
 11%|█         | 401/3753 [32:37<19:27:30, 20.90s/it] 11%|█         | 402/3753 [32:42<14:56:22, 16.05s/it] 11%|█         | 403/3753 [32:45<11:21:03, 12.20s/it] 11%|█         | 404/3753 [32:49<9:04:33,  9.76s/it]  11%|█         | 405/3753 [32:54<7:50:19,  8.43s/it] 11%|█         | 406/3753 [32:59<6:41:06,  7.19s/it] 11%|█         | 407/3753 [33:02<5:44:06,  6.17s/it] 11%|█         | 408/3753 [33:07<5:24:13,  5.82s/it] 11%|█         | 409/3753 [33:11<4:53:21,  5.26s/it] 11%|█         | 410/3753 [33:16<4:37:17,  4.98s/it]{'loss': -4.1402, 'grad_norm': 0.0, 'learning_rate': 9.997644021063695e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 37.434322357177734, 'weight_rejected': 0.28776782751083374, 'kl_term_chosen': -36.78741455078125, 'epoch': 0.10926049300466356}
                                                    {'loss': -4.1402, 'grad_norm': 0.0, 'learning_rate': 9.997644021063695e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 37.434322357177734, 'weight_rejected': 0.28776782751083374, 'kl_term_chosen': -36.78741455078125, 'epoch': 0.11}
 11%|█         | 410/3753 [33:16<4:37:17,  4.98s/it] 11%|█         | 411/3753 [33:21<4:39:46,  5.02s/it] 11%|█         | 412/3753 [33:25<4:21:00,  4.69s/it] 11%|█         | 413/3753 [33:30<4:36:53,  4.97s/it] 11%|█         | 414/3753 [33:35<4:36:05,  4.96s/it] 11%|█         | 415/3753 [33:39<4:17:59,  4.64s/it] 11%|█         | 416/3753 [33:44<4:15:35,  4.60s/it] 11%|█         | 417/3753 [33:48<4:04:55,  4.41s/it] 11%|█         | 418/3753 [33:51<3:57:04,  4.27s/it] 11%|█         | 419/3753 [33:55<3:51:49,  4.17s/it] 11%|█         | 420/3753 [33:59<3:46:19,  4.07s/it]{'loss': -4.2655, 'grad_norm': 0.0, 'learning_rate': 9.99600003095912e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 46.5095100402832, 'weight_rejected': 0.2727987766265869, 'kl_term_chosen': -45.858154296875, 'epoch': 0.11192538307794804}
                                                    {'loss': -4.2655, 'grad_norm': 0.0, 'learning_rate': 9.99600003095912e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 46.5095100402832, 'weight_rejected': 0.2727987766265869, 'kl_term_chosen': -45.858154296875, 'epoch': 0.11}
 11%|█         | 420/3753 [33:59<3:46:19,  4.07s/it]