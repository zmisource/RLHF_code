nohup: ignoring input
[W1123 09:38:02.896379877 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
设置随机种子为: 42
设置随机种子为: 42
设置随机种子为: 42
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
加载用于训练的策略模型 (Policy Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 95.31it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 96.40it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 95.91it/s]
加载作为参考的SFT模型 (Reference Model)...
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 96.08it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 97.72it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 96.77it/s]
⚙️  Running in WANDB offline mode
设置随机种子为: 42
从磁盘加载数据集 (注意: 数据集中不再需要 ref_logp)...
加载用于训练的策略模型 (Policy Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 84.14it/s]
加载作为参考的SFT模型 (Reference Model)...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 84.66it/s]
[W1123 09:38:07.388941696 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1123 09:38:07.399961356 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1123 09:38:07.402665965 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
[W1123 09:38:08.662699462 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:29500 (errno: 97 - Address family not supported by protocol).
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_kl_term2.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_kl_term2.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_kl_term2.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
初始化 CustomSymPOTrainer...
/rlhf_code/code/implement_final_v3_no_kl_term2.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSymPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(model=model, args=args, **kwargs)
Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
开始分布式训练...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in LlamaForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in LlamaDecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/accelerate/accelerator.py:1968: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.
  warnings.warn(
wandb: Tracking run with wandb version 0.21.3
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /root/wandb/offline-run-20251123_093815-npdm7pqu
  0%|          | 0/3753 [00:00<?, ?it/s]--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -44.0
Sample 0 - pi_logp_chosen:  -44.16946029663086
Sample 0 - Difference (pi - ref): -0.16946029663085938
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -100.5
Sample 0 - pi_logp_chosen:  -100.34402465820312
Sample 0 - Difference (pi - ref): 0.155975341796875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -112.5
Sample 0 - pi_logp_chosen:  -112.27210998535156
Sample 0 - Difference (pi - ref): 0.2278900146484375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -233.0
Sample 0 - pi_logp_chosen:  -233.74398803710938
Sample 0 - Difference (pi - ref): -0.743988037109375
---------------------------------
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -66.0
Sample 0 - pi_logp_chosen:  -66.15788269042969
Sample 0 - Difference (pi - ref): -0.1578826904296875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -246.0
Sample 0 - pi_logp_chosen:  -246.4224090576172
Sample 0 - Difference (pi - ref): -0.4224090576171875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -209.0
Sample 0 - pi_logp_chosen:  -208.26136779785156
Sample 0 - Difference (pi - ref): 0.7386322021484375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -382.0
Sample 0 - pi_logp_chosen:  -380.2176818847656
Sample 0 - Difference (pi - ref): 1.782318115234375
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -70.0
Sample 0 - pi_logp_chosen:  -70.24352264404297
Sample 0 - Difference (pi - ref): -0.24352264404296875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -167.0
Sample 0 - pi_logp_chosen:  -167.0938720703125
Sample 0 - Difference (pi - ref): -0.0938720703125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -350.0
Sample 0 - pi_logp_chosen:  -349.3069152832031
Sample 0 - Difference (pi - ref): 0.693084716796875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -452.0
Sample 0 - pi_logp_chosen:  -453.64239501953125
Sample 0 - Difference (pi - ref): -1.64239501953125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -82.0
Sample 0 - pi_logp_chosen:  -82.05374145507812
Sample 0 - Difference (pi - ref): -0.053741455078125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -98.5
Sample 0 - pi_logp_chosen:  -98.78225708007812
Sample 0 - Difference (pi - ref): -0.282257080078125
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -251.0
Sample 0 - pi_logp_chosen:  -249.6898956298828
Sample 0 - Difference (pi - ref): 1.3101043701171875
---------------------------------
--- Sanity Check at Step 0 ---
Sample 0 - ref_logp_chosen: -242.0
Sample 0 - pi_logp_chosen:  -242.80050659179688
Sample 0 - Difference (pi - ref): -0.800506591796875
---------------------------------
  0%|          | 1/3753 [00:04<4:55:17,  4.72s/it]{'loss': -0.311, 'grad_norm': 1853.1488037109375, 'learning_rate': 0.0, 'mean_ratio_chosen': 3.7065606117248535, 'mean_ratio_rejected': 1.4225232601165771, 'weight_chosen': 0.2668696641921997, 'weight_rejected': 0.044680867344141006, 'kl_term_chosen': 0.6550521850585938, 'epoch': 0.0002664890073284477}
                                                  {'loss': -0.311, 'grad_norm': 1853.1488037109375, 'learning_rate': 0.0, 'mean_ratio_chosen': 3.7065606117248535, 'mean_ratio_rejected': 1.4225232601165771, 'weight_chosen': 0.2668696641921997, 'weight_rejected': 0.044680867344141006, 'kl_term_chosen': 0.6550521850585938, 'epoch': 0.0}
  0%|          | 1/3753 [00:04<4:55:17,  4.72s/it]  0%|          | 2/3753 [00:10<5:18:10,  5.09s/it]  0%|          | 3/3753 [00:14<5:06:45,  4.91s/it]  0%|          | 4/3753 [00:18<4:42:38,  4.52s/it]  0%|          | 5/3753 [00:22<4:35:10,  4.41s/it]  0%|          | 6/3753 [00:28<4:56:20,  4.75s/it]  0%|          | 7/3753 [00:32<4:44:30,  4.56s/it]  0%|          | 8/3753 [00:36<4:42:33,  4.53s/it]  0%|          | 9/3753 [00:40<4:24:29,  4.24s/it]  0%|          | 10/3753 [00:44<4:15:57,  4.10s/it]{'loss': -0.4232, 'grad_norm': 1930.7095947265625, 'learning_rate': 2.3936170212765956e-08, 'mean_ratio_chosen': 0.7337622046470642, 'mean_ratio_rejected': 0.9156234860420227, 'weight_chosen': 0.3998701572418213, 'weight_rejected': 0.4163219630718231, 'kl_term_chosen': -0.15478515625, 'epoch': 0.002664890073284477}
                                                   {'loss': -0.4232, 'grad_norm': 1930.7095947265625, 'learning_rate': 2.3936170212765956e-08, 'mean_ratio_chosen': 0.7337622046470642, 'mean_ratio_rejected': 0.9156234860420227, 'weight_chosen': 0.3998701572418213, 'weight_rejected': 0.4163219630718231, 'kl_term_chosen': -0.15478515625, 'epoch': 0.0}
  0%|          | 10/3753 [00:44<4:15:57,  4.10s/it]  0%|          | 11/3753 [00:48<4:22:56,  4.22s/it]  0%|          | 12/3753 [00:53<4:37:45,  4.45s/it]  0%|          | 13/3753 [00:58<4:35:05,  4.41s/it]  0%|          | 14/3753 [01:02<4:28:17,  4.31s/it]  0%|          | 15/3753 [01:05<4:17:35,  4.13s/it]  0%|          | 16/3753 [01:09<4:12:50,  4.06s/it]  0%|          | 17/3753 [01:13<4:07:56,  3.98s/it]  0%|          | 18/3753 [01:18<4:24:55,  4.26s/it]  1%|          | 19/3753 [01:21<4:08:05,  3.99s/it]  1%|          | 20/3753 [01:25<4:05:50,  3.95s/it]{'loss': -0.3484, 'grad_norm': 1689.76171875, 'learning_rate': 5.053191489361702e-08, 'mean_ratio_chosen': 0.7691493034362793, 'mean_ratio_rejected': 1.2593389749526978, 'weight_chosen': 0.8638269305229187, 'weight_rejected': 0.29665425419807434, 'kl_term_chosen': -0.13123512268066406, 'epoch': 0.005329780146568954}
                                                   {'loss': -0.3484, 'grad_norm': 1689.76171875, 'learning_rate': 5.053191489361702e-08, 'mean_ratio_chosen': 0.7691493034362793, 'mean_ratio_rejected': 1.2593389749526978, 'weight_chosen': 0.8638269305229187, 'weight_rejected': 0.29665425419807434, 'kl_term_chosen': -0.13123512268066406, 'epoch': 0.01}
  1%|          | 20/3753 [01:25<4:05:50,  3.95s/it]  1%|          | 21/3753 [01:29<4:08:29,  4.00s/it]  1%|          | 22/3753 [01:37<5:15:45,  5.08s/it]  1%|          | 23/3753 [01:42<5:14:59,  5.07s/it]  1%|          | 24/3753 [01:45<4:41:25,  4.53s/it]  1%|          | 25/3753 [01:49<4:35:32,  4.43s/it]  1%|          | 26/3753 [01:52<4:09:12,  4.01s/it]  1%|          | 27/3753 [01:56<4:08:57,  4.01s/it]  1%|          | 28/3753 [02:00<4:06:23,  3.97s/it]  1%|          | 29/3753 [02:04<4:03:39,  3.93s/it]  1%|          | 30/3753 [02:08<4:08:20,  4.00s/it]{'loss': -0.1854, 'grad_norm': 1825.9117431640625, 'learning_rate': 7.712765957446808e-08, 'mean_ratio_chosen': 2.222442626953125, 'mean_ratio_rejected': 2.119828462600708, 'weight_chosen': 0.22864776849746704, 'weight_rejected': 0.5987815260887146, 'kl_term_chosen': 0.3993034362792969, 'epoch': 0.007994670219853431}
                                                   {'loss': -0.1854, 'grad_norm': 1825.9117431640625, 'learning_rate': 7.712765957446808e-08, 'mean_ratio_chosen': 2.222442626953125, 'mean_ratio_rejected': 2.119828462600708, 'weight_chosen': 0.22864776849746704, 'weight_rejected': 0.5987815260887146, 'kl_term_chosen': 0.3993034362792969, 'epoch': 0.01}
  1%|          | 30/3753 [02:08<4:08:20,  4.00s/it]  1%|          | 31/3753 [02:14<4:31:38,  4.38s/it]  1%|          | 32/3753 [02:17<4:17:37,  4.15s/it]  1%|          | 33/3753 [02:21<4:06:55,  3.98s/it]  1%|          | 34/3753 [02:28<4:58:15,  4.81s/it]  1%|          | 35/3753 [02:33<5:12:09,  5.04s/it]  1%|          | 36/3753 [02:39<5:34:43,  5.40s/it]  1%|          | 37/3753 [02:45<5:40:14,  5.49s/it]  1%|          | 38/3753 [02:49<5:17:54,  5.13s/it]  1%|          | 39/3753 [02:58<6:17:30,  6.10s/it]  1%|          | 40/3753 [03:02<5:39:52,  5.49s/it]{'loss': 1.4525, 'grad_norm': 2607.99560546875, 'learning_rate': 1.0372340425531915e-07, 'mean_ratio_chosen': 1.991242527961731, 'mean_ratio_rejected': 8.284889221191406, 'weight_chosen': 0.41341495513916016, 'weight_rejected': 0.1276526302099228, 'kl_term_chosen': 0.3443794250488281, 'epoch': 0.010659560293137908}
                                                   {'loss': 1.4525, 'grad_norm': 2607.99560546875, 'learning_rate': 1.0372340425531915e-07, 'mean_ratio_chosen': 1.991242527961731, 'mean_ratio_rejected': 8.284889221191406, 'weight_chosen': 0.41341495513916016, 'weight_rejected': 0.1276526302099228, 'kl_term_chosen': 0.3443794250488281, 'epoch': 0.01}
  1%|          | 40/3753 [03:02<5:39:52,  5.49s/it]  1%|          | 41/3753 [03:08<6:00:21,  5.82s/it]  1%|          | 42/3753 [03:11<5:08:20,  4.99s/it]  1%|          | 43/3753 [03:16<5:03:09,  4.90s/it]  1%|          | 44/3753 [03:20<4:47:33,  4.65s/it]  1%|          | 45/3753 [03:25<4:53:42,  4.75s/it]  1%|          | 46/3753 [03:30<4:58:10,  4.83s/it]  1%|▏         | 47/3753 [03:33<4:23:07,  4.26s/it]  1%|▏         | 48/3753 [03:37<4:19:24,  4.20s/it]  1%|▏         | 49/3753 [03:42<4:22:49,  4.26s/it]  1%|▏         | 50/3753 [03:45<4:15:37,  4.14s/it]{'loss': 0.2729, 'grad_norm': 1556.497802734375, 'learning_rate': 1.3031914893617022e-07, 'mean_ratio_chosen': 0.9993174076080322, 'mean_ratio_rejected': 2.009495496749878, 'weight_chosen': 0.8637330532073975, 'weight_rejected': 0.6851073503494263, 'kl_term_chosen': -0.0003414154052734375, 'epoch': 0.013324450366422385}
                                                   {'loss': 0.2729, 'grad_norm': 1556.497802734375, 'learning_rate': 1.3031914893617022e-07, 'mean_ratio_chosen': 0.9993174076080322, 'mean_ratio_rejected': 2.009495496749878, 'weight_chosen': 0.8637330532073975, 'weight_rejected': 0.6851073503494263, 'kl_term_chosen': -0.0003414154052734375, 'epoch': 0.01}
  1%|▏         | 50/3753 [03:46<4:15:37,  4.14s/it]  1%|▏         | 51/3753 [03:50<4:15:41,  4.14s/it]  1%|▏         | 52/3753 [03:54<4:26:18,  4.32s/it]  1%|▏         | 53/3753 [04:00<4:54:48,  4.78s/it]  1%|▏         | 54/3753 [04:04<4:30:26,  4.39s/it]  1%|▏         | 55/3753 [04:12<5:36:16,  5.46s/it]  1%|▏         | 56/3753 [04:19<6:20:26,  6.17s/it]  2%|▏         | 57/3753 [04:24<5:53:47,  5.74s/it]  2%|▏         | 58/3753 [04:28<5:11:40,  5.06s/it]  2%|▏         | 59/3753 [04:35<5:58:12,  5.82s/it]  2%|▏         | 60/3753 [04:38<5:08:36,  5.01s/it]{'loss': 0.3803, 'grad_norm': 1682.4705810546875, 'learning_rate': 1.5691489361702126e-07, 'mean_ratio_chosen': 0.926123321056366, 'mean_ratio_rejected': 1.3082771301269531, 'weight_chosen': 0.06858855485916138, 'weight_rejected': 0.937210738658905, 'kl_term_chosen': -0.03837394714355469, 'epoch': 0.015989340439706862}
                                                   {'loss': 0.3803, 'grad_norm': 1682.4705810546875, 'learning_rate': 1.5691489361702126e-07, 'mean_ratio_chosen': 0.926123321056366, 'mean_ratio_rejected': 1.3082771301269531, 'weight_chosen': 0.06858855485916138, 'weight_rejected': 0.937210738658905, 'kl_term_chosen': -0.03837394714355469, 'epoch': 0.02}
  2%|▏         | 60/3753 [04:39<5:08:36,  5.01s/it]  2%|▏         | 61/3753 [04:44<5:17:57,  5.17s/it]  2%|▏         | 62/3753 [04:48<5:01:51,  4.91s/it]  2%|▏         | 63/3753 [04:53<4:52:30,  4.76s/it]  2%|▏         | 64/3753 [04:58<5:01:25,  4.90s/it]  2%|▏         | 65/3753 [05:02<4:43:33,  4.61s/it]  2%|▏         | 66/3753 [05:07<4:52:50,  4.77s/it]  2%|▏         | 67/3753 [05:12<4:50:38,  4.73s/it]  2%|▏         | 68/3753 [05:15<4:34:23,  4.47s/it]  2%|▏         | 69/3753 [05:20<4:37:45,  4.52s/it]  2%|▏         | 70/3753 [05:24<4:24:01,  4.30s/it]{'loss': 0.8323, 'grad_norm': 1750.2701416015625, 'learning_rate': 1.8351063829787234e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 3.1927692890167236, 'weight_chosen': -0.812587320804596, 'weight_rejected': 0.044680867344141006, 'kl_term_chosen': 1.77581787109375, 'epoch': 0.018654230512991338}
                                                   {'loss': 0.8323, 'grad_norm': 1750.2701416015625, 'learning_rate': 1.8351063829787234e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 3.1927692890167236, 'weight_chosen': -0.812587320804596, 'weight_rejected': 0.044680867344141006, 'kl_term_chosen': 1.77581787109375, 'epoch': 0.02}
  2%|▏         | 70/3753 [05:24<4:24:01,  4.30s/it]  2%|▏         | 71/3753 [05:29<4:33:15,  4.45s/it]  2%|▏         | 72/3753 [05:33<4:35:40,  4.49s/it]  2%|▏         | 73/3753 [05:37<4:23:32,  4.30s/it]  2%|▏         | 74/3753 [05:41<4:13:17,  4.13s/it]  2%|▏         | 75/3753 [05:45<4:04:26,  3.99s/it]  2%|▏         | 76/3753 [05:50<4:37:07,  4.52s/it]  2%|▏         | 77/3753 [05:54<4:13:24,  4.14s/it]  2%|▏         | 78/3753 [05:59<4:31:05,  4.43s/it]  2%|▏         | 79/3753 [06:03<4:29:12,  4.40s/it]  2%|▏         | 80/3753 [06:08<4:39:30,  4.57s/it]{'loss': 0.5478, 'grad_norm': 2039.096435546875, 'learning_rate': 2.101063829787234e-07, 'mean_ratio_chosen': 2.097066640853882, 'mean_ratio_rejected': 1.5529954433441162, 'weight_chosen': 0.5581390261650085, 'weight_rejected': 0.10230471193790436, 'kl_term_chosen': 0.370269775390625, 'epoch': 0.021319120586275817}
                                                   {'loss': 0.5478, 'grad_norm': 2039.096435546875, 'learning_rate': 2.101063829787234e-07, 'mean_ratio_chosen': 2.097066640853882, 'mean_ratio_rejected': 1.5529954433441162, 'weight_chosen': 0.5581390261650085, 'weight_rejected': 0.10230471193790436, 'kl_term_chosen': 0.370269775390625, 'epoch': 0.02}
  2%|▏         | 80/3753 [06:08<4:39:30,  4.57s/it]  2%|▏         | 81/3753 [06:13<4:43:52,  4.64s/it]  2%|▏         | 82/3753 [06:17<4:28:54,  4.40s/it]  2%|▏         | 83/3753 [06:23<5:12:01,  5.10s/it]  2%|▏         | 84/3753 [06:28<5:07:52,  5.03s/it]  2%|▏         | 85/3753 [06:32<4:40:05,  4.58s/it]  2%|▏         | 86/3753 [06:35<4:17:44,  4.22s/it]  2%|▏         | 87/3753 [06:42<5:08:29,  5.05s/it]  2%|▏         | 88/3753 [06:46<4:45:41,  4.68s/it]  2%|▏         | 89/3753 [06:50<4:40:21,  4.59s/it]  2%|▏         | 90/3753 [06:55<4:36:06,  4.52s/it]{'loss': 0.5561, 'grad_norm': 1705.6512451171875, 'learning_rate': 2.3670212765957445e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -1.40299654006958, 'weight_rejected': 0.15002880990505219, 'kl_term_chosen': 2.192230224609375, 'epoch': 0.023984010659560292}
                                                   {'loss': 0.5561, 'grad_norm': 1705.6512451171875, 'learning_rate': 2.3670212765957445e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -1.40299654006958, 'weight_rejected': 0.15002880990505219, 'kl_term_chosen': 2.192230224609375, 'epoch': 0.02}
  2%|▏         | 90/3753 [06:55<4:36:06,  4.52s/it]  2%|▏         | 91/3753 [06:58<4:14:00,  4.16s/it]  2%|▏         | 92/3753 [07:02<4:10:27,  4.10s/it]  2%|▏         | 93/3753 [07:07<4:34:28,  4.50s/it]  3%|▎         | 94/3753 [07:11<4:25:51,  4.36s/it]  3%|▎         | 95/3753 [07:18<5:08:00,  5.05s/it]  3%|▎         | 96/3753 [07:22<4:49:17,  4.75s/it]  3%|▎         | 97/3753 [07:26<4:40:42,  4.61s/it]  3%|▎         | 98/3753 [07:32<5:03:30,  4.98s/it]  3%|▎         | 99/3753 [07:36<4:35:56,  4.53s/it]  3%|▎         | 100/3753 [07:40<4:22:59,  4.32s/it]{'loss': 0.5936, 'grad_norm': 1302.8135986328125, 'learning_rate': 2.632978723404255e-07, 'mean_ratio_chosen': 1.3299800157546997, 'mean_ratio_rejected': 1.3778104782104492, 'weight_chosen': 0.4853692650794983, 'weight_rejected': 0.14414885640144348, 'kl_term_chosen': 0.14258193969726562, 'epoch': 0.02664890073284477}
                                                    {'loss': 0.5936, 'grad_norm': 1302.8135986328125, 'learning_rate': 2.632978723404255e-07, 'mean_ratio_chosen': 1.3299800157546997, 'mean_ratio_rejected': 1.3778104782104492, 'weight_chosen': 0.4853692650794983, 'weight_rejected': 0.14414885640144348, 'kl_term_chosen': 0.14258193969726562, 'epoch': 0.03}
  3%|▎         | 100/3753 [07:40<4:22:59,  4.32s/it]  3%|▎         | 101/3753 [07:44<4:28:13,  4.41s/it]  3%|▎         | 102/3753 [07:48<4:23:35,  4.33s/it]  3%|▎         | 103/3753 [07:52<4:12:56,  4.16s/it]  3%|▎         | 104/3753 [07:58<4:40:31,  4.61s/it]  3%|▎         | 105/3753 [08:02<4:35:15,  4.53s/it]  3%|▎         | 106/3753 [08:07<4:51:19,  4.79s/it]  3%|▎         | 107/3753 [08:12<4:48:55,  4.75s/it]  3%|▎         | 108/3753 [08:16<4:35:30,  4.54s/it]  3%|▎         | 109/3753 [08:20<4:24:42,  4.36s/it]  3%|▎         | 110/3753 [08:24<4:17:04,  4.23s/it]{'loss': 0.528, 'grad_norm': 1666.8480224609375, 'learning_rate': 2.8989361702127657e-07, 'mean_ratio_chosen': 6.044498920440674, 'mean_ratio_rejected': 5.7057271003723145, 'weight_chosen': 0.01410222053527832, 'weight_rejected': 0.04146226868033409, 'kl_term_chosen': 0.8995742797851562, 'epoch': 0.029313790806129246}
                                                    {'loss': 0.528, 'grad_norm': 1666.8480224609375, 'learning_rate': 2.8989361702127657e-07, 'mean_ratio_chosen': 6.044498920440674, 'mean_ratio_rejected': 5.7057271003723145, 'weight_chosen': 0.01410222053527832, 'weight_rejected': 0.04146226868033409, 'kl_term_chosen': 0.8995742797851562, 'epoch': 0.03}
  3%|▎         | 110/3753 [08:24<4:17:04,  4.23s/it]  3%|▎         | 111/3753 [08:30<4:47:23,  4.73s/it]  3%|▎         | 112/3753 [08:34<4:27:30,  4.41s/it]  3%|▎         | 113/3753 [08:37<4:07:57,  4.09s/it]  3%|▎         | 114/3753 [08:40<3:58:48,  3.94s/it]  3%|▎         | 115/3753 [08:44<3:56:36,  3.90s/it]  3%|▎         | 116/3753 [08:49<4:10:57,  4.14s/it]  3%|▎         | 117/3753 [08:53<4:10:49,  4.14s/it]  3%|▎         | 118/3753 [08:58<4:14:57,  4.21s/it]  3%|▎         | 119/3753 [09:05<5:17:45,  5.25s/it]  3%|▎         | 120/3753 [09:10<5:03:55,  5.02s/it]{'loss': 0.7916, 'grad_norm': 1191.3037109375, 'learning_rate': 3.1648936170212764e-07, 'mean_ratio_chosen': 1.7259281873703003, 'mean_ratio_rejected': 0.28741997480392456, 'weight_chosen': 0.5636720657348633, 'weight_rejected': 0.49502578377723694, 'kl_term_chosen': 0.27288246154785156, 'epoch': 0.031978680879413725}
                                                    {'loss': 0.7916, 'grad_norm': 1191.3037109375, 'learning_rate': 3.1648936170212764e-07, 'mean_ratio_chosen': 1.7259281873703003, 'mean_ratio_rejected': 0.28741997480392456, 'weight_chosen': 0.5636720657348633, 'weight_rejected': 0.49502578377723694, 'kl_term_chosen': 0.27288246154785156, 'epoch': 0.03}
  3%|▎         | 120/3753 [09:10<5:03:55,  5.02s/it]  3%|▎         | 121/3753 [09:14<4:50:47,  4.80s/it]  3%|▎         | 122/3753 [09:19<4:48:54,  4.77s/it]  3%|▎         | 123/3753 [09:23<4:38:54,  4.61s/it]  3%|▎         | 124/3753 [09:27<4:23:55,  4.36s/it]  3%|▎         | 125/3753 [09:31<4:28:56,  4.45s/it]  3%|▎         | 126/3753 [09:36<4:38:33,  4.61s/it]  3%|▎         | 127/3753 [09:40<4:20:41,  4.31s/it]  3%|▎         | 128/3753 [09:45<4:29:26,  4.46s/it]  3%|▎         | 129/3753 [09:50<4:51:08,  4.82s/it]  3%|▎         | 130/3753 [09:55<4:46:14,  4.74s/it]{'loss': 1.3257, 'grad_norm': 4879.63232421875, 'learning_rate': 3.430851063829787e-07, 'mean_ratio_chosen': 9.686090469360352, 'mean_ratio_rejected': 0.44635164737701416, 'weight_chosen': -0.8378755450248718, 'weight_rejected': 0.10970578342676163, 'kl_term_chosen': 1.135345458984375, 'epoch': 0.034643570952698204}
                                                    {'loss': 1.3257, 'grad_norm': 4879.63232421875, 'learning_rate': 3.430851063829787e-07, 'mean_ratio_chosen': 9.686090469360352, 'mean_ratio_rejected': 0.44635164737701416, 'weight_chosen': -0.8378755450248718, 'weight_rejected': 0.10970578342676163, 'kl_term_chosen': 1.135345458984375, 'epoch': 0.03}
  3%|▎         | 130/3753 [09:55<4:46:14,  4.74s/it]  3%|▎         | 131/3753 [09:59<4:31:10,  4.49s/it]  4%|▎         | 132/3753 [10:02<4:10:01,  4.14s/it]  4%|▎         | 133/3753 [10:07<4:20:28,  4.32s/it]  4%|▎         | 134/3753 [10:11<4:24:50,  4.39s/it]  4%|▎         | 135/3753 [10:15<4:12:46,  4.19s/it]  4%|▎         | 136/3753 [10:19<4:01:06,  4.00s/it]  4%|▎         | 137/3753 [10:23<3:56:12,  3.92s/it]  4%|▎         | 138/3753 [10:27<4:09:27,  4.14s/it]  4%|▎         | 139/3753 [10:31<4:03:33,  4.04s/it]  4%|▎         | 140/3753 [10:36<4:20:51,  4.33s/it]{'loss': 0.4145, 'grad_norm': 2034.7601318359375, 'learning_rate': 3.696808510638298e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 5.159022331237793, 'weight_chosen': -0.503495454788208, 'weight_rejected': 0.14706450700759888, 'kl_term_chosen': 1.3631591796875, 'epoch': 0.037308461025982675}
                                                    {'loss': 0.4145, 'grad_norm': 2034.7601318359375, 'learning_rate': 3.696808510638298e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 5.159022331237793, 'weight_chosen': -0.503495454788208, 'weight_rejected': 0.14706450700759888, 'kl_term_chosen': 1.3631591796875, 'epoch': 0.04}
  4%|▎         | 140/3753 [10:36<4:20:51,  4.33s/it]  4%|▍         | 141/3753 [10:40<4:11:49,  4.18s/it]  4%|▍         | 142/3753 [10:46<4:52:20,  4.86s/it]  4%|▍         | 143/3753 [10:50<4:38:37,  4.63s/it]  4%|▍         | 144/3753 [10:56<5:01:47,  5.02s/it]  4%|▍         | 145/3753 [11:04<5:41:38,  5.68s/it]  4%|▍         | 146/3753 [11:09<5:34:36,  5.57s/it]  4%|▍         | 147/3753 [11:16<6:00:41,  6.00s/it]  4%|▍         | 148/3753 [11:21<5:44:33,  5.73s/it]  4%|▍         | 149/3753 [11:25<5:13:06,  5.21s/it]  4%|▍         | 150/3753 [11:32<5:42:47,  5.71s/it]{'loss': 0.8349, 'grad_norm': 2007.4786376953125, 'learning_rate': 3.962765957446808e-07, 'mean_ratio_chosen': 0.8089478015899658, 'mean_ratio_rejected': 0.33885470032691956, 'weight_chosen': 0.4414701461791992, 'weight_rejected': 0.5903069972991943, 'kl_term_chosen': -0.10601043701171875, 'epoch': 0.039973351099267154}
                                                    {'loss': 0.8349, 'grad_norm': 2007.4786376953125, 'learning_rate': 3.962765957446808e-07, 'mean_ratio_chosen': 0.8089478015899658, 'mean_ratio_rejected': 0.33885470032691956, 'weight_chosen': 0.4414701461791992, 'weight_rejected': 0.5903069972991943, 'kl_term_chosen': -0.10601043701171875, 'epoch': 0.04}
  4%|▍         | 150/3753 [11:32<5:42:47,  5.71s/it]  4%|▍         | 151/3753 [11:37<5:37:20,  5.62s/it]  4%|▍         | 152/3753 [11:41<5:06:46,  5.11s/it]  4%|▍         | 153/3753 [11:46<5:01:38,  5.03s/it]  4%|▍         | 154/3753 [11:50<4:49:33,  4.83s/it]  4%|▍         | 155/3753 [11:55<4:41:54,  4.70s/it]  4%|▍         | 156/3753 [11:59<4:36:43,  4.62s/it]  4%|▍         | 157/3753 [12:03<4:21:54,  4.37s/it]  4%|▍         | 158/3753 [12:08<4:28:26,  4.48s/it]  4%|▍         | 159/3753 [12:12<4:20:49,  4.35s/it]  4%|▍         | 160/3753 [12:16<4:25:42,  4.44s/it]{'loss': 1.1725, 'grad_norm': 1752.8992919921875, 'learning_rate': 4.228723404255319e-07, 'mean_ratio_chosen': 0.715180516242981, 'mean_ratio_rejected': 0.7570773959159851, 'weight_chosen': 0.8476394414901733, 'weight_rejected': 0.3372035324573517, 'kl_term_chosen': -0.16761016845703125, 'epoch': 0.04263824117255163}
                                                    {'loss': 1.1725, 'grad_norm': 1752.8992919921875, 'learning_rate': 4.228723404255319e-07, 'mean_ratio_chosen': 0.715180516242981, 'mean_ratio_rejected': 0.7570773959159851, 'weight_chosen': 0.8476394414901733, 'weight_rejected': 0.3372035324573517, 'kl_term_chosen': -0.16761016845703125, 'epoch': 0.04}
  4%|▍         | 160/3753 [12:16<4:25:42,  4.44s/it]  4%|▍         | 161/3753 [12:20<4:18:07,  4.31s/it]  4%|▍         | 162/3753 [12:25<4:17:20,  4.30s/it]  4%|▍         | 163/3753 [12:28<4:01:08,  4.03s/it]  4%|▍         | 164/3753 [12:35<4:50:33,  4.86s/it]  4%|▍         | 165/3753 [12:41<5:22:11,  5.39s/it]  4%|▍         | 166/3753 [12:45<4:50:10,  4.85s/it]  4%|▍         | 167/3753 [12:50<4:52:55,  4.90s/it]  4%|▍         | 168/3753 [12:55<4:58:46,  5.00s/it]  5%|▍         | 169/3753 [12:59<4:31:05,  4.54s/it]  5%|▍         | 170/3753 [13:02<4:08:31,  4.16s/it]{'loss': 0.695, 'grad_norm': 2202.352783203125, 'learning_rate': 4.4946808510638295e-07, 'mean_ratio_chosen': 9.958245277404785, 'mean_ratio_rejected': 2.7673308849334717, 'weight_chosen': -0.276853084564209, 'weight_rejected': 0.13568955659866333, 'kl_term_chosen': 1.149200439453125, 'epoch': 0.04530313124583611}
                                                    {'loss': 0.695, 'grad_norm': 2202.352783203125, 'learning_rate': 4.4946808510638295e-07, 'mean_ratio_chosen': 9.958245277404785, 'mean_ratio_rejected': 2.7673308849334717, 'weight_chosen': -0.276853084564209, 'weight_rejected': 0.13568955659866333, 'kl_term_chosen': 1.149200439453125, 'epoch': 0.05}
  5%|▍         | 170/3753 [13:02<4:08:31,  4.16s/it]  5%|▍         | 171/3753 [13:07<4:15:48,  4.28s/it]  5%|▍         | 172/3753 [13:16<5:40:05,  5.70s/it]  5%|▍         | 173/3753 [13:19<5:03:51,  5.09s/it]  5%|▍         | 174/3753 [13:24<4:51:36,  4.89s/it]  5%|▍         | 175/3753 [13:28<4:43:30,  4.75s/it]  5%|▍         | 176/3753 [13:32<4:24:52,  4.44s/it]  5%|▍         | 177/3753 [13:36<4:27:08,  4.48s/it]  5%|▍         | 178/3753 [13:41<4:33:36,  4.59s/it]  5%|▍         | 179/3753 [13:45<4:16:33,  4.31s/it]  5%|▍         | 180/3753 [13:49<4:14:23,  4.27s/it]{'loss': 1.0292, 'grad_norm': 1636.9248046875, 'learning_rate': 4.76063829787234e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 4.072040557861328, 'weight_chosen': -0.6677266359329224, 'weight_rejected': 0.09268777817487717, 'kl_term_chosen': 1.6030731201171875, 'epoch': 0.047968021319120584}
                                                    {'loss': 1.0292, 'grad_norm': 1636.9248046875, 'learning_rate': 4.76063829787234e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 4.072040557861328, 'weight_chosen': -0.6677266359329224, 'weight_rejected': 0.09268777817487717, 'kl_term_chosen': 1.6030731201171875, 'epoch': 0.05}
  5%|▍         | 180/3753 [13:49<4:14:23,  4.27s/it]  5%|▍         | 181/3753 [13:54<4:19:32,  4.36s/it]  5%|▍         | 182/3753 [14:01<5:07:40,  5.17s/it]  5%|▍         | 183/3753 [14:05<4:57:37,  5.00s/it]  5%|▍         | 184/3753 [14:11<5:06:57,  5.16s/it]  5%|▍         | 185/3753 [14:16<5:03:20,  5.10s/it]  5%|▍         | 186/3753 [14:20<4:44:30,  4.79s/it]  5%|▍         | 187/3753 [14:24<4:30:03,  4.54s/it]  5%|▌         | 188/3753 [14:28<4:24:07,  4.45s/it]  5%|▌         | 189/3753 [14:35<5:11:02,  5.24s/it]  5%|▌         | 190/3753 [14:40<4:56:17,  4.99s/it]{'loss': 1.3178, 'grad_norm': 3080.493408203125, 'learning_rate': 5.026595744680851e-07, 'mean_ratio_chosen': 1.9214423894882202, 'mean_ratio_rejected': 1.613972783088684, 'weight_chosen': 0.5542590022087097, 'weight_rejected': 0.08509904146194458, 'kl_term_chosen': 0.3265380859375, 'epoch': 0.05063291139240506}
                                                    {'loss': 1.3178, 'grad_norm': 3080.493408203125, 'learning_rate': 5.026595744680851e-07, 'mean_ratio_chosen': 1.9214423894882202, 'mean_ratio_rejected': 1.613972783088684, 'weight_chosen': 0.5542590022087097, 'weight_rejected': 0.08509904146194458, 'kl_term_chosen': 0.3265380859375, 'epoch': 0.05}
  5%|▌         | 190/3753 [14:40<4:56:17,  4.99s/it]  5%|▌         | 191/3753 [14:43<4:32:53,  4.60s/it]  5%|▌         | 192/3753 [14:51<5:23:02,  5.44s/it]  5%|▌         | 193/3753 [14:55<5:04:20,  5.13s/it]  5%|▌         | 194/3753 [15:00<5:07:04,  5.18s/it]  5%|▌         | 195/3753 [15:05<4:48:28,  4.86s/it]  5%|▌         | 196/3753 [15:08<4:25:15,  4.47s/it]  5%|▌         | 197/3753 [15:17<5:42:12,  5.77s/it]  5%|▌         | 198/3753 [15:21<5:08:48,  5.21s/it]  5%|▌         | 199/3753 [15:26<5:16:32,  5.34s/it]  5%|▌         | 200/3753 [15:31<4:57:54,  5.03s/it]{'loss': 2.1239, 'grad_norm': 1796.10498046875, 'learning_rate': 5.292553191489362e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 1.6822658777236938, 'weight_chosen': -1.138759970664978, 'weight_rejected': 0.05582314357161522, 'kl_term_chosen': 2.0821075439453125, 'epoch': 0.05329780146568954}
                                                    {'loss': 2.1239, 'grad_norm': 1796.10498046875, 'learning_rate': 5.292553191489362e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 1.6822658777236938, 'weight_chosen': -1.138759970664978, 'weight_rejected': 0.05582314357161522, 'kl_term_chosen': 2.0821075439453125, 'epoch': 0.05}
  5%|▌         | 200/3753 [15:31<4:57:54,  5.03s/it]  5%|▌         | 201/3753 [15:37<5:25:06,  5.49s/it]  5%|▌         | 202/3753 [15:41<4:59:35,  5.06s/it]  5%|▌         | 203/3753 [15:46<4:58:34,  5.05s/it]  5%|▌         | 204/3753 [15:50<4:33:45,  4.63s/it]  5%|▌         | 205/3753 [15:54<4:26:35,  4.51s/it]  5%|▌         | 206/3753 [16:01<4:58:11,  5.04s/it]  6%|▌         | 207/3753 [16:05<4:42:30,  4.78s/it]  6%|▌         | 208/3753 [16:10<4:50:01,  4.91s/it]  6%|▌         | 209/3753 [16:15<4:48:37,  4.89s/it]  6%|▌         | 210/3753 [16:19<4:44:04,  4.81s/it]{'loss': 1.1382, 'grad_norm': 1385.25537109375, 'learning_rate': 5.558510638297872e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 7.312895774841309, 'weight_chosen': -0.36576104164123535, 'weight_rejected': 0.15304215252399445, 'kl_term_chosen': 1.1844978332519531, 'epoch': 0.05596269153897402}
                                                    {'loss': 1.1382, 'grad_norm': 1385.25537109375, 'learning_rate': 5.558510638297872e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 7.312895774841309, 'weight_chosen': -0.36576104164123535, 'weight_rejected': 0.15304215252399445, 'kl_term_chosen': 1.1844978332519531, 'epoch': 0.06}
  6%|▌         | 210/3753 [16:20<4:44:04,  4.81s/it]  6%|▌         | 211/3753 [16:25<4:56:53,  5.03s/it]  6%|▌         | 212/3753 [16:28<4:28:25,  4.55s/it]  6%|▌         | 213/3753 [16:33<4:30:41,  4.59s/it]  6%|▌         | 214/3753 [16:39<4:50:07,  4.92s/it]  6%|▌         | 215/3753 [16:43<4:34:12,  4.65s/it]  6%|▌         | 216/3753 [16:47<4:32:06,  4.62s/it]  6%|▌         | 217/3753 [16:51<4:18:42,  4.39s/it]  6%|▌         | 218/3753 [16:56<4:20:59,  4.43s/it]  6%|▌         | 219/3753 [16:59<4:09:42,  4.24s/it]  6%|▌         | 220/3753 [17:03<4:01:44,  4.11s/it]{'loss': 0.7043, 'grad_norm': 1694.5970458984375, 'learning_rate': 5.824468085106384e-07, 'mean_ratio_chosen': 6.6534905433654785, 'mean_ratio_rejected': 2.0496761798858643, 'weight_chosen': -0.2658439874649048, 'weight_rejected': 0.1200256198644638, 'kl_term_chosen': 0.94757080078125, 'epoch': 0.05862758161225849}
                                                    {'loss': 0.7043, 'grad_norm': 1694.5970458984375, 'learning_rate': 5.824468085106384e-07, 'mean_ratio_chosen': 6.6534905433654785, 'mean_ratio_rejected': 2.0496761798858643, 'weight_chosen': -0.2658439874649048, 'weight_rejected': 0.1200256198644638, 'kl_term_chosen': 0.94757080078125, 'epoch': 0.06}
  6%|▌         | 220/3753 [17:03<4:01:44,  4.11s/it]  6%|▌         | 221/3753 [17:07<3:54:30,  3.98s/it]  6%|▌         | 222/3753 [17:11<4:03:09,  4.13s/it]  6%|▌         | 223/3753 [17:15<3:51:32,  3.94s/it]  6%|▌         | 224/3753 [17:19<3:52:39,  3.96s/it]  6%|▌         | 225/3753 [17:23<3:57:34,  4.04s/it]  6%|▌         | 226/3753 [17:27<3:54:43,  3.99s/it]  6%|▌         | 227/3753 [17:32<4:18:25,  4.40s/it]  6%|▌         | 228/3753 [17:36<4:03:14,  4.14s/it]  6%|▌         | 229/3753 [17:41<4:14:03,  4.33s/it]  6%|▌         | 230/3753 [17:47<4:54:18,  5.01s/it]{'loss': 1.7085, 'grad_norm': 1441.6004638671875, 'learning_rate': 6.090425531914894e-07, 'mean_ratio_chosen': 7.339613437652588, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -0.059432268142700195, 'weight_rejected': 0.1490352898836136, 'kl_term_chosen': 0.99664306640625, 'epoch': 0.06129247168554297}
                                                    {'loss': 1.7085, 'grad_norm': 1441.6004638671875, 'learning_rate': 6.090425531914894e-07, 'mean_ratio_chosen': 7.339613437652588, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -0.059432268142700195, 'weight_rejected': 0.1490352898836136, 'kl_term_chosen': 0.99664306640625, 'epoch': 0.06}
  6%|▌         | 230/3753 [17:47<4:54:18,  5.01s/it]  6%|▌         | 231/3753 [17:53<5:08:23,  5.25s/it]  6%|▌         | 232/3753 [17:57<4:52:42,  4.99s/it]  6%|▌         | 233/3753 [18:02<4:52:15,  4.98s/it]  6%|▌         | 234/3753 [18:08<4:57:55,  5.08s/it]  6%|▋         | 235/3753 [18:16<5:59:43,  6.14s/it]  6%|▋         | 236/3753 [18:21<5:27:32,  5.59s/it]  6%|▋         | 237/3753 [18:24<4:49:56,  4.95s/it]  6%|▋         | 238/3753 [18:29<4:47:15,  4.90s/it]  6%|▋         | 239/3753 [18:34<4:47:14,  4.90s/it]  6%|▋         | 240/3753 [18:38<4:40:05,  4.78s/it]{'loss': 2.4887, 'grad_norm': 1483.8724365234375, 'learning_rate': 6.356382978723404e-07, 'mean_ratio_chosen': 1.5330065488815308, 'mean_ratio_rejected': 0.4056055247783661, 'weight_chosen': 0.6175276041030884, 'weight_rejected': 0.17781086266040802, 'kl_term_chosen': 0.21361541748046875, 'epoch': 0.06395736175882745}
                                                    {'loss': 2.4887, 'grad_norm': 1483.8724365234375, 'learning_rate': 6.356382978723404e-07, 'mean_ratio_chosen': 1.5330065488815308, 'mean_ratio_rejected': 0.4056055247783661, 'weight_chosen': 0.6175276041030884, 'weight_rejected': 0.17781086266040802, 'kl_term_chosen': 0.21361541748046875, 'epoch': 0.06}
  6%|▋         | 240/3753 [18:38<4:40:05,  4.78s/it]  6%|▋         | 241/3753 [18:42<4:28:36,  4.59s/it]  6%|▋         | 242/3753 [18:47<4:20:28,  4.45s/it]  6%|▋         | 243/3753 [18:51<4:23:17,  4.50s/it]  7%|▋         | 244/3753 [18:57<4:47:20,  4.91s/it]  7%|▋         | 245/3753 [19:02<4:42:31,  4.83s/it]  7%|▋         | 246/3753 [19:05<4:22:16,  4.49s/it]  7%|▋         | 247/3753 [19:12<5:06:39,  5.25s/it]  7%|▋         | 248/3753 [19:17<5:02:23,  5.18s/it]  7%|▋         | 249/3753 [19:23<5:08:15,  5.28s/it]  7%|▋         | 250/3753 [19:28<5:06:06,  5.24s/it]{'loss': 2.9086, 'grad_norm': 4033.857421875, 'learning_rate': 6.622340425531915e-07, 'mean_ratio_chosen': 2.66021990776062, 'mean_ratio_rejected': 2.679898262023926, 'weight_chosen': 0.40558505058288574, 'weight_rejected': 0.14804719388484955, 'kl_term_chosen': 0.48920440673828125, 'epoch': 0.06662225183211193}
                                                    {'loss': 2.9086, 'grad_norm': 4033.857421875, 'learning_rate': 6.622340425531915e-07, 'mean_ratio_chosen': 2.66021990776062, 'mean_ratio_rejected': 2.679898262023926, 'weight_chosen': 0.40558505058288574, 'weight_rejected': 0.14804719388484955, 'kl_term_chosen': 0.48920440673828125, 'epoch': 0.07}
  7%|▋         | 250/3753 [19:28<5:06:06,  5.24s/it]  7%|▋         | 251/3753 [19:35<5:31:43,  5.68s/it]  7%|▋         | 252/3753 [19:40<5:22:39,  5.53s/it]  7%|▋         | 253/3753 [19:45<5:07:48,  5.28s/it]  7%|▋         | 254/3753 [19:48<4:36:11,  4.74s/it]  7%|▋         | 255/3753 [19:52<4:26:26,  4.57s/it]  7%|▋         | 256/3753 [19:56<4:09:47,  4.29s/it]  7%|▋         | 257/3753 [20:00<4:13:27,  4.35s/it]  7%|▋         | 258/3753 [20:04<4:06:31,  4.23s/it]  7%|▋         | 259/3753 [20:08<3:54:19,  4.02s/it]  7%|▋         | 260/3753 [20:13<4:07:49,  4.26s/it]{'loss': 3.2464, 'grad_norm': 1471.0152587890625, 'learning_rate': 6.888297872340425e-07, 'mean_ratio_chosen': 1.0481828451156616, 'mean_ratio_rejected': 6.979611396789551, 'weight_chosen': 0.8636755347251892, 'weight_rejected': 0.13386748731136322, 'kl_term_chosen': 0.023529052734375, 'epoch': 0.06928714190539641}
                                                    {'loss': 3.2464, 'grad_norm': 1471.0152587890625, 'learning_rate': 6.888297872340425e-07, 'mean_ratio_chosen': 1.0481828451156616, 'mean_ratio_rejected': 6.979611396789551, 'weight_chosen': 0.8636755347251892, 'weight_rejected': 0.13386748731136322, 'kl_term_chosen': 0.023529052734375, 'epoch': 0.07}
  7%|▋         | 260/3753 [20:13<4:07:49,  4.26s/it]  7%|▋         | 261/3753 [20:17<4:13:24,  4.35s/it]  7%|▋         | 262/3753 [20:22<4:17:28,  4.43s/it]  7%|▋         | 263/3753 [20:27<4:20:01,  4.47s/it]  7%|▋         | 264/3753 [20:31<4:12:10,  4.34s/it]  7%|▋         | 265/3753 [20:35<4:10:22,  4.31s/it]  7%|▋         | 266/3753 [20:39<4:01:36,  4.16s/it]  7%|▋         | 267/3753 [20:43<3:58:42,  4.11s/it]  7%|▋         | 268/3753 [20:48<4:13:01,  4.36s/it]  7%|▋         | 269/3753 [20:51<3:54:30,  4.04s/it]  7%|▋         | 270/3753 [20:55<4:04:12,  4.21s/it]{'loss': 5.5264, 'grad_norm': 1003.0199584960938, 'learning_rate': 7.154255319148937e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -2.231576442718506, 'weight_rejected': 0.05033063143491745, 'kl_term_chosen': 3.2071533203125, 'epoch': 0.07195203197868089}
                                                    {'loss': 5.5264, 'grad_norm': 1003.0199584960938, 'learning_rate': 7.154255319148937e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -2.231576442718506, 'weight_rejected': 0.05033063143491745, 'kl_term_chosen': 3.2071533203125, 'epoch': 0.07}
  7%|▋         | 270/3753 [20:56<4:04:12,  4.21s/it]  7%|▋         | 271/3753 [21:00<4:13:41,  4.37s/it]  7%|▋         | 272/3753 [21:05<4:15:41,  4.41s/it]  7%|▋         | 273/3753 [21:08<4:03:48,  4.20s/it]  7%|▋         | 274/3753 [21:12<3:55:48,  4.07s/it]  7%|▋         | 275/3753 [21:16<3:56:35,  4.08s/it]  7%|▋         | 276/3753 [21:20<3:48:09,  3.94s/it]  7%|▋         | 277/3753 [21:25<4:10:00,  4.32s/it]  7%|▋         | 278/3753 [21:29<3:56:13,  4.08s/it]  7%|▋         | 279/3753 [21:33<4:06:14,  4.25s/it]  7%|▋         | 280/3753 [21:38<4:14:57,  4.40s/it]{'loss': 6.8693, 'grad_norm': 1078.6500244140625, 'learning_rate': 7.420212765957447e-07, 'mean_ratio_chosen': 0.7858257293701172, 'mean_ratio_rejected': 0.3690122961997986, 'weight_chosen': 1.0509684085845947, 'weight_rejected': 0.07263670116662979, 'kl_term_chosen': -0.12051010131835938, 'epoch': 0.07461692205196535}
                                                    {'loss': 6.8693, 'grad_norm': 1078.6500244140625, 'learning_rate': 7.420212765957447e-07, 'mean_ratio_chosen': 0.7858257293701172, 'mean_ratio_rejected': 0.3690122961997986, 'weight_chosen': 1.0509684085845947, 'weight_rejected': 0.07263670116662979, 'kl_term_chosen': -0.12051010131835938, 'epoch': 0.07}
  7%|▋         | 280/3753 [21:38<4:14:57,  4.40s/it]  7%|▋         | 281/3753 [21:42<4:11:26,  4.35s/it]  8%|▊         | 282/3753 [21:48<4:43:45,  4.91s/it]  8%|▊         | 283/3753 [21:53<4:34:43,  4.75s/it]  8%|▊         | 284/3753 [21:57<4:25:58,  4.60s/it]  8%|▊         | 285/3753 [22:04<5:11:35,  5.39s/it]  8%|▊         | 286/3753 [22:08<4:43:09,  4.90s/it]  8%|▊         | 287/3753 [22:12<4:29:25,  4.66s/it]  8%|▊         | 288/3753 [22:21<5:42:49,  5.94s/it]  8%|▊         | 289/3753 [22:26<5:24:13,  5.62s/it]  8%|▊         | 290/3753 [22:35<6:18:07,  6.55s/it]{'loss': 7.0316, 'grad_norm': 2785.3720703125, 'learning_rate': 7.686170212765957e-07, 'mean_ratio_chosen': 3.1780903339385986, 'mean_ratio_rejected': 2.9978649616241455, 'weight_chosen': 0.175326406955719, 'weight_rejected': 0.20946967601776123, 'kl_term_chosen': 0.5781402587890625, 'epoch': 0.07728181212524983}
                                                    {'loss': 7.0316, 'grad_norm': 2785.3720703125, 'learning_rate': 7.686170212765957e-07, 'mean_ratio_chosen': 3.1780903339385986, 'mean_ratio_rejected': 2.9978649616241455, 'weight_chosen': 0.175326406955719, 'weight_rejected': 0.20946967601776123, 'kl_term_chosen': 0.5781402587890625, 'epoch': 0.08}
  8%|▊         | 290/3753 [22:35<6:18:07,  6.55s/it]  8%|▊         | 291/3753 [22:38<5:28:32,  5.69s/it]  8%|▊         | 292/3753 [22:44<5:34:05,  5.79s/it]  8%|▊         | 293/3753 [22:48<5:00:07,  5.20s/it]  8%|▊         | 294/3753 [22:52<4:40:14,  4.86s/it]  8%|▊         | 295/3753 [22:57<4:42:34,  4.90s/it]  8%|▊         | 296/3753 [23:01<4:26:11,  4.62s/it]  8%|▊         | 297/3753 [23:06<4:23:10,  4.57s/it]  8%|▊         | 298/3753 [23:12<4:47:11,  4.99s/it]  8%|▊         | 299/3753 [23:16<4:38:21,  4.84s/it]  8%|▊         | 300/3753 [23:20<4:28:05,  4.66s/it]{'loss': 4.2202, 'grad_norm': 1072.15478515625, 'learning_rate': 7.952127659574468e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 0.14235919713974, 'weight_chosen': -0.7581396698951721, 'weight_rejected': 0.052618950605392456, 'kl_term_chosen': 1.682281494140625, 'epoch': 0.07994670219853431}
                                                    {'loss': 4.2202, 'grad_norm': 1072.15478515625, 'learning_rate': 7.952127659574468e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 0.14235919713974, 'weight_chosen': -0.7581396698951721, 'weight_rejected': 0.052618950605392456, 'kl_term_chosen': 1.682281494140625, 'epoch': 0.08}
  8%|▊         | 300/3753 [23:20<4:28:05,  4.66s/it]  8%|▊         | 301/3753 [23:25<4:22:13,  4.56s/it]  8%|▊         | 302/3753 [23:30<4:29:02,  4.68s/it]  8%|▊         | 303/3753 [23:34<4:31:08,  4.72s/it]  8%|▊         | 304/3753 [23:38<4:05:50,  4.28s/it]  8%|▊         | 305/3753 [23:42<3:59:45,  4.17s/it]  8%|▊         | 306/3753 [23:46<3:57:44,  4.14s/it]  8%|▊         | 307/3753 [23:50<4:08:45,  4.33s/it]  8%|▊         | 308/3753 [23:56<4:33:34,  4.76s/it]  8%|▊         | 309/3753 [24:01<4:28:59,  4.69s/it]  8%|▊         | 310/3753 [24:05<4:23:43,  4.60s/it]{'loss': 8.2571, 'grad_norm': 623.3629760742188, 'learning_rate': 8.218085106382978e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.8708794116973877, 'weight_rejected': 0.22815649211406708, 'kl_term_chosen': -2.2072105407714844, 'epoch': 0.08261159227181879}
                                                    {'loss': 8.2571, 'grad_norm': 623.3629760742188, 'learning_rate': 8.218085106382978e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.8708794116973877, 'weight_rejected': 0.22815649211406708, 'kl_term_chosen': -2.2072105407714844, 'epoch': 0.08}
  8%|▊         | 310/3753 [24:05<4:23:43,  4.60s/it]  8%|▊         | 311/3753 [24:09<4:19:04,  4.52s/it]  8%|▊         | 312/3753 [24:14<4:11:49,  4.39s/it]  8%|▊         | 313/3753 [24:18<4:09:43,  4.36s/it]  8%|▊         | 314/3753 [24:25<4:55:12,  5.15s/it]  8%|▊         | 315/3753 [24:29<4:32:43,  4.76s/it]  8%|▊         | 316/3753 [24:33<4:18:27,  4.51s/it]  8%|▊         | 317/3753 [24:37<4:08:09,  4.33s/it]  8%|▊         | 318/3753 [24:41<4:12:58,  4.42s/it]  8%|▊         | 319/3753 [24:45<4:08:28,  4.34s/it]  9%|▊         | 320/3753 [24:50<4:15:18,  4.46s/it]{'loss': 5.697, 'grad_norm': 883.24853515625, 'learning_rate': 8.48404255319149e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 7.763303279876709, 'weight_chosen': -2.20695424079895, 'weight_rejected': 0.04084571450948715, 'kl_term_chosen': 3.1441650390625, 'epoch': 0.08527648234510327}
                                                    {'loss': 5.697, 'grad_norm': 883.24853515625, 'learning_rate': 8.48404255319149e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 7.763303279876709, 'weight_chosen': -2.20695424079895, 'weight_rejected': 0.04084571450948715, 'kl_term_chosen': 3.1441650390625, 'epoch': 0.09}
  9%|▊         | 320/3753 [24:50<4:15:18,  4.46s/it]  9%|▊         | 321/3753 [24:55<4:19:39,  4.54s/it]  9%|▊         | 322/3753 [24:59<4:11:47,  4.40s/it]  9%|▊         | 323/3753 [25:02<3:53:58,  4.09s/it]  9%|▊         | 324/3753 [25:10<4:48:28,  5.05s/it]  9%|▊         | 325/3753 [25:14<4:37:38,  4.86s/it]  9%|▊         | 326/3753 [25:18<4:15:46,  4.48s/it]  9%|▊         | 327/3753 [25:22<4:15:04,  4.47s/it]  9%|▊         | 328/3753 [25:26<4:03:54,  4.27s/it]  9%|▉         | 329/3753 [25:30<4:08:43,  4.36s/it]  9%|▉         | 330/3753 [25:35<4:17:37,  4.52s/it]{'loss': 9.4592, 'grad_norm': 435.0958557128906, 'learning_rate': 8.75e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -0.8020283579826355, 'weight_rejected': 0.22135066986083984, 'kl_term_chosen': 1.6635665893554688, 'epoch': 0.08794137241838774}
                                                    {'loss': 9.4592, 'grad_norm': 435.0958557128906, 'learning_rate': 8.75e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -0.8020283579826355, 'weight_rejected': 0.22135066986083984, 'kl_term_chosen': 1.6635665893554688, 'epoch': 0.09}
  9%|▉         | 330/3753 [25:35<4:17:37,  4.52s/it]  9%|▉         | 331/3753 [25:39<4:12:05,  4.42s/it]  9%|▉         | 332/3753 [25:43<4:02:29,  4.25s/it]  9%|▉         | 333/3753 [25:48<4:07:45,  4.35s/it]  9%|▉         | 334/3753 [25:56<5:10:29,  5.45s/it]  9%|▉         | 335/3753 [26:01<4:58:50,  5.25s/it]  9%|▉         | 336/3753 [26:04<4:25:47,  4.67s/it]  9%|▉         | 337/3753 [26:09<4:29:54,  4.74s/it]  9%|▉         | 338/3753 [26:12<4:07:11,  4.34s/it]  9%|▉         | 339/3753 [26:16<3:55:45,  4.14s/it]  9%|▉         | 340/3753 [26:20<4:00:55,  4.24s/it]{'loss': 5.6804, 'grad_norm': 721.7127075195312, 'learning_rate': 9.01595744680851e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 3.330432415008545, 'weight_rejected': 0.7826625108718872, 'kl_term_chosen': -3.086790084838867, 'epoch': 0.09060626249167222}
                                                    {'loss': 5.6804, 'grad_norm': 721.7127075195312, 'learning_rate': 9.01595744680851e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 3.330432415008545, 'weight_rejected': 0.7826625108718872, 'kl_term_chosen': -3.086790084838867, 'epoch': 0.09}
  9%|▉         | 340/3753 [26:21<4:00:55,  4.24s/it]  9%|▉         | 341/3753 [26:24<3:51:16,  4.07s/it]  9%|▉         | 342/3753 [26:28<3:48:14,  4.01s/it]  9%|▉         | 343/3753 [26:33<3:59:16,  4.21s/it]  9%|▉         | 344/3753 [26:36<3:52:46,  4.10s/it]  9%|▉         | 345/3753 [26:42<4:23:14,  4.63s/it]  9%|▉         | 346/3753 [26:46<4:13:06,  4.46s/it]  9%|▉         | 347/3753 [26:51<4:13:46,  4.47s/it]  9%|▉         | 348/3753 [26:56<4:32:09,  4.80s/it]  9%|▉         | 349/3753 [27:00<4:10:54,  4.42s/it]  9%|▉         | 350/3753 [27:05<4:18:08,  4.55s/it]{'loss': 19.4774, 'grad_norm': 1474.934326171875, 'learning_rate': 9.281914893617021e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.86538314819336, 'weight_chosen': -1.3188066482543945, 'weight_rejected': 0.2186693549156189, 'kl_term_chosen': 2.1532135009765625, 'epoch': 0.09327115256495669}
                                                    {'loss': 19.4774, 'grad_norm': 1474.934326171875, 'learning_rate': 9.281914893617021e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.86538314819336, 'weight_chosen': -1.3188066482543945, 'weight_rejected': 0.2186693549156189, 'kl_term_chosen': 2.1532135009765625, 'epoch': 0.09}
  9%|▉         | 350/3753 [27:05<4:18:08,  4.55s/it]  9%|▉         | 351/3753 [27:09<4:07:17,  4.36s/it]  9%|▉         | 352/3753 [27:12<3:48:42,  4.03s/it]  9%|▉         | 353/3753 [27:16<3:50:10,  4.06s/it]  9%|▉         | 354/3753 [27:21<3:59:01,  4.22s/it]  9%|▉         | 355/3753 [27:27<4:28:30,  4.74s/it]  9%|▉         | 356/3753 [27:31<4:26:46,  4.71s/it] 10%|▉         | 357/3753 [27:36<4:17:27,  4.55s/it] 10%|▉         | 358/3753 [27:40<4:19:18,  4.58s/it] 10%|▉         | 359/3753 [27:45<4:15:41,  4.52s/it] 10%|▉         | 360/3753 [27:48<3:59:48,  4.24s/it]{'loss': 8.1358, 'grad_norm': 857.4568481445312, 'learning_rate': 9.54787234042553e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': -2.168989419937134, 'weight_rejected': 0.16667540371418, 'kl_term_chosen': 2.713306427001953, 'epoch': 0.09593604263824117}
                                                    {'loss': 8.1358, 'grad_norm': 857.4568481445312, 'learning_rate': 9.54787234042553e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': -2.168989419937134, 'weight_rejected': 0.16667540371418, 'kl_term_chosen': 2.713306427001953, 'epoch': 0.1}
 10%|▉         | 360/3753 [27:48<3:59:48,  4.24s/it] 10%|▉         | 361/3753 [27:53<4:10:47,  4.44s/it] 10%|▉         | 362/3753 [27:57<4:07:28,  4.38s/it] 10%|▉         | 363/3753 [28:01<4:03:02,  4.30s/it] 10%|▉         | 364/3753 [28:06<4:06:30,  4.36s/it] 10%|▉         | 365/3753 [28:09<3:52:26,  4.12s/it] 10%|▉         | 366/3753 [28:13<3:42:47,  3.95s/it] 10%|▉         | 367/3753 [28:17<3:46:41,  4.02s/it] 10%|▉         | 368/3753 [28:22<4:04:11,  4.33s/it] 10%|▉         | 369/3753 [28:27<4:06:44,  4.37s/it] 10%|▉         | 370/3753 [28:31<4:04:06,  4.33s/it]{'loss': 7.4323, 'grad_norm': 996.4569702148438, 'learning_rate': 9.813829787234042e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 7.023623466491699, 'weight_rejected': 0.20561504364013672, 'kl_term_chosen': -6.159313201904297, 'epoch': 0.09860093271152565}
                                                    {'loss': 7.4323, 'grad_norm': 996.4569702148438, 'learning_rate': 9.813829787234042e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 7.023623466491699, 'weight_rejected': 0.20561504364013672, 'kl_term_chosen': -6.159313201904297, 'epoch': 0.1}
 10%|▉         | 370/3753 [28:31<4:04:06,  4.33s/it] 10%|▉         | 371/3753 [28:35<3:53:19,  4.14s/it] 10%|▉         | 372/3753 [28:39<4:02:41,  4.31s/it] 10%|▉         | 373/3753 [28:43<3:57:14,  4.21s/it] 10%|▉         | 374/3753 [28:51<4:49:23,  5.14s/it] 10%|▉         | 375/3753 [28:56<4:55:38,  5.25s/it] 10%|█         | 376/3753 [29:05<5:57:34,  6.35s/it] 10%|█         | 377/3753 [29:12<6:01:51,  6.43s/it] 10%|█         | 378/3753 [29:16<5:18:29,  5.66s/it] 10%|█         | 379/3753 [29:19<4:45:33,  5.08s/it] 10%|█         | 380/3753 [29:24<4:40:40,  4.99s/it]{'loss': 6.7612, 'grad_norm': 629.7515258789062, 'learning_rate': 9.999980527583044e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -3.744762659072876, 'weight_rejected': 0.02595735713839531, 'kl_term_chosen': 4.7051239013671875, 'epoch': 0.10126582278481013}
                                                    {'loss': 6.7612, 'grad_norm': 629.7515258789062, 'learning_rate': 9.999980527583044e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -3.744762659072876, 'weight_rejected': 0.02595735713839531, 'kl_term_chosen': 4.7051239013671875, 'epoch': 0.1}
 10%|█         | 380/3753 [29:24<4:40:40,  4.99s/it] 10%|█         | 381/3753 [29:28<4:25:15,  4.72s/it] 10%|█         | 382/3753 [29:33<4:24:35,  4.71s/it] 10%|█         | 383/3753 [29:37<4:12:40,  4.50s/it] 10%|█         | 384/3753 [29:42<4:26:56,  4.75s/it] 10%|█         | 385/3753 [29:52<5:54:18,  6.31s/it] 10%|█         | 386/3753 [29:56<5:12:24,  5.57s/it] 10%|█         | 387/3753 [30:00<4:53:23,  5.23s/it] 10%|█         | 388/3753 [30:05<4:40:13,  5.00s/it] 10%|█         | 389/3753 [30:10<4:37:37,  4.95s/it] 10%|█         | 390/3753 [30:16<4:53:16,  5.23s/it]{'loss': 12.2623, 'grad_norm': 761.2601318359375, 'learning_rate': 9.999634355500922e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 6.967330455780029, 'weight_rejected': 0.8164063692092896, 'kl_term_chosen': -6.328495025634766, 'epoch': 0.1039307128580946}
                                                    {'loss': 12.2623, 'grad_norm': 761.2601318359375, 'learning_rate': 9.999634355500922e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 6.967330455780029, 'weight_rejected': 0.8164063692092896, 'kl_term_chosen': -6.328495025634766, 'epoch': 0.1}
 10%|█         | 390/3753 [30:16<4:53:16,  5.23s/it] 10%|█         | 391/3753 [30:20<4:38:59,  4.98s/it] 10%|█         | 392/3753 [30:24<4:22:30,  4.69s/it] 10%|█         | 393/3753 [30:29<4:23:17,  4.70s/it] 10%|█         | 394/3753 [30:33<4:18:01,  4.61s/it] 11%|█         | 395/3753 [30:38<4:21:22,  4.67s/it] 11%|█         | 396/3753 [30:47<5:27:47,  5.86s/it] 11%|█         | 397/3753 [30:50<4:47:00,  5.13s/it] 11%|█         | 398/3753 [30:56<4:59:54,  5.36s/it] 11%|█         | 399/3753 [31:01<4:57:08,  5.32s/it] 11%|█         | 400/3753 [31:05<4:39:27,  5.00s/it]{'loss': 4.7205, 'grad_norm': 527.9431762695312, 'learning_rate': 9.998855497526103e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 6.471599578857422, 'weight_rejected': 0.25091278553009033, 'kl_term_chosen': -5.776468276977539, 'epoch': 0.10659560293137908}
                                                    {'loss': 4.7205, 'grad_norm': 527.9431762695312, 'learning_rate': 9.998855497526103e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 6.471599578857422, 'weight_rejected': 0.25091278553009033, 'kl_term_chosen': -5.776468276977539, 'epoch': 0.11}
 11%|█         | 400/3753 [31:06<4:39:27,  5.00s/it] 11%|█         | 401/3753 [31:09<4:22:43,  4.70s/it] 11%|█         | 402/3753 [31:14<4:23:25,  4.72s/it] 11%|█         | 403/3753 [31:18<4:02:11,  4.34s/it] 11%|█         | 404/3753 [31:22<3:57:47,  4.26s/it] 11%|█         | 405/3753 [31:27<4:11:19,  4.50s/it] 11%|█         | 406/3753 [31:31<4:13:11,  4.54s/it] 11%|█         | 407/3753 [31:35<4:00:49,  4.32s/it] 11%|█         | 408/3753 [31:40<4:10:32,  4.49s/it] 11%|█         | 409/3753 [31:44<4:08:37,  4.46s/it] 11%|█         | 410/3753 [31:49<4:09:31,  4.48s/it]{'loss': 11.3756, 'grad_norm': 467.93731689453125, 'learning_rate': 9.997644021063695e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 1.9724889993667603, 'weight_rejected': 0.28776782751083374, 'kl_term_chosen': -1.325582504272461, 'epoch': 0.10926049300466356}
                                                    {'loss': 11.3756, 'grad_norm': 467.93731689453125, 'learning_rate': 9.997644021063695e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 1.9724889993667603, 'weight_rejected': 0.28776782751083374, 'kl_term_chosen': -1.325582504272461, 'epoch': 0.11}
 11%|█         | 410/3753 [31:49<4:09:31,  4.48s/it] 11%|█         | 411/3753 [31:54<4:18:07,  4.63s/it] 11%|█         | 412/3753 [31:58<4:09:54,  4.49s/it] 11%|█         | 413/3753 [32:04<4:26:55,  4.80s/it] 11%|█         | 414/3753 [32:08<4:27:26,  4.81s/it] 11%|█         | 415/3753 [32:13<4:18:55,  4.65s/it] 11%|█         | 416/3753 [32:17<4:12:31,  4.54s/it] 11%|█         | 417/3753 [32:21<4:07:40,  4.45s/it] 11%|█         | 418/3753 [32:25<3:59:10,  4.30s/it] 11%|█         | 419/3753 [32:29<3:53:29,  4.20s/it] 11%|█         | 420/3753 [32:33<3:52:54,  4.19s/it]{'loss': 13.5088, 'grad_norm': 707.1337890625, 'learning_rate': 9.99600003095912e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': 1.9854499101638794, 'weight_rejected': 0.2727987766265869, 'kl_term_chosen': -1.3340950012207031, 'epoch': 0.11192538307794804}
                                                    {'loss': 13.5088, 'grad_norm': 707.1337890625, 'learning_rate': 9.99600003095912e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': 1.9854499101638794, 'weight_rejected': 0.2727987766265869, 'kl_term_chosen': -1.3340950012207031, 'epoch': 0.11}
 11%|█         | 420/3753 [32:33<3:52:54,  4.19s/it] 11%|█         | 421/3753 [32:38<3:55:51,  4.25s/it] 11%|█         | 422/3753 [32:42<3:51:37,  4.17s/it] 11%|█▏        | 423/3753 [32:46<3:47:18,  4.10s/it] 11%|█▏        | 424/3753 [32:49<3:30:29,  3.79s/it] 11%|█▏        | 425/3753 [32:53<3:37:34,  3.92s/it] 11%|█▏        | 426/3753 [32:58<3:47:27,  4.10s/it] 11%|█▏        | 427/3753 [33:02<3:53:28,  4.21s/it] 11%|█▏        | 428/3753 [33:07<4:01:25,  4.36s/it] 11%|█▏        | 429/3753 [33:11<3:56:25,  4.27s/it] 11%|█▏        | 430/3753 [33:16<4:08:03,  4.48s/it]{'loss': 8.4969, 'grad_norm': 615.159912109375, 'learning_rate': 9.99392366948904e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 4.677341938018799, 'weight_rejected': 0.27125170826911926, 'kl_term_chosen': -4.1137237548828125, 'epoch': 0.1145902731512325}
                                                    {'loss': 8.4969, 'grad_norm': 615.159912109375, 'learning_rate': 9.99392366948904e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 4.677341938018799, 'weight_rejected': 0.27125170826911926, 'kl_term_chosen': -4.1137237548828125, 'epoch': 0.11}
 11%|█▏        | 430/3753 [33:16<4:08:03,  4.48s/it] 11%|█▏        | 431/3753 [33:19<3:54:32,  4.24s/it] 12%|█▏        | 432/3753 [33:28<5:08:31,  5.57s/it] 12%|█▏        | 433/3753 [33:32<4:41:29,  5.09s/it] 12%|█▏        | 434/3753 [33:36<4:24:53,  4.79s/it] 12%|█▏        | 435/3753 [33:40<4:08:00,  4.48s/it] 12%|█▏        | 436/3753 [33:44<4:05:40,  4.44s/it] 12%|█▏        | 437/3753 [33:49<4:06:14,  4.46s/it] 12%|█▏        | 438/3753 [33:54<4:13:43,  4.59s/it] 12%|█▏        | 439/3753 [33:58<4:09:15,  4.51s/it] 12%|█▏        | 440/3753 [34:02<4:03:43,  4.41s/it]{'loss': 8.4186, 'grad_norm': 0.0, 'learning_rate': 9.991415116349067e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -4.815715312957764, 'weight_rejected': 0.048136770725250244, 'kl_term_chosen': 5.770362854003906, 'epoch': 0.11725516322451698}
                                                    {'loss': 8.4186, 'grad_norm': 0.0, 'learning_rate': 9.991415116349067e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -4.815715312957764, 'weight_rejected': 0.048136770725250244, 'kl_term_chosen': 5.770362854003906, 'epoch': 0.12}
 12%|█▏        | 440/3753 [34:02<4:03:43,  4.41s/it] 12%|█▏        | 441/3753 [34:08<4:29:44,  4.89s/it] 12%|█▏        | 442/3753 [34:13<4:24:36,  4.79s/it] 12%|█▏        | 443/3753 [34:16<3:57:47,  4.31s/it] 12%|█▏        | 444/3753 [34:21<4:08:08,  4.50s/it] 12%|█▏        | 445/3753 [34:25<3:56:07,  4.28s/it] 12%|█▏        | 446/3753 [34:29<4:01:03,  4.37s/it] 12%|█▏        | 447/3753 [34:34<4:00:52,  4.37s/it] 12%|█▏        | 448/3753 [34:37<3:42:17,  4.04s/it] 12%|█▏        | 449/3753 [34:42<4:04:12,  4.43s/it] 12%|█▏        | 450/3753 [34:46<4:00:51,  4.38s/it]{'loss': 25.4788, 'grad_norm': 1358.698974609375, 'learning_rate': 9.988474588638185e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -5.351967811584473, 'weight_rejected': 0.05582314357161522, 'kl_term_chosen': 6.245277404785156, 'epoch': 0.11992005329780146}
                                                    {'loss': 25.4788, 'grad_norm': 1358.698974609375, 'learning_rate': 9.988474588638185e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -5.351967811584473, 'weight_rejected': 0.05582314357161522, 'kl_term_chosen': 6.245277404785156, 'epoch': 0.12}
 12%|█▏        | 450/3753 [34:47<4:00:51,  4.38s/it] 12%|█▏        | 451/3753 [34:51<4:03:19,  4.42s/it] 12%|█▏        | 452/3753 [34:56<4:15:04,  4.64s/it] 12%|█▏        | 453/3753 [35:00<3:56:07,  4.29s/it] 12%|█▏        | 454/3753 [35:05<4:16:56,  4.67s/it] 12%|█▏        | 455/3753 [35:10<4:13:36,  4.61s/it] 12%|█▏        | 456/3753 [35:16<4:40:02,  5.10s/it] 12%|█▏        | 457/3753 [35:20<4:23:44,  4.80s/it] 12%|█▏        | 458/3753 [35:24<4:07:57,  4.52s/it] 12%|█▏        | 459/3753 [35:28<4:03:13,  4.43s/it] 12%|█▏        | 460/3753 [35:34<4:22:19,  4.78s/it]{'loss': 14.8372, 'grad_norm': 551.3660278320312, 'learning_rate': 9.985102340839976e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 3.8886797428131104, 'weight_rejected': 0.04084571450948715, 'kl_term_chosen': -2.9701995849609375, 'epoch': 0.12258494337108594}
                                                    {'loss': 14.8372, 'grad_norm': 551.3660278320312, 'learning_rate': 9.985102340839976e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 3.8886797428131104, 'weight_rejected': 0.04084571450948715, 'kl_term_chosen': -2.9701995849609375, 'epoch': 0.12}
 12%|█▏        | 460/3753 [35:34<4:22:19,  4.78s/it] 12%|█▏        | 461/3753 [35:37<4:04:31,  4.46s/it] 12%|█▏        | 462/3753 [35:40<3:40:35,  4.02s/it] 12%|█▏        | 463/3753 [35:45<3:45:18,  4.11s/it] 12%|█▏        | 464/3753 [35:49<3:46:33,  4.13s/it] 12%|█▏        | 465/3753 [35:53<3:45:16,  4.11s/it] 12%|█▏        | 466/3753 [35:57<3:48:45,  4.18s/it] 12%|█▏        | 467/3753 [36:02<3:56:25,  4.32s/it] 12%|█▏        | 468/3753 [36:10<4:53:09,  5.35s/it] 12%|█▏        | 469/3753 [36:15<4:54:37,  5.38s/it] 13%|█▎        | 470/3753 [36:21<5:06:01,  5.59s/it]{'loss': 6.815, 'grad_norm': 647.8219604492188, 'learning_rate': 9.981298664800593e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 0.11374136805534363, 'weight_chosen': -6.967633247375488, 'weight_rejected': 0.08151976019144058, 'kl_term_chosen': 7.912628173828125, 'epoch': 0.1252498334443704}
                                                    {'loss': 6.815, 'grad_norm': 647.8219604492188, 'learning_rate': 9.981298664800593e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 0.11374136805534363, 'weight_chosen': -6.967633247375488, 'weight_rejected': 0.08151976019144058, 'kl_term_chosen': 7.912628173828125, 'epoch': 0.13}
 13%|█▎        | 470/3753 [36:21<5:06:01,  5.59s/it] 13%|█▎        | 471/3753 [36:27<5:07:38,  5.62s/it] 13%|█▎        | 472/3753 [36:30<4:34:29,  5.02s/it] 13%|█▎        | 473/3753 [36:34<4:01:48,  4.42s/it] 13%|█▎        | 474/3753 [36:38<4:03:01,  4.45s/it] 13%|█▎        | 475/3753 [36:43<4:04:51,  4.48s/it] 13%|█▎        | 476/3753 [36:46<3:54:05,  4.29s/it] 13%|█▎        | 477/3753 [36:51<3:57:14,  4.35s/it] 13%|█▎        | 478/3753 [36:55<3:56:25,  4.33s/it] 13%|█▎        | 479/3753 [37:01<4:20:42,  4.78s/it] 13%|█▎        | 480/3753 [37:06<4:19:10,  4.75s/it]{'loss': 31.3012, 'grad_norm': 676.2782592773438, 'learning_rate': 9.9770638897035e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.1041908264160156, 'weight_rejected': 0.23792988061904907, 'kl_term_chosen': -1.52764892578125, 'epoch': 0.1279147235176549}
                                                    {'loss': 31.3012, 'grad_norm': 676.2782592773438, 'learning_rate': 9.9770638897035e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 2.1041908264160156, 'weight_rejected': 0.23792988061904907, 'kl_term_chosen': -1.52764892578125, 'epoch': 0.13}
 13%|█▎        | 480/3753 [37:06<4:19:10,  4.75s/it] 13%|█▎        | 481/3753 [37:11<4:22:48,  4.82s/it] 13%|█▎        | 482/3753 [37:15<4:09:36,  4.58s/it] 13%|█▎        | 483/3753 [37:19<3:58:49,  4.38s/it] 13%|█▎        | 484/3753 [37:23<3:53:22,  4.28s/it] 13%|█▎        | 485/3753 [37:26<3:42:36,  4.09s/it] 13%|█▎        | 486/3753 [37:31<3:59:02,  4.39s/it] 13%|█▎        | 487/3753 [37:36<4:05:53,  4.52s/it] 13%|█▎        | 488/3753 [37:40<3:54:09,  4.30s/it] 13%|█▎        | 489/3753 [37:45<4:05:07,  4.51s/it] 13%|█▎        | 490/3753 [37:50<4:18:40,  4.76s/it]{'loss': 64.4775, 'grad_norm': 0.0, 'learning_rate': 9.972398382040988e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -14.405089378356934, 'weight_rejected': 0.054198723286390305, 'kl_term_chosen': 15.349266052246094, 'epoch': 0.13057961359093936}
                                                    {'loss': 64.4775, 'grad_norm': 0.0, 'learning_rate': 9.972398382040988e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 9.974181175231934, 'weight_chosen': -14.405089378356934, 'weight_rejected': 0.054198723286390305, 'kl_term_chosen': 15.349266052246094, 'epoch': 0.13}
 13%|█▎        | 490/3753 [37:50<4:18:40,  4.76s/it] 13%|█▎        | 491/3753 [37:55<4:21:07,  4.80s/it] 13%|█▎        | 492/3753 [37:59<4:04:04,  4.49s/it] 13%|█▎        | 493/3753 [38:09<5:31:07,  6.09s/it] 13%|█▎        | 494/3753 [38:13<4:58:09,  5.49s/it] 13%|█▎        | 495/3753 [38:16<4:23:55,  4.86s/it] 13%|█▎        | 496/3753 [38:21<4:24:03,  4.86s/it] 13%|█▎        | 497/3753 [38:25<4:04:12,  4.50s/it] 13%|█▎        | 498/3753 [38:33<4:59:49,  5.53s/it] 13%|█▎        | 499/3753 [38:39<5:11:53,  5.75s/it] 13%|█▎        | 500/3753 [38:44<5:00:26,  5.54s/it]{'loss': 68.0137, 'grad_norm': 12.840426445007324, 'learning_rate': 9.967302545582453e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 4.187912940979004, 'weight_rejected': 0.22270013391971588, 'kl_term_chosen': -3.545480728149414, 'epoch': 0.13324450366422386}
                                                    {'loss': 68.0137, 'grad_norm': 12.840426445007324, 'learning_rate': 9.967302545582453e-07, 'mean_ratio_chosen': 0.10025884956121445, 'mean_ratio_rejected': 0.10025884956121445, 'weight_chosen': 4.187912940979004, 'weight_rejected': 0.22270013391971588, 'kl_term_chosen': -3.545480728149414, 'epoch': 0.13}
 13%|█▎        | 500/3753 [38:44<5:00:26,  5.54s/it]/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. 
  warnings.warn(
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/envs/advan/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
 13%|█▎        | 501/3753 [39:46<20:11:47, 22.36s/it] 13%|█▎        | 502/3753 [39:54<16:16:16, 18.02s/it] 13%|█▎        | 503/3753 [39:58<12:35:15, 13.94s/it] 13%|█▎        | 504/3753 [40:02<9:48:52, 10.87s/it]  13%|█▎        | 505/3753 [40:05<7:50:48,  8.70s/it] 13%|█▎        | 506/3753 [40:09<6:33:32,  7.27s/it] 14%|█▎        | 507/3753 [40:15<6:04:01,  6.73s/it] 14%|█▎        | 508/3753 [40:20<5:39:00,  6.27s/it] 14%|█▎        | 509/3753 [40:24<5:01:41,  5.58s/it] 14%|█▎        | 510/3753 [40:28<4:41:42,  5.21s/it]{'loss': 32.7536, 'grad_norm': 609.9403686523438, 'learning_rate': 9.961776821339456e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 0.4645325839519501, 'weight_chosen': -0.25635993480682373, 'weight_rejected': 0.03358950838446617, 'kl_term_chosen': 1.21234130859375, 'epoch': 0.13590939373750832}
                                                    {'loss': 32.7536, 'grad_norm': 609.9403686523438, 'learning_rate': 9.961776821339456e-07, 'mean_ratio_chosen': 9.974181175231934, 'mean_ratio_rejected': 0.4645325839519501, 'weight_chosen': -0.25635993480682373, 'weight_rejected': 0.03358950838446617, 'kl_term_chosen': 1.21234130859375, 'epoch': 0.14}
 14%|█▎        | 510/3753 [40:28<4:41:42,  5.21s/it] 14%|█▎        | 511/3753 [40:33<4:27:41,  4.95s/it] 14%|█▎        | 512/3753 [40:37<4:15:58,  4.74s/it]