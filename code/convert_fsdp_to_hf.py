#!/usr/bin/env python3
"""
å°† FSDP SHARDED_STATE_DICT æ ¼å¼çš„ checkpoint è½¬æ¢ä¸º HuggingFace æ ¼å¼

ä½¿ç”¨æ–¹æ³•:
    python convert_fsdp_to_hf.py \
        --base_model_path /path/to/base/model \
        --checkpoint_path /path/to/checkpoint-500 \
        --output_path /path/to/output/model

æˆ–è€…ä½¿ç”¨ accelerate è¿è¡Œï¼ˆå¦‚æœ checkpoint éœ€è¦å¤š GPU ç¯å¢ƒï¼‰:
    accelerate launch convert_fsdp_to_hf.py \
        --base_model_path /path/to/base/model \
        --checkpoint_path /path/to/checkpoint-500 \
        --output_path /path/to/output/model
"""

import argparse
import os
import torch
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer
from accelerate import Accelerator


def convert_fsdp_to_hf(
    base_model_path: str,
    checkpoint_path: str,
    output_path: str,
    dtype: str = "bfloat16",
    model_name_in_checkpoint: str = None,
):
    """
    å°† FSDP åˆ†ç‰‡ checkpoint è½¬æ¢ä¸º HuggingFace æ ¼å¼
    
    Args:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºè·å– config å’Œ tokenizerï¼‰
        checkpoint_path: FSDP checkpoint ç›®å½•è·¯å¾„ï¼ˆåŒ…å« pytorch_model_fsdp_0 ç­‰å­ç›®å½•ï¼‰
        output_path: è¾“å‡º HuggingFace æ ¼å¼æ¨¡å‹çš„ä¿å­˜è·¯å¾„
        dtype: æ¨¡å‹æ•°æ®ç±»å‹ï¼Œå¯é€‰ "bfloat16", "float16", "float32"
        model_name_in_checkpoint: checkpoint ä¸­æ¨¡å‹æƒé‡å­ç›®å½•åï¼ˆå¦‚ "pytorch_model_fsdp_0"ï¼‰ï¼Œ
                                   å¦‚æœä¸º None åˆ™è‡ªåŠ¨æ£€æµ‹
    """
    print("=" * 60)
    print("FSDP Checkpoint â†’ HuggingFace æ ¼å¼è½¬æ¢å·¥å…·")
    print("=" * 60)
    
    # 1. ç¡®å®šæ•°æ®ç±»å‹
    torch_dtype_map = {
        "bfloat16": torch.bfloat16,
        "float16": torch.float16,
        "float32": torch.float32,
    }
    if dtype not in torch_dtype_map:
        raise ValueError(f"ä¸æ”¯æŒçš„ dtype: {dtype}ï¼Œè¯·é€‰æ‹© {list(torch_dtype_map.keys())}")
    torch_dtype = torch_dtype_map[dtype]
    
    # 2. ä»åŸºç¡€æ¨¡å‹åŠ è½½é…ç½®
    print(f"\næ­¥éª¤ 1/5: ä»åŸºç¡€æ¨¡å‹åŠ è½½é…ç½®...")
    print(f"  åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")
    config = AutoConfig.from_pretrained(base_model_path)
    print("  âœ… é…ç½®åŠ è½½å®Œæˆ")
    
    # 3. åˆå§‹åŒ– Acceleratorï¼ˆå•è¿›ç¨‹æ¨¡å¼ï¼Œç”¨äºåŠ è½½ FSDP checkpointï¼‰
    print(f"\næ­¥éª¤ 2/5: åˆå§‹åŒ– Accelerator...")
    accelerator = Accelerator()
    print("  âœ… Accelerator åˆå§‹åŒ–å®Œæˆ")
    
    # 4. ä»åŸºç¡€æ¨¡å‹åŠ è½½å®Œæ•´æ¨¡å‹ç»“æ„ï¼ˆåœ¨CPUä¸Šï¼Œé¿å…æ˜¾å­˜å ç”¨ï¼‰
    # æ³¨æ„ï¼šä½¿ç”¨ from_pretrained è€Œä¸æ˜¯ from_configï¼Œå› ä¸º FSDP éœ€è¦å®Œæ•´çš„æ¨¡å‹ç»“æ„
    print(f"\næ­¥éª¤ 3/5: ä»åŸºç¡€æ¨¡å‹åŠ è½½æ¨¡å‹ç»“æ„...")
    print(f"  âš ï¸  æ³¨æ„: æ­¤æ­¥éª¤ä¼šä»åŸºç¡€æ¨¡å‹åŠ è½½æƒé‡ï¼Œä½†åç»­ä¼šè¢« FSDP æƒé‡è¦†ç›–")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch_dtype,
        device_map="cpu",  # åœ¨ CPU ä¸ŠåŠ è½½ï¼ŒèŠ‚çœæ˜¾å­˜
    )
    print(f"  âœ… æ¨¡å‹ç»“æ„åŠ è½½å®Œæˆï¼Œå‚æ•°é‡: {model.num_parameters() / 1e9:.2f}B")
    
    # 5. ä½¿ç”¨ Accelerator åŒ…è£…æ¨¡å‹ï¼ˆè¿™æ˜¯ FSDP åŠ è½½æ‰€å¿…éœ€çš„ï¼‰
    print(f"\næ­¥éª¤ 3.5/5: ä½¿ç”¨ Accelerator åŒ…è£…æ¨¡å‹...")
    model = accelerator.prepare(model)
    print("  âœ… æ¨¡å‹åŒ…è£…å®Œæˆ")
    
    # 6. è‡ªåŠ¨æ£€æµ‹æˆ–ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹æƒé‡å­ç›®å½•
    if model_name_in_checkpoint is None:
        print(f"\næ­¥éª¤ 4/5: æ£€æµ‹ checkpoint ä¸­çš„æ¨¡å‹æƒé‡ç›®å½•...")
        print(f"  Checkpoint è·¯å¾„: {checkpoint_path}")
        
        # æŸ¥æ‰¾ pytorch_model_fsdp_* ç›®å½•
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint è·¯å¾„ä¸å­˜åœ¨: {checkpoint_path}")
        
        model_name_in_checkpoint = None
        for item in os.listdir(checkpoint_path):
            if item.startswith("pytorch_model_fsdp_"):
                model_name_in_checkpoint = item
                break
        
        if model_name_in_checkpoint is None:
            raise FileNotFoundError(
                f"åœ¨ '{checkpoint_path}' ä¸­æ‰¾ä¸åˆ° 'pytorch_model_fsdp_*' ç›®å½•ã€‚\n"
                f"è¯·æ£€æŸ¥ checkpoint è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨ --model_name_in_checkpoint å‚æ•°æ‰‹åŠ¨æŒ‡å®šã€‚"
            )
        print(f"  âœ… æ£€æµ‹åˆ°æ¨¡å‹æƒé‡ç›®å½•: {model_name_in_checkpoint}")
    else:
        print(f"\næ­¥éª¤ 4/5: ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹æƒé‡ç›®å½•: {model_name_in_checkpoint}")
    
    # 7. åŠ è½½ FSDP åˆ†ç‰‡æƒé‡
    print(f"\næ­¥éª¤ 4.5/5: åŠ è½½ FSDP åˆ†ç‰‡æƒé‡...")
    print(f"  âš ï¸  æ³¨æ„: æ­¤æ­¥éª¤å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    # ä½¿ç”¨ accelerator.load_state åŠ è½½ FSDP æ£€æŸ¥ç‚¹
    # è¿™æ˜¯æœ€å¯é çš„æ–¹æ³•ï¼Œå…¼å®¹ä¸åŒç‰ˆæœ¬çš„ accelerate
    try:
        # å°è¯•ä½¿ç”¨ model_name å‚æ•°ï¼ˆè¾ƒæ–°ç‰ˆæœ¬ï¼‰
        accelerator.load_state(
            checkpoint_path,
            model_name=model_name_in_checkpoint,
            strict=False,  # åªåŠ è½½æ¨¡å‹æƒé‡ï¼Œå¿½ç•¥ä¼˜åŒ–å™¨ç­‰
        )
    except TypeError:
        # å¦‚æœ model_name å‚æ•°ä¸å­˜åœ¨ï¼Œå°è¯•ä¸ä½¿ç”¨è¯¥å‚æ•°
        try:
            accelerator.load_state(
                checkpoint_path,
                strict=False,
            )
            print("  âš ï¸  æ³¨æ„: ä½¿ç”¨äº†ä¸å¸¦ model_name çš„åŠ è½½æ–¹å¼ï¼Œå¦‚æœå¤±è´¥è¯·æ£€æŸ¥ checkpoint ç»“æ„")
        except Exception as e:
            raise RuntimeError(
                f"åŠ è½½ checkpoint å¤±è´¥: {e}\n"
                f"è¯·æ£€æŸ¥ accelerate ç‰ˆæœ¬ï¼Œæˆ–å°è¯•ä½¿ç”¨ Accelerator çš„ load_state æ–¹æ³•ã€‚\n"
                f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}"
            )
    
    print("  âœ… FSDP æƒé‡åŠ è½½å®Œæˆ")
    
    # 8. è§£åŒ…æ¨¡å‹ï¼Œè·å–åº•å±‚çš„ HuggingFace æ¨¡å‹
    print(f"\næ­¥éª¤ 4.6/5: è§£åŒ…æ¨¡å‹...")
    model = accelerator.unwrap_model(model)
    print("  âœ… æ¨¡å‹è§£åŒ…å®Œæˆ")
    
    # 9. ç»‘å®šæƒé‡ï¼ˆå¯¹äº LLaMA ç­‰æ¨¡å‹å¾ˆé‡è¦ï¼‰
    if hasattr(model, "tie_weights"):
        print("\næ­¥éª¤ 4.7/5: ç»‘å®šæƒé‡...")
        model.tie_weights()
        print("  âœ… æƒé‡ç»‘å®šå®Œæˆ")
    
    # 10. ä¿å­˜ä¸º HuggingFace æ ¼å¼
    print(f"\næ­¥éª¤ 5/5: ä¿å­˜ä¸º HuggingFace æ ¼å¼...")
    print(f"  è¾“å‡ºè·¯å¾„: {output_path}")
    print(f"  âš ï¸  æ³¨æ„: æ­¤æ­¥éª¤å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œå–å†³äºæ¨¡å‹å¤§å°...")
    
    os.makedirs(output_path, exist_ok=True)
    model.save_pretrained(output_path, safe_serialization=True)
    print("  âœ… æ¨¡å‹æƒé‡ä¿å­˜å®Œæˆ")
    
    # 11. ä¿å­˜ tokenizer
    print(f"\né¢å¤–æ­¥éª¤: ä¿å­˜ tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        tokenizer.save_pretrained(output_path)
        print("  âœ… Tokenizer ä¿å­˜å®Œæˆ")
    except Exception as e:
        print(f"  âš ï¸  è­¦å‘Š: æ— æ³•è‡ªåŠ¨ä¿å­˜ tokenizer: {e}")
        print(f"  è¯·æ‰‹åŠ¨ä» {base_model_path} å¤åˆ¶ tokenizer æ–‡ä»¶åˆ° {output_path}")
    
    # å®Œæˆ
    print("\n" + "=" * 60)
    print("ğŸ‰ è½¬æ¢å®Œæˆï¼")
    print("=" * 60)
    print(f"\nåˆå¹¶åçš„ HuggingFace æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print(f"  from transformers import AutoModelForCausalLM, AutoTokenizer")
    print(f"  model = AutoModelForCausalLM.from_pretrained('{output_path}')")
    print(f"  tokenizer = AutoTokenizer.from_pretrained('{output_path}')")
    print("\n" + "=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="å°† FSDP SHARDED_STATE_DICT æ ¼å¼çš„ checkpoint è½¬æ¢ä¸º HuggingFace æ ¼å¼",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ç”¨æ³•
  python convert_fsdp_to_hf.py \\
      --base_model_path /path/to/Llama-3-8B-Instruct \\
      --checkpoint_path /path/to/checkpoint-500 \\
      --output_path /path/to/merged-model

  # æŒ‡å®šæ¨¡å‹æƒé‡å­ç›®å½•å
  python convert_fsdp_to_hf.py \\
      --base_model_path /path/to/Llama-3-8B-Instruct \\
      --checkpoint_path /path/to/checkpoint-500 \\
      --output_path /path/to/merged-model \\
      --model_name_in_checkpoint pytorch_model_fsdp_0

  # ä½¿ç”¨ accelerate è¿è¡Œï¼ˆå¦‚æœéœ€è¦å¤š GPU ç¯å¢ƒï¼‰
  accelerate launch convert_fsdp_to_hf.py \\
      --base_model_path /path/to/Llama-3-8B-Instruct \\
      --checkpoint_path /path/to/checkpoint-500 \\
      --output_path /path/to/merged-model
        """
    )
    
    parser.add_argument(
        "--base_model_path",
        type=str,
        required=True,
        help="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºè·å– config.json å’Œ tokenizerï¼Œä¾‹å¦‚: /path/to/Llama-3-8B-Instructï¼‰"
    )
    
    parser.add_argument(
        "--checkpoint_path",
        type=str,
        required=True,
        help="FSDP checkpoint ç›®å½•è·¯å¾„ï¼ˆåŒ…å« pytorch_model_fsdp_0 ç­‰å­ç›®å½•ï¼Œä¾‹å¦‚: /path/to/checkpoint-500ï¼‰"
    )
    
    parser.add_argument(
        "--output_path",
        type=str,
        required=True,
        help="è¾“å‡º HuggingFace æ ¼å¼æ¨¡å‹çš„ä¿å­˜è·¯å¾„"
    )
    
    parser.add_argument(
        "--dtype",
        type=str,
        default="bfloat16",
        choices=["bfloat16", "float16", "float32"],
        help="æ¨¡å‹æ•°æ®ç±»å‹ï¼ˆé»˜è®¤: bfloat16ï¼‰"
    )
    
    parser.add_argument(
        "--model_name_in_checkpoint",
        type=str,
        default=None,
        help="checkpoint ä¸­æ¨¡å‹æƒé‡å­ç›®å½•åï¼ˆå¦‚ 'pytorch_model_fsdp_0'ï¼‰ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨æ£€æµ‹"
    )
    
    args = parser.parse_args()
    
    convert_fsdp_to_hf(
        base_model_path=args.base_model_path,
        checkpoint_path=args.checkpoint_path,
        output_path=args.output_path,
        dtype=args.dtype,
        model_name_in_checkpoint=args.model_name_in_checkpoint,
    )


if __name__ == "__main__":
    main()

